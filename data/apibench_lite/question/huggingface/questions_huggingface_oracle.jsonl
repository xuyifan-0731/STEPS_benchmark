{"question_id": 1, "text": " Design a feature for a social media website to recommend articles to users based on how similar the articles are to their previously liked articles.\n \n Use this API documentation for reference:  {\"domain\": \"Natural Language Processing Sentence Similarity\", \"framework\": \"Hugging Face Transformers\", \"functionality\": \"Feature Extraction\", \"api_name\": \"princeton-nlp/unsup-simcse-roberta-base\", \"api_call\": \"AutoModel.from_pretrained('princeton-nlp/unsup-simcse-roberta-base')\", \"api_arguments\": null, \"python_environment_requirements\": [\"transformers\"], \"example_code\": null, \"performance\": {\"dataset\": null, \"accuracy\": null}, \"description\": \"An unsupervised sentence embedding model trained using the SimCSE approach with a Roberta base architecture.\"}}\nWrite a python program in 1 to 2 lines to call API in huggingface.\n\nThe answer should follow the format: {<<<domain>>> $DOMAIN, <<<api_call>>>: $API_CALL, <<<api_provider>>>: $API_PROVIDER, <<<explanation>>>: $EXPLANATION, <<<code>>>: $CODE}. Here are the requirements:\n1. The $API_CALL should have only 1 line of code that calls api.\n2. The $API_PROVIDER should be the programming framework used.\n3. $EXPLANATION should be a step-by-step explanation.\n4. The $CODE is the python code.\n5. Do not repeat the format in your answer.", "category": "generic"}
{"question_id": 2, "text": " The user is interested in a tool to find relationships between medical terms.\n \n Use this API documentation for reference:  {\"domain\": \"Multimodal Feature Extraction\", \"framework\": \"Hugging Face Transformers\", \"functionality\": \"Feature Extraction\", \"api_name\": \"GanjinZero/UMLSBert_ENG\", \"api_call\": \"AutoModel.from_pretrained('GanjinZero/UMLSBert_ENG')\", \"api_arguments\": [], \"python_environment_requirements\": [\"transformers\"], \"example_code\": \"\", \"performance\": {\"dataset\": \"\", \"accuracy\": \"\"}, \"description\": \"CODER: Knowledge infused cross-lingual medical term embedding for term normalization. English Version. Old name. This model is not UMLSBert! Github Link: https://github.com/GanjinZero/CODER\"}}\nWrite a python program in 1 to 2 lines to call API in huggingface.\n\nThe answer should follow the format: {<<<domain>>> $DOMAIN, <<<api_call>>>: $API_CALL, <<<api_provider>>>: $API_PROVIDER, <<<explanation>>>: $EXPLANATION, <<<code>>>: $CODE}. Here are the requirements:\n1. The $API_CALL should have only 1 line of code that calls api.\n2. The $API_PROVIDER should be the programming framework used.\n3. $EXPLANATION should be a step-by-step explanation.\n4. The $CODE is the python code.\n5. Do not repeat the format in your answer.", "category": "generic"}
{"question_id": 3, "text": " As a journalist, I am curious about speech sentiment analysis in a group of people in a crowd. I want to extract features from the audio to run sentiment analysis.\n \n Use this API documentation for reference:  {\"domain\": \"Multimodal Feature Extraction\", \"framework\": \"Hugging Face Transformers\", \"functionality\": \"Feature Extraction\", \"api_name\": \"hubert-large-ll60k\", \"api_call\": \"HubertModel.from_pretrained('facebook/hubert-large-ll60k')\", \"api_arguments\": \"pretrained model name\", \"python_environment_requirements\": \"transformers\", \"example_code\": \"hubert = HubertModel.from_pretrained('facebook/hubert-large-ll60k')\", \"performance\": {\"dataset\": \"Libri-Light\", \"accuracy\": \"matches or improves upon the state-of-the-art wav2vec 2.0 performance\"}, \"description\": \"Hubert-Large is a self-supervised speech representation learning model pretrained on 16kHz sampled speech audio. It is designed to deal with the unique problems in speech representation learning, such as multiple sound units in each input utterance, no lexicon of input sound units during the pre-training phase, and variable lengths of sound units with no explicit segmentation. The model relies on an offline clustering step to provide aligned target labels for a BERT-like prediction loss.\"}}\nWrite a python program in 1 to 2 lines to call API in huggingface.\n\nThe answer should follow the format: {<<<domain>>> $DOMAIN, <<<api_call>>>: $API_CALL, <<<api_provider>>>: $API_PROVIDER, <<<explanation>>>: $EXPLANATION, <<<code>>>: $CODE}. Here are the requirements:\n1. The $API_CALL should have only 1 line of code that calls api.\n2. The $API_PROVIDER should be the programming framework used.\n3. $EXPLANATION should be a step-by-step explanation.\n4. The $CODE is the python code.\n5. Do not repeat the format in your answer.", "category": "generic"}
{"question_id": 4, "text": " A chat service needs a way to compare and cluster similar sentences from users in different languages. Find a suitable feature extraction method to achieve this.\n \n Use this API documentation for reference:  {\"domain\": \"Multimodal Feature Extraction\", \"framework\": \"Hugging Face Transformers\", \"functionality\": \"Feature Extraction\", \"api_name\": \"rasa/LaBSE\", \"api_call\": \"AutoModel.from_pretrained('rasa/LaBSE')\", \"api_arguments\": \"input_text\", \"python_environment_requirements\": [\"transformers\"], \"example_code\": \"\", \"performance\": {\"dataset\": \"\", \"accuracy\": \"\"}, \"description\": \"LaBSE (Language-agnostic BERT Sentence Embedding) model for extracting sentence embeddings in multiple languages.\"}}\nWrite a python program in 1 to 2 lines to call API in huggingface.\n\nThe answer should follow the format: {<<<domain>>> $DOMAIN, <<<api_call>>>: $API_CALL, <<<api_provider>>>: $API_PROVIDER, <<<explanation>>>: $EXPLANATION, <<<code>>>: $CODE}. Here are the requirements:\n1. The $API_CALL should have only 1 line of code that calls api.\n2. The $API_PROVIDER should be the programming framework used.\n3. $EXPLANATION should be a step-by-step explanation.\n4. The $CODE is the python code.\n5. Do not repeat the format in your answer.", "category": "generic"}
{"question_id": 5, "text": " I am an interior designer and want to showcase a modern living room with a fireplace and a large window overlooking a forest. Create an image according to this description.\n \n Use this API documentation for reference:  {\"domain\": \"Multimodal Text-to-Image\", \"framework\": \"Hugging Face\", \"functionality\": \"Image generation and modification based on text prompts\", \"api_name\": \"stabilityai/stable-diffusion-2-inpainting\", \"api_call\": \"StableDiffusionInpaintPipeline.from_pretrained('stabilityai/stable-diffusion-2-inpainting', torch_dtype=torch.float16)\", \"api_arguments\": [\"prompt\", \"image\", \"mask_image\"], \"python_environment_requirements\": [\"diffusers\", \"transformers\", \"accelerate\", \"scipy\", \"safetensors\"], \"example_code\": \"from diffusers import StableDiffusionInpaintPipeline\npipe = StableDiffusionInpaintPipeline.from_pretrained('stabilityai/stable-diffusion-2-inpainting', torch_dtype=torch.float16)\npipe.to(cuda)\nprompt = Face of a yellow cat, high resolution, sitting on a park bench\nimage = pipe(prompt=prompt, image=image, mask_image=mask_image).images[0]\nimage.save(./yellow_cat_on_park_bench.png)\", \"performance\": {\"dataset\": \"COCO2017 validation set\", \"accuracy\": \"Not optimized for FID scores\"}, \"description\": \"A Latent Diffusion Model that uses a fixed, pretrained text encoder (OpenCLIP-ViT/H) to generate and modify images based on text prompts.\"}}\nWrite a python program in 1 to 2 lines to call API in huggingface.\n\nThe answer should follow the format: {<<<domain>>> $DOMAIN, <<<api_call>>>: $API_CALL, <<<api_provider>>>: $API_PROVIDER, <<<explanation>>>: $EXPLANATION, <<<code>>>: $CODE}. Here are the requirements:\n1. The $API_CALL should have only 1 line of code that calls api.\n2. The $API_PROVIDER should be the programming framework used.\n3. $EXPLANATION should be a step-by-step explanation.\n4. The $CODE is the python code.\n5. Do not repeat the format in your answer.", "category": "generic"}
{"question_id": 6, "text": " We need a product description for an image-based online store platform that will help customers understand the specifics of the product.\n \n Use this API documentation for reference:  {\"domain\": \"Multimodal Image-to-Text\", \"framework\": \"Hugging Face Transformers\", \"functionality\": \"Transformers\", \"api_name\": \"git-large-coco\", \"api_call\": \"GenerativeImage2TextModel.from_pretrained('microsoft/git-large-coco')\", \"api_arguments\": \"image, text\", \"python_environment_requirements\": \"transformers\", \"example_code\": \"For code examples, we refer to the documentation.\", \"performance\": {\"dataset\": \"COCO\", \"accuracy\": \"See table 11 in the paper for more details.\"}, \"description\": \"GIT (short for GenerativeImage2Text) model, large-sized version, fine-tuned on COCO. It was introduced in the paper GIT: A Generative Image-to-text Transformer for Vision and Language by Wang et al. and first released in this repository.\"}}\nWrite a python program in 1 to 2 lines to call API in huggingface.\n\nThe answer should follow the format: {<<<domain>>> $DOMAIN, <<<api_call>>>: $API_CALL, <<<api_provider>>>: $API_PROVIDER, <<<explanation>>>: $EXPLANATION, <<<code>>>: $CODE}. Here are the requirements:\n1. The $API_CALL should have only 1 line of code that calls api.\n2. The $API_PROVIDER should be the programming framework used.\n3. $EXPLANATION should be a step-by-step explanation.\n4. The $CODE is the python code.\n5. Do not repeat the format in your answer.", "category": "generic"}
{"question_id": 7, "text": " Create a program to generate a description for an image provided as input.\n \n Use this API documentation for reference:  {\"domain\": \"Multimodal Image-to-Text\", \"framework\": \"Hugging Face Transformers\", \"functionality\": \"Transformers\", \"api_name\": \"git-large-r-textcaps\", \"api_call\": \"pipeline('text-generation', model='microsoft/git-large-r-textcaps')\", \"api_arguments\": \"image\", \"python_environment_requirements\": \"transformers\", \"example_code\": \"\", \"performance\": {\"dataset\": \"TextCaps\", \"accuracy\": \"\"}, \"description\": \"GIT (short for GenerativeImage2Text) model, large-sized version, fine-tuned on TextCaps. It was introduced in the paper GIT: A Generative Image-to-text Transformer for Vision and Language by Wang et al. and first released in this repository. The model is trained using 'teacher forcing' on a lot of (image, text) pairs. The goal for the model is simply to predict the next text token, giving the image tokens and previous text tokens. This allows the model to be used for tasks like image and video captioning, visual question answering (VQA) on images and videos, and even image classification (by simply conditioning the model on the image and asking it to generate a class for it in text).\"}}\nWrite a python program in 1 to 2 lines to call API in huggingface.\n\nThe answer should follow the format: {<<<domain>>> $DOMAIN, <<<api_call>>>: $API_CALL, <<<api_provider>>>: $API_PROVIDER, <<<explanation>>>: $EXPLANATION, <<<code>>>: $CODE}. Here are the requirements:\n1. The $API_CALL should have only 1 line of code that calls api.\n2. The $API_PROVIDER should be the programming framework used.\n3. $EXPLANATION should be a step-by-step explanation.\n4. The $CODE is the python code.\n5. Do not repeat the format in your answer.", "category": "generic"}
{"question_id": 8, "text": " I am a financial analyst, and I receive report after report filled with charts helping to explain trends and data in my field. However, I also need to have this information in tabular format. Please help me extract a linearized table from this chart.\n \n Use this API documentation for reference:  {\"domain\": \"Multimodal Image-to-Text\", \"framework\": \"Hugging Face Transformers\", \"functionality\": \"Transformers\", \"api_name\": \"google/deplot\", \"api_call\": \"Pix2StructForConditionalGeneration.from_pretrained('google/deplot')\", \"api_arguments\": {\"images\": \"image\", \"text\": \"question\", \"return_tensors\": \"pt\", \"max_new_tokens\": 512}, \"python_environment_requirements\": {\"transformers\": \"Pix2StructForConditionalGeneration, Pix2StructProcessor\", \"requests\": \"requests\", \"PIL\": \"Image\"}, \"example_code\": \"from transformers import Pix2StructForConditionalGeneration, Pix2StructProcessor\nimport requests\nfrom PIL import Image\nmodel = Pix2StructForConditionalGeneration.from_pretrained('google/deplot')\nprocessor = Pix2StructProcessor.from_pretrained('google/deplot')\nurl = https://raw.githubusercontent.com/vis-nlp/ChartQA/main/ChartQA%20Dataset/val/png/5090.png\nimage = Image.open(requests.get(url, stream=True).raw)\ninputs = processor(images=image, text=Generate underlying data table of the figure below:, return_tensors=pt)\npredictions = model.generate(**inputs, max_new_tokens=512)\nprint(processor.decode(predictions[0], skip_special_tokens=True))\", \"performance\": {\"dataset\": \"ChartQA\", \"accuracy\": \"24.0% improvement over finetuned SOTA\"}, \"description\": \"DePlot is a model that translates the image of a plot or chart to a linearized table. It decomposes the challenge of visual language reasoning into two steps: (1) plot-to-text translation, and (2) reasoning over the translated text. The output of DePlot can then be directly used to prompt a pretrained large language model (LLM), exploiting the few-shot reasoning capabilities of LLMs.\"}}\nWrite a python program in 1 to 2 lines to call API in huggingface.\n\nThe answer should follow the format: {<<<domain>>> $DOMAIN, <<<api_call>>>: $API_CALL, <<<api_provider>>>: $API_PROVIDER, <<<explanation>>>: $EXPLANATION, <<<code>>>: $CODE}. Here are the requirements:\n1. The $API_CALL should have only 1 line of code that calls api.\n2. The $API_PROVIDER should be the programming framework used.\n3. $EXPLANATION should be a step-by-step explanation.\n4. The $CODE is the python code.\n5. Do not repeat the format in your answer.", "category": "generic"}
{"question_id": 9, "text": " We are building an automatic video generation platform based on user-provided text. We need a reliable model to convert text instructions into appropriate videos.\n \n Use this API documentation for reference:  {\"domain\": \"Multimodal Text-to-Video\", \"framework\": \"Hugging Face\", \"functionality\": \"Text-to-Video\", \"api_name\": \"ImRma/Brucelee\", \"api_call\": \"pipeline('text-to-video', model='ImRma/Brucelee')\", \"api_arguments\": [\"your_text\"], \"python_environment_requirements\": [\"transformers\"], \"example_code\": \"\", \"performance\": {\"dataset\": \"\", \"accuracy\": \"\"}, \"description\": \"A Hugging Face model for converting Persian and English text into video.\"}}\nWrite a python program in 1 to 2 lines to call API in huggingface.\n\nThe answer should follow the format: {<<<domain>>> $DOMAIN, <<<api_call>>>: $API_CALL, <<<api_provider>>>: $API_PROVIDER, <<<explanation>>>: $EXPLANATION, <<<code>>>: $CODE}. Here are the requirements:\n1. The $API_CALL should have only 1 line of code that calls api.\n2. The $API_PROVIDER should be the programming framework used.\n3. $EXPLANATION should be a step-by-step explanation.\n4. The $CODE is the python code.\n5. Do not repeat the format in your answer.", "category": "generic"}
{"question_id": 10, "text": " How can I extract video content from a text file? Provide a code sample to generate the video based on the text.\n \n Use this API documentation for reference:  {\"domain\": \"Multimodal Text-to-Video\", \"framework\": \"Hugging Face\", \"functionality\": \"Text-to-Video\", \"api_name\": \"duncan93/video\", \"api_call\": \"BaseModel.from_pretrained('duncan93/video')\", \"api_arguments\": \"\", \"python_environment_requirements\": \"Asteroid\", \"example_code\": \"\", \"performance\": {\"dataset\": \"OpenAssistant/oasst1\", \"accuracy\": \"\"}, \"description\": \"A text-to-video model trained on OpenAssistant/oasst1 dataset.\"}}\nWrite a python program in 1 to 2 lines to call API in huggingface.\n\nThe answer should follow the format: {<<<domain>>> $DOMAIN, <<<api_call>>>: $API_CALL, <<<api_provider>>>: $API_PROVIDER, <<<explanation>>>: $EXPLANATION, <<<code>>>: $CODE}. Here are the requirements:\n1. The $API_CALL should have only 1 line of code that calls api.\n2. The $API_PROVIDER should be the programming framework used.\n3. $EXPLANATION should be a step-by-step explanation.\n4. The $CODE is the python code.\n5. Do not repeat the format in your answer.", "category": "generic"}
{"question_id": 11, "text": " We are developing a mobile app to demonstrate the AI's ability to generate a short video from text. The app focuses on processing written stories into video.\n \n Use this API documentation for reference:  {\"domain\": \"Multimodal Text-to-Video\", \"framework\": \"Hugging Face\", \"functionality\": \"Text-to-Video Synthesis\", \"api_name\": \"damo-vilab/text-to-video-ms-1.7b-legacy\", \"api_call\": \"DiffusionPipeline.from_pretrained('damo-vilab/text-to-video-ms-1.7b-legacy', torch_dtype=torch.float16)\", \"api_arguments\": [\"prompt\", \"num_inference_steps\"], \"python_environment_requirements\": [\"diffusers\", \"transformers\", \"accelerate\"], \"example_code\": \"import torch\nfrom diffusers import DiffusionPipeline, DPMSolverMultistepScheduler\nfrom diffusers.utils import export_to_video\npipe = DiffusionPipeline.from_pretrained(damo-vilab/text-to-video-ms-1.7b-legacy, torch_dtype=torch.float16)\npipe.scheduler = DPMSolverMultistepScheduler.from_config(pipe.scheduler.config)\npipe.enable_model_cpu_offload()\nprompt = Spiderman is surfing\nvideo_frames = pipe(prompt, num_inference_steps=25).frames\nvideo_path = export_to_video(video_frames)\", \"performance\": {\"dataset\": [\"LAION5B\", \"ImageNet\", \"Webvid\"], \"accuracy\": \"Not provided\"}, \"description\": \"This model is based on a multi-stage text-to-video generation diffusion model, which inputs a description text and returns a video that matches the text description. Only English input is supported.\"}}\nWrite a python program in 1 to 2 lines to call API in huggingface.\n\nThe answer should follow the format: {<<<domain>>> $DOMAIN, <<<api_call>>>: $API_CALL, <<<api_provider>>>: $API_PROVIDER, <<<explanation>>>: $EXPLANATION, <<<code>>>: $CODE}. Here are the requirements:\n1. The $API_CALL should have only 1 line of code that calls api.\n2. The $API_PROVIDER should be the programming framework used.\n3. $EXPLANATION should be a step-by-step explanation.\n4. The $CODE is the python code.\n5. Do not repeat the format in your answer.", "category": "generic"}
{"question_id": 12, "text": " Hey, I want to analyze images in my phone gallery and answer questions about them.\n \n Use this API documentation for reference:  {\"domain\": \"Multimodal Visual Question Answering\", \"framework\": \"Hugging Face Transformers\", \"functionality\": \"Transformers\", \"api_name\": \"microsoft/git-base-vqav2\", \"api_call\": \"pipeline('visual-question-answering', model='microsoft/git-base-vqav2')\", \"api_arguments\": \"image, question\", \"python_environment_requirements\": [\"transformers\"], \"example_code\": \"vqa(image='path/to/image.jpg', question='What is in the image?')\", \"performance\": {\"dataset\": \"VQAv2\", \"accuracy\": \"Refer to the paper for evaluation results\"}, \"description\": \"GIT (short for GenerativeImage2Text) model, base-sized version, fine-tuned on VQAv2. It was introduced in the paper GIT: A Generative Image-to-text Transformer for Vision and Language by Wang et al. and first released in this repository.\"}}\nWrite a python program in 1 to 2 lines to call API in huggingface.\n\nThe answer should follow the format: {<<<domain>>> $DOMAIN, <<<api_call>>>: $API_CALL, <<<api_provider>>>: $API_PROVIDER, <<<explanation>>>: $EXPLANATION, <<<code>>>: $CODE}. Here are the requirements:\n1. The $API_CALL should have only 1 line of code that calls api.\n2. The $API_PROVIDER should be the programming framework used.\n3. $EXPLANATION should be a step-by-step explanation.\n4. The $CODE is the python code.\n5. Do not repeat the format in your answer.", "category": "generic"}
{"question_id": 13, "text": " My company wants to develop an application that will analyze images in relation to food and answer questions about them. We want it to handle questions like \\\"what is in the dish\\\" and \\\"how many calories does it have\\\".\n \n Use this API documentation for reference:  {\"domain\": \"Multimodal Visual Question Answering\", \"framework\": \"Hugging Face Transformers\", \"functionality\": \"Transformers\", \"api_name\": \"azwierzc/vilt-b32-finetuned-vqa-pl\", \"api_call\": \"pipeline('visual-question-answering', model='azwierzc/vilt-b32-finetuned-vqa-pl')\", \"api_arguments\": {\"image\": \"path_to_image\", \"question\": \"question_text\"}, \"python_environment_requirements\": [\"transformers\"], \"example_code\": \"\", \"performance\": {\"dataset\": \"\", \"accuracy\": \"\"}, \"description\": \"A Visual Question Answering model fine-tuned on the Polish language.\"}}\nWrite a python program in 1 to 2 lines to call API in huggingface.\n\nThe answer should follow the format: {<<<domain>>> $DOMAIN, <<<api_call>>>: $API_CALL, <<<api_provider>>>: $API_PROVIDER, <<<explanation>>>: $EXPLANATION, <<<code>>>: $CODE}. Here are the requirements:\n1. The $API_CALL should have only 1 line of code that calls api.\n2. The $API_PROVIDER should be the programming framework used.\n3. $EXPLANATION should be a step-by-step explanation.\n4. The $CODE is the python code.\n5. Do not repeat the format in your answer.", "category": "generic"}
{"question_id": 14, "text": " We have received an invoice document, and would like to extract the total amount from it.\n###Input: {'question': 'What is the total amount?', 'context': 'Invoice information for order ABC_123\\\nProduct: Widget A, Quantity: 10, Price: $5 each\\\nProduct: Widget B, Quantity: 5, Price: $3 each\\\nProduct: Widget C, Quantity: 15, Price: $2 each\\\nSubtotal: $75, Tax: $6.38, Total Amount Due: $81.38'}\n \n Use this API documentation for reference:  {\"domain\": \"Multimodal Document Question Answer\", \"framework\": \"Hugging Face Transformers\", \"functionality\": \"Transformers\", \"api_name\": \"layoutlm-invoices\", \"api_call\": \"AutoModelForDocumentQuestionAnswering.from_pretrained('impira/layoutlm-invoices')\", \"api_arguments\": \"question, context\", \"python_environment_requirements\": \"transformers\", \"example_code\": \"nlp(question='What is the total amount?', context='your_invoice_text')\", \"performance\": {\"dataset\": \"proprietary dataset of invoices, SQuAD2.0, and DocVQA\", \"accuracy\": \"Not provided\"}, \"description\": \"A fine-tuned version of the multi-modal LayoutLM model for the task of question answering on invoices and other documents. It has been fine-tuned on a proprietary dataset of invoices as well as both SQuAD2.0 and DocVQA for general comprehension. Unlike other QA models, which can only extract consecutive tokens, this model can predict longer-range, non-consecutive sequences with an additional classifier head.\"}}\nWrite a python program in 1 to 2 lines to call API in huggingface.\n\nThe answer should follow the format: {<<<domain>>> $DOMAIN, <<<api_call>>>: $API_CALL, <<<api_provider>>>: $API_PROVIDER, <<<explanation>>>: $EXPLANATION, <<<code>>>: $CODE}. Here are the requirements:\n1. The $API_CALL should have only 1 line of code that calls api.\n2. The $API_PROVIDER should be the programming framework used.\n3. $EXPLANATION should be a step-by-step explanation.\n4. The $CODE is the python code.\n5. Do not repeat the format in your answer.", "category": "generic"}
{"question_id": 15, "text": " As a clerk in a school, you want to extract information from some student enrollment forms. These forms contain students' details such as Name, age, and address.\n \n Use this API documentation for reference:  {\"domain\": \"Multimodal Document Question Answer\", \"framework\": \"Transformers\", \"functionality\": \"Document Question Answering\", \"api_name\": \"tiny-random-LayoutLMv3ForQuestionAnswering\", \"api_call\": \"LayoutLMv3ForQuestionAnswering.from_pretrained('hf-tiny-model-private/tiny-random-LayoutLMv3ForQuestionAnswering')\", \"api_arguments\": {\"image\": \"path/to/image/file\"}, \"python_environment_requirements\": [\"transformers\", \"torch\", \"tensorflow\"], \"example_code\": \"\", \"performance\": {\"dataset\": \"\", \"accuracy\": \"\"}, \"description\": \"A tiny random LayoutLMv3 model for document question answering. Can be used with the Hugging Face Inference API.\"}}\nWrite a python program in 1 to 2 lines to call API in huggingface.\n\nThe answer should follow the format: {<<<domain>>> $DOMAIN, <<<api_call>>>: $API_CALL, <<<api_provider>>>: $API_PROVIDER, <<<explanation>>>: $EXPLANATION, <<<code>>>: $CODE}. Here are the requirements:\n1. The $API_CALL should have only 1 line of code that calls api.\n2. The $API_PROVIDER should be the programming framework used.\n3. $EXPLANATION should be a step-by-step explanation.\n4. The $CODE is the python code.\n5. Do not repeat the format in your answer.", "category": "generic"}
{"question_id": 16, "text": " Find a model that can be used to predict the properties of molecules based on their graph representations.\n \n Use this API documentation for reference:  {\"domain\": \"Multimodal Graph Machine Learning\", \"framework\": \"Hugging Face Transformers\", \"functionality\": \"Transformers\", \"api_name\": \"graphormer-base-pcqm4mv1\", \"api_call\": \"AutoModel.from_pretrained('graphormer-base-pcqm4mv1')\", \"api_arguments\": [\"model_name\"], \"python_environment_requirements\": [\"transformers\"], \"example_code\": \"See the Graph Classification with Transformers tutorial\", \"performance\": {\"dataset\": \"PCQM4M-LSC\", \"accuracy\": \"1st place on the KDD CUP 2021 (quantum prediction track)\"}, \"description\": \"The Graphormer is a graph Transformer model, pretrained on PCQM4M-LSC, and which got 1st place on the KDD CUP 2021 (quantum prediction track). Developed by Microsoft, this model should be used for graph classification tasks or graph representation tasks; the most likely associated task is molecule modeling. It can either be used as such, or finetuned on downstream tasks.\"}}\nWrite a python program in 1 to 2 lines to call API in huggingface.\n\nThe answer should follow the format: {<<<domain>>> $DOMAIN, <<<api_call>>>: $API_CALL, <<<api_provider>>>: $API_PROVIDER, <<<explanation>>>: $EXPLANATION, <<<code>>>: $CODE}. Here are the requirements:\n1. The $API_CALL should have only 1 line of code that calls api.\n2. The $API_PROVIDER should be the programming framework used.\n3. $EXPLANATION should be a step-by-step explanation.\n4. The $CODE is the python code.\n5. Do not repeat the format in your answer.", "category": "generic"}
{"question_id": 17, "text": " Estimate the depth of a pool using computational depth estimation, given an underwater photo.\n \n Use this API documentation for reference:  {\"domain\": \"Computer Vision Depth Estimation\", \"framework\": \"Hugging Face Transformers\", \"functionality\": \"Transformers\", \"api_name\": \"hf-tiny-model-private/tiny-random-GLPNForDepthEstimation\", \"api_call\": \"AutoModel.from_pretrained('hf-tiny-model-private/tiny-random-GLPNForDepthEstimation')\", \"api_arguments\": [], \"python_environment_requirements\": [\"torch\", \"transformers\"], \"example_code\": \"\", \"performance\": {\"dataset\": \"\", \"accuracy\": \"\"}, \"description\": \"A tiny random GLPN model for depth estimation using the Hugging Face Transformers library.\"}}\nWrite a python program in 1 to 2 lines to call API in huggingface.\n\nThe answer should follow the format: {<<<domain>>> $DOMAIN, <<<api_call>>>: $API_CALL, <<<api_provider>>>: $API_PROVIDER, <<<explanation>>>: $EXPLANATION, <<<code>>>: $CODE}. Here are the requirements:\n1. The $API_CALL should have only 1 line of code that calls api.\n2. The $API_PROVIDER should be the programming framework used.\n3. $EXPLANATION should be a step-by-step explanation.\n4. The $CODE is the python code.\n5. Do not repeat the format in your answer.", "category": "generic"}
{"question_id": 18, "text": " I need technology that can analyze images and estimate their depth in a single camera.\n \n Use this API documentation for reference:  {\"domain\": \"Computer Vision Depth Estimation\", \"framework\": \"Hugging Face Transformers\", \"functionality\": \"Monocular Depth Estimation\", \"api_name\": \"Intel/dpt-large\", \"api_call\": \"DPTForDepthEstimation.from_pretrained('Intel/dpt-large')\", \"api_arguments\": {\"pretrained_model_name_or_path\": \"Intel/dpt-large\"}, \"python_environment_requirements\": [\"transformers\"], \"example_code\": \"from transformers import DPTImageProcessor, DPTForDepthEstimation\nimport torch\nimport numpy as np\nfrom PIL import Image\nimport requests\nurl = http://images.cocodataset.org/val2017/000000039769.jpg\nimage = Image.open(requests.get(url, stream=True).raw)\nprocessor = DPTImageProcessor.from_pretrained(Intel/dpt-large)\nmodel = DPTForDepthEstimation.from_pretrained(Intel/dpt-large)\ninputs = processor(images=image, return_tensors=pt)\nwith torch.no_grad():\n outputs = model(**inputs)\n predicted_depth = outputs.predicted_depth\nprediction = torch.nn.functional.interpolate(\n predicted_depth.unsqueeze(1),\n size=image.size[::-1],\n mode=bicubic,\n align_corners=False,\n)\noutput = prediction.squeeze().cpu().numpy()\nformatted = (output * 255 / np.max(output)).astype(uint8)\ndepth = Image.fromarray(formatted)\", \"performance\": {\"dataset\": \"MIX 6\", \"accuracy\": \"10.82\"}, \"description\": \"Dense Prediction Transformer (DPT) model trained on 1.4 million images for monocular depth estimation. Introduced in the paper Vision Transformers for Dense Prediction by Ranftl et al. (2021). DPT uses the Vision Transformer (ViT) as backbone and adds a neck + head on top for monocular depth estimation.\"}}\nWrite a python program in 1 to 2 lines to call API in huggingface.\n\nThe answer should follow the format: {<<<domain>>> $DOMAIN, <<<api_call>>>: $API_CALL, <<<api_provider>>>: $API_PROVIDER, <<<explanation>>>: $EXPLANATION, <<<code>>>: $CODE}. Here are the requirements:\n1. The $API_CALL should have only 1 line of code that calls api.\n2. The $API_PROVIDER should be the programming framework used.\n3. $EXPLANATION should be a step-by-step explanation.\n4. The $CODE is the python code.\n5. Do not repeat the format in your answer.", "category": "generic"}
{"question_id": 19, "text": " The client is a real estate company working on virtual tours. We need to help them estimate depth in images of houses.\n \n Use this API documentation for reference:  {\"domain\": \"Computer Vision Depth Estimation\", \"framework\": \"Hugging Face Transformers\", \"functionality\": \"Transformers\", \"api_name\": \"glpn-kitti-finetuned-diode-221214-123047\", \"api_call\": \"pipeline('depth-estimation', model='sayakpaul/glpn-kitti-finetuned-diode-221214-123047')\", \"api_arguments\": [], \"python_environment_requirements\": [\"transformers==4.24.0\", \"torch==1.12.1+cu116\", \"tokenizers==0.13.2\"], \"example_code\": \"\", \"performance\": {\"dataset\": \"diode-subset\", \"accuracy\": {\"Loss\": 0.3497, \"Mae\": 0.2847, \"Rmse\": 0.3977, \"Abs Rel\": 0.3477, \"Log Mae\": 0.1203, \"Log Rmse\": 0.1726, \"Delta1\": 0.5217, \"Delta2\": 0.8246, \"Delta3\": 0.9436}}, \"description\": \"This model is a fine-tuned version of vinvino02/glpn-kitti on the diode-subset dataset. It is used for depth estimation in computer vision applications.\"}}\nWrite a python program in 1 to 2 lines to call API in huggingface.\n\nThe answer should follow the format: {<<<domain>>> $DOMAIN, <<<api_call>>>: $API_CALL, <<<api_provider>>>: $API_PROVIDER, <<<explanation>>>: $EXPLANATION, <<<code>>>: $CODE}. Here are the requirements:\n1. The $API_CALL should have only 1 line of code that calls api.\n2. The $API_PROVIDER should be the programming framework used.\n3. $EXPLANATION should be a step-by-step explanation.\n4. The $CODE is the python code.\n5. Do not repeat the format in your answer.", "category": "generic"}
{"question_id": 20, "text": " Assist me in setting up an image classifier that can recognize objects within an image.\n \n Use this API documentation for reference:  {\"domain\": \"Computer Vision Image Classification\", \"framework\": \"Hugging Face Transformers\", \"functionality\": \"Image Classification\", \"api_name\": \"timm/mobilenetv3_large_100.ra_in1k\", \"api_call\": \"timm.create_model('mobilenetv3_large_100.ra_in1k', pretrained=True)\", \"api_arguments\": {\"pretrained\": \"True\"}, \"python_environment_requirements\": {\"timm\": \"latest\"}, \"example_code\": \"from urllib.request import urlopen\nfrom PIL import Image\nimport timm\nimg = Image.open(urlopen(\n 'https://huggingface.co/datasets/huggingface/documentation-images/resolve/main/beignets-task-guide.png'\n))\nmodel = timm.create_model('mobilenetv3_large_100.ra_in1k', pretrained=True)\nmodel = model.eval()\ndata_config = timm.data.resolve_model_data_config(model)\ntransforms = timm.data.create_transform(**data_config, is_training=False)\noutput = model(transforms(img).unsqueeze(0))\", \"performance\": {\"dataset\": \"imagenet-1k\", \"accuracy\": \"Not provided\"}, \"description\": \"A MobileNet-v3 image classification model. Trained on ImageNet-1k in timm using recipe template described below. Recipe details: RandAugment RA recipe. Inspired by and evolved from EfficientNet RandAugment recipes. Published as B recipe in ResNet Strikes Back. RMSProp (TF 1.0 behaviour) optimizer, EMA weight averaging. Step (exponential decay w/ staircase) LR schedule with warmup.\"}}\nWrite a python program in 1 to 2 lines to call API in huggingface.\n\nThe answer should follow the format: {<<<domain>>> $DOMAIN, <<<api_call>>>: $API_CALL, <<<api_provider>>>: $API_PROVIDER, <<<explanation>>>: $EXPLANATION, <<<code>>>: $CODE}. Here are the requirements:\n1. The $API_CALL should have only 1 line of code that calls api.\n2. The $API_PROVIDER should be the programming framework used.\n3. $EXPLANATION should be a step-by-step explanation.\n4. The $CODE is the python code.\n5. Do not repeat the format in your answer.", "category": "generic"}
{"question_id": 21, "text": " Identify an object within an image based on textual description. For example, find a dog in the image.\n \n Use this API documentation for reference:  {\"domain\": \"Computer Vision Object Detection\", \"framework\": \"Hugging Face Transformers\", \"functionality\": \"zero-shot-object-detection\", \"api_name\": \"google/owlvit-base-patch32\", \"api_call\": \"OwlViTForObjectDetection.from_pretrained('google/owlvit-base-patch32')\", \"api_arguments\": {\"texts\": \"List of text queries\", \"images\": \"Image to be processed\"}, \"python_environment_requirements\": \"transformers\", \"example_code\": \"import requests\nfrom PIL import Image\nimport torch\nfrom transformers import OwlViTProcessor, OwlViTForObjectDetection\nprocessor = OwlViTProcessor.from_pretrained(google/owlvit-base-patch32)\nmodel = OwlViTForObjectDetection.from_pretrained(google/owlvit-base-patch32)\nurl = http://images.cocodataset.org/val2017/000000039769.jpg\nimage = Image.open(requests.get(url, stream=True).raw)\ntexts = [[a photo of a cat, a photo of a dog]]\ninputs = processor(text=texts, images=image, return_tensors=pt)\noutputs = model(**inputs)\ntarget_sizes = torch.Tensor([image.size[::-1]])\nresults = processor.post_process(outputs=outputs, target_sizes=target_sizes)\", \"performance\": {\"dataset\": \"COCO and OpenImages\", \"accuracy\": \"Not specified\"}, \"description\": \"OWL-ViT is a zero-shot text-conditioned object detection model that uses CLIP as its multi-modal backbone, with a ViT-like Transformer to get visual features and a causal language model to get the text features. The model can be used to query an image with one or multiple text queries.\"}}\nWrite a python program in 1 to 2 lines to call API in huggingface.\n\nThe answer should follow the format: {<<<domain>>> $DOMAIN, <<<api_call>>>: $API_CALL, <<<api_provider>>>: $API_PROVIDER, <<<explanation>>>: $EXPLANATION, <<<code>>>: $CODE}. Here are the requirements:\n1. The $API_CALL should have only 1 line of code that calls api.\n2. The $API_PROVIDER should be the programming framework used.\n3. $EXPLANATION should be a step-by-step explanation.\n4. The $CODE is the python code.\n5. Do not repeat the format in your answer.", "category": "generic"}
{"question_id": 22, "text": " Our client is an AI gaming company and we need to develop a bot for the game Valorant. The bot should detect objects like dropped spike, enemy, planted spike, and teammate within the game.\n \n Use this API documentation for reference:  {\"domain\": \"Computer Vision Object Detection\", \"framework\": \"Hugging Face Transformers\", \"functionality\": \"Object Detection\", \"api_name\": \"keremberke/yolov8m-valorant-detection\", \"api_call\": \"YOLO('keremberke/yolov8m-valorant-detection')\", \"api_arguments\": {\"conf\": 0.25, \"iou\": 0.45, \"agnostic_nms\": false, \"max_det\": 1000}, \"python_environment_requirements\": \"pip install ultralyticsplus==0.0.23 ultralytics==8.0.21\", \"example_code\": \"from ultralyticsplus import YOLO, render_result\nmodel = YOLO('keremberke/yolov8m-valorant-detection')\nmodel.overrides['conf'] = 0.25\nmodel.overrides['iou'] = 0.45\nmodel.overrides['agnostic_nms'] = False\nmodel.overrides['max_det'] = 1000\nimage = 'https://github.com/ultralytics/yolov5/raw/master/data/images/zidane.jpg'\nresults = model.predict(image)\nprint(results[0].boxes)\nrender = render_result(model=model, image=image, result=results[0])\nrender.show()\", \"performance\": {\"dataset\": \"valorant-object-detection\", \"accuracy\": 0.965}, \"description\": \"A YOLOv8 model for object detection in Valorant game, trained on a custom dataset. It detects dropped spike, enemy, planted spike, and teammate objects.\"}}\nWrite a python program in 1 to 2 lines to call API in huggingface.\n\nThe answer should follow the format: {<<<domain>>> $DOMAIN, <<<api_call>>>: $API_CALL, <<<api_provider>>>: $API_PROVIDER, <<<explanation>>>: $EXPLANATION, <<<code>>>: $CODE}. Here are the requirements:\n1. The $API_CALL should have only 1 line of code that calls api.\n2. The $API_PROVIDER should be the programming framework used.\n3. $EXPLANATION should be a step-by-step explanation.\n4. The $CODE is the python code.\n5. Do not repeat the format in your answer.", "category": "generic"}
{"question_id": 23, "text": " A client from real estate agency needs to get a list of objects present in a series of pictures to prepare their property listings.\n \n Use this API documentation for reference:  {\"domain\": \"Computer Vision Object Detection\", \"framework\": \"Hugging Face Transformers\", \"functionality\": \"zero-shot-object-detection\", \"api_name\": \"google/owlvit-base-patch16\", \"api_call\": \"OwlViTForObjectDetection.from_pretrained('google/owlvit-base-patch16')\", \"api_arguments\": [\"texts\", \"images\"], \"python_environment_requirements\": [\"requests\", \"PIL\", \"torch\", \"transformers\"], \"example_code\": \"processor = OwlViTProcessor.from_pretrained(google/owlvit-base-patch16)\nmodel = OwlViTForObjectDetection.from_pretrained(google/owlvit-base-patch16)\nurl = http://images.cocodataset.org/val2017/000000039769.jpg\nimage = Image.open(requests.get(url, stream=True).raw)\ntexts = [[a photo of a cat, a photo of a dog]]\ninputs = processor(text=texts, images=image, return_tensors=pt)\noutputs = model(**inputs)\ntarget_sizes = torch.Tensor([image.size[::-1]])\nresults = processor.post_process(outputs=outputs, target_sizes=target_sizes)\", \"performance\": {\"dataset\": \"COCO\", \"accuracy\": \"Not provided\"}, \"description\": \"OWL-ViT is a zero-shot text-conditioned object detection model that can be used to query an image with one or multiple text queries. OWL-ViT uses CLIP as its multi-modal backbone, with a ViT-like Transformer to get visual features and a causal language model to get the text features.\"}}\nWrite a python program in 1 to 2 lines to call API in huggingface.\n\nThe answer should follow the format: {<<<domain>>> $DOMAIN, <<<api_call>>>: $API_CALL, <<<api_provider>>>: $API_PROVIDER, <<<explanation>>>: $EXPLANATION, <<<code>>>: $CODE}. Here are the requirements:\n1. The $API_CALL should have only 1 line of code that calls api.\n2. The $API_PROVIDER should be the programming framework used.\n3. $EXPLANATION should be a step-by-step explanation.\n4. The $CODE is the python code.\n5. Do not repeat the format in your answer.", "category": "generic"}
{"question_id": 24, "text": " We are developing an application for smartphones which automatically separates elements in a user's photo, and we need to implement this feature.\n \n Use this API documentation for reference:  {\"domain\": \"Computer Vision Image Segmentation\", \"framework\": \"Hugging Face Transformers\", \"functionality\": \"Image Segmentation\", \"api_name\": \"facebook/detr-resnet-50-panoptic\", \"api_call\": \"DetrForSegmentation.from_pretrained('facebook/detr-resnet-50-panoptic')\", \"api_arguments\": [\"image\"], \"python_environment_requirements\": [\"torch\", \"numpy\", \"transformers\", \"PIL\", \"requests\", \"io\"], \"example_code\": [\"import io\", \"import requests\", \"from PIL import Image\", \"import torch\", \"import numpy\", \"from transformers import DetrFeatureExtractor, DetrForSegmentation\", \"from transformers.models.detr.feature_extraction_detr import rgb_to_id\", \"url = 'http://images.cocodataset.org/val2017/000000039769.jpg'\", \"image = Image.open(requests.get(url, stream=True).raw)\", \"feature_extractor = DetrFeatureExtractor.from_pretrained('facebook/detr-resnet-50-panoptic')\", \"model = DetrForSegmentation.from_pretrained('facebook/detr-resnet-50-panoptic')\", \"inputs = feature_extractor(images=image, return_tensors='pt')\", \"outputs = model(**inputs)\", \"processed_sizes = torch.as_tensor(inputs['pixel_values'].shape[-2:]).unsqueeze(0)\", \"result = feature_extractor.post_process_panoptic(outputs, processed_sizes)[0]\", \"panoptic_seg = Image.open(io.BytesIO(result['png_string']))\", \"panoptic_seg = numpy.array(panoptic_seg, dtype=numpy.uint8)\", \"panoptic_seg_id = rgb_to_id(panoptic_seg)\"], \"performance\": {\"dataset\": \"COCO 2017 validation\", \"accuracy\": {\"box_AP\": 38.8, \"segmentation_AP\": 31.1, \"PQ\": 43.4}}, \"description\": \"DEtection TRansformer (DETR) model trained end-to-end on COCO 2017 panoptic (118k annotated images). It was introduced in the paper End-to-End Object Detection with Transformers by Carion et al. and first released in this repository.\"}}\nWrite a python program in 1 to 2 lines to call API in huggingface.\n\nThe answer should follow the format: {<<<domain>>> $DOMAIN, <<<api_call>>>: $API_CALL, <<<api_provider>>>: $API_PROVIDER, <<<explanation>>>: $EXPLANATION, <<<code>>>: $CODE}. Here are the requirements:\n1. The $API_CALL should have only 1 line of code that calls api.\n2. The $API_PROVIDER should be the programming framework used.\n3. $EXPLANATION should be a step-by-step explanation.\n4. The $CODE is the python code.\n5. Do not repeat the format in your answer.", "category": "generic"}
{"question_id": 25, "text": " I have a picture of a room demonstrating a mixture of objects. The model needs to seperate the objects and label them accordingly.\n \n Use this API documentation for reference:  {\"domain\": \"Computer Vision Image Segmentation\", \"framework\": \"Hugging Face Transformers\", \"functionality\": \"Transformers\", \"api_name\": \"facebook/maskformer-swin-large-ade\", \"api_call\": \"MaskFormerForInstanceSegmentation.from_pretrained('facebook/maskformer-swin-large-ade')\", \"api_arguments\": {\"from_pretrained\": \"facebook/maskformer-swin-large-ade\"}, \"python_environment_requirements\": {\"packages\": [\"transformers\", \"PIL\", \"requests\"]}, \"example_code\": \"from transformers import MaskFormerImageProcessor, MaskFormerForInstanceSegmentation\nfrom PIL import Image\nimport requests\nurl = 'https://huggingface.co/datasets/hf-internal-testing/fixtures_ade20k/resolve/main/ADE_val_00000001.jpg'\nimage = Image.open(requests.get(url, stream=True).raw)\nprocessor = MaskFormerImageProcessor.from_pretrained('facebook/maskformer-swin-large-ade')\ninputs = processor(images=image, return_tensors='pt')\nmodel = MaskFormerForInstanceSegmentation.from_pretrained('facebook/maskformer-swin-large-ade')\noutputs = model(**inputs)\nclass_queries_logits = outputs.class_queries_logits\nmasks_queries_logits = outputs.masks_queries_logits\npredicted_semantic_map = processor.post_process_semantic_segmentation(outputs, target_sizes=[image.size[::-1]])[0]\", \"performance\": {\"dataset\": \"ADE20k\", \"accuracy\": \"Not provided\"}, \"description\": \"MaskFormer model trained on ADE20k semantic segmentation (large-sized version, Swin backbone). It was introduced in the paper Per-Pixel Classification is Not All You Need for Semantic Segmentation and first released in this repository. This model addresses instance, semantic and panoptic segmentation with the same paradigm: by predicting a set of masks and corresponding labels. Hence, all 3 tasks are treated as if they were instance segmentation.\"}}\nWrite a python program in 1 to 2 lines to call API in huggingface.\n\nThe answer should follow the format: {<<<domain>>> $DOMAIN, <<<api_call>>>: $API_CALL, <<<api_provider>>>: $API_PROVIDER, <<<explanation>>>: $EXPLANATION, <<<code>>>: $CODE}. Here are the requirements:\n1. The $API_CALL should have only 1 line of code that calls api.\n2. The $API_PROVIDER should be the programming framework used.\n3. $EXPLANATION should be a step-by-step explanation.\n4. The $CODE is the python code.\n5. Do not repeat the format in your answer.", "category": "generic"}
{"question_id": 26, "text": " We want to randomly generate high-quality images of celebrity faces.\n \n Use this API documentation for reference:  {\"domain\": \"Computer Vision Unconditional Image Generation\", \"framework\": \"Hugging Face Transformers\", \"functionality\": \"Denoising Diffusion Probabilistic Models (DDPM)\", \"api_name\": \"google/ddpm-ema-celebahq-256\", \"api_call\": \"DDPMPipeline.from_pretrained('google/ddpm-ema-celebahq-256')\", \"api_arguments\": {\"model_id\": \"google/ddpm-ema-celebahq-256\"}, \"python_environment_requirements\": \"diffusers\", \"example_code\": \"!pip install diffusers\nfrom diffusers import DDPMPipeline, DDIMPipeline, PNDMPipeline\nmodel_id = google/ddpm-ema-celebahq-256\nddpm = DDPMPipeline.from_pretrained(model_id)\nimage = ddpm().images[0]\nimage.save(ddpm_generated_image.png)\", \"performance\": {\"dataset\": {\"CIFAR10\": {\"Inception_score\": 9.46, \"FID_score\": 3.17}, \"LSUN\": {\"sample_quality\": \"similar to ProgressiveGAN\"}}}, \"description\": \"High quality image synthesis using diffusion probabilistic models, a class of latent variable models inspired by considerations from nonequilibrium thermodynamics.\"}}\nWrite a python program in 1 to 2 lines to call API in huggingface.\n\nThe answer should follow the format: {<<<domain>>> $DOMAIN, <<<api_call>>>: $API_CALL, <<<api_provider>>>: $API_PROVIDER, <<<explanation>>>: $EXPLANATION, <<<code>>>: $CODE}. Here are the requirements:\n1. The $API_CALL should have only 1 line of code that calls api.\n2. The $API_PROVIDER should be the programming framework used.\n3. $EXPLANATION should be a step-by-step explanation.\n4. The $CODE is the python code.\n5. Do not repeat the format in your answer.", "category": "generic"}
{"question_id": 27, "text": " Generate a new image based on the online database of bedroom art.\n \n Use this API documentation for reference:  {\"domain\": \"Computer Vision Unconditional Image Generation\", \"framework\": \"Hugging Face Transformers\", \"functionality\": \"Unconditional Image Generation\", \"api_name\": \"johnowhitaker/sd-class-wikiart-from-bedrooms\", \"api_call\": \"DDPMPipeline.from_pretrained('johnowhitaker/sd-class-wikiart-from-bedrooms')\", \"api_arguments\": \"\", \"python_environment_requirements\": \"diffusers\", \"example_code\": \"from diffusers import DDPMPipeline\npipeline = DDPMPipeline.from_pretrained('johnowhitaker/sd-class-wikiart-from-bedrooms')\nimage = pipeline().images[0]\nimage\", \"performance\": {\"dataset\": \"https://huggingface.co/datasets/huggan/wikiart\", \"accuracy\": \"Not provided\"}, \"description\": \"This model is a diffusion model initialized from https://huggingface.co/google/ddpm-bedroom-256 and trained for 5000 steps on https://huggingface.co/datasets/huggan/wikiart.\"}}\nWrite a python program in 1 to 2 lines to call API in huggingface.\n\nThe answer should follow the format: {<<<domain>>> $DOMAIN, <<<api_call>>>: $API_CALL, <<<api_provider>>>: $API_PROVIDER, <<<explanation>>>: $EXPLANATION, <<<code>>>: $CODE}. Here are the requirements:\n1. The $API_CALL should have only 1 line of code that calls api.\n2. The $API_PROVIDER should be the programming framework used.\n3. $EXPLANATION should be a step-by-step explanation.\n4. The $CODE is the python code.\n5. Do not repeat the format in your answer.", "category": "generic"}
{"question_id": 28, "text": " I run an online store that sells butterfly-themed products. Please generate an image of a cute butterfly for our social media page.\n \n Use this API documentation for reference:  {\"domain\": \"Computer Vision Unconditional Image Generation\", \"framework\": \"Hugging Face Transformers\", \"functionality\": \"Unconditional Image Generation\", \"api_name\": \"sd-class-butterflies-32\", \"api_call\": \"DDPMPipeline.from_pretrained('clp/sd-class-butterflies-32')\", \"api_arguments\": {\"model_id\": \"clp/sd-class-butterflies-32\"}, \"python_environment_requirements\": [\"diffusers\"], \"example_code\": \"from diffusers import DDPMPipeline\npipeline = DDPMPipeline.from_pretrained('clp/sd-class-butterflies-32')\nimage = pipeline().images[0]\nimage\", \"performance\": {\"dataset\": null, \"accuracy\": null}, \"description\": \"This model is a diffusion model for unconditional image generation of cute butterflies.\"}}\nWrite a python program in 1 to 2 lines to call API in huggingface.\n\nThe answer should follow the format: {<<<domain>>> $DOMAIN, <<<api_call>>>: $API_CALL, <<<api_provider>>>: $API_PROVIDER, <<<explanation>>>: $EXPLANATION, <<<code>>>: $CODE}. Here are the requirements:\n1. The $API_CALL should have only 1 line of code that calls api.\n2. The $API_PROVIDER should be the programming framework used.\n3. $EXPLANATION should be a step-by-step explanation.\n4. The $CODE is the python code.\n5. Do not repeat the format in your answer.", "category": "generic"}
{"question_id": 29, "text": " We need a video-based AI model for security purposes. We want the AI to check and categorize footage based on existing security guidelines.\n \n Use this API documentation for reference:  {\"domain\": \"Computer Vision Video Classification\", \"framework\": \"Hugging Face Transformers\", \"functionality\": \"Transformers\", \"api_name\": \"microsoft/xclip-base-patch32\", \"api_call\": \"XClipModel.from_pretrained('microsoft/xclip-base-patch32')\", \"api_arguments\": \"N/A\", \"python_environment_requirements\": \"transformers\", \"example_code\": \"For code examples, we refer to the documentation.\", \"performance\": {\"dataset\": \"Kinetics 400\", \"accuracy\": {\"top-1\": 80.4, \"top-5\": 95.0}}, \"description\": \"X-CLIP is a minimal extension of CLIP for general video-language understanding. The model is trained in a contrastive way on (video, text) pairs. This allows the model to be used for tasks like zero-shot, few-shot or fully supervised video classification and video-text retrieval.\"}}\nWrite a python program in 1 to 2 lines to call API in huggingface.\n\nThe answer should follow the format: {<<<domain>>> $DOMAIN, <<<api_call>>>: $API_CALL, <<<api_provider>>>: $API_PROVIDER, <<<explanation>>>: $EXPLANATION, <<<code>>>: $CODE}. Here are the requirements:\n1. The $API_CALL should have only 1 line of code that calls api.\n2. The $API_PROVIDER should be the programming framework used.\n3. $EXPLANATION should be a step-by-step explanation.\n4. The $CODE is the python code.\n5. Do not repeat the format in your answer.", "category": "generic"}
{"question_id": 30, "text": " A new project demands to classify videos for a social media platform. Let us create a video classification pipeline.\n \n Use this API documentation for reference:  {\"domain\": \"Computer Vision Video Classification\", \"framework\": \"Hugging Face Transformers\", \"functionality\": \"Video Classification\", \"api_name\": \"MCG-NJU/videomae-base\", \"api_call\": \"VideoMAEForPreTraining.from_pretrained('MCG-NJU/videomae-base')\", \"api_arguments\": [\"video\"], \"python_environment_requirements\": [\"transformers\"], \"example_code\": \"from transformers import VideoMAEImageProcessor, VideoMAEForPreTraining\nimport numpy as np\nimport torch\nnum_frames = 16\nvideo = list(np.random.randn(16, 3, 224, 224))\nprocessor = VideoMAEImageProcessor.from_pretrained(MCG-NJU/videomae-base)\nmodel = VideoMAEForPreTraining.from_pretrained(MCG-NJU/videomae-base)\npixel_values = processor(video, return_tensors=pt).pixel_values\nnum_patches_per_frame = (model.config.image_size // model.config.patch_size) ** 2\nseq_length = (num_frames // model.config.tubelet_size) * num_patches_per_frame\nbool_masked_pos = torch.randint(0, 2, (1, seq_length)).bool()\noutputs = model(pixel_values, bool_masked_pos=bool_masked_pos)\nloss = outputs.loss\", \"performance\": {\"dataset\": \"Kinetics-400\", \"accuracy\": \"To be provided\"}, \"description\": \"VideoMAE is an extension of Masked Autoencoders (MAE) to video. The architecture of the model is very similar to that of a standard Vision Transformer (ViT), with a decoder on top for predicting pixel values for masked patches.\"}}\nWrite a python program in 1 to 2 lines to call API in huggingface.\n\nThe answer should follow the format: {<<<domain>>> $DOMAIN, <<<api_call>>>: $API_CALL, <<<api_provider>>>: $API_PROVIDER, <<<explanation>>>: $EXPLANATION, <<<code>>>: $CODE}. Here are the requirements:\n1. The $API_CALL should have only 1 line of code that calls api.\n2. The $API_PROVIDER should be the programming framework used.\n3. $EXPLANATION should be a step-by-step explanation.\n4. The $CODE is the python code.\n5. Do not repeat the format in your answer.", "category": "generic"}
{"question_id": 31, "text": " I am an insurance adjustor. I need a zero-shot image classifier that will tell me whether a car has been involved in a major accident or had minor damages.\n \n Use this API documentation for reference:  {\"domain\": \"Computer Vision Zero-Shot Image Classification\", \"framework\": \"Hugging Face\", \"functionality\": \"Zero-Shot Image Classification\", \"api_name\": \"laion/CLIP-ViT-B-16-laion2B-s34B-b88K\", \"api_call\": \"pipeline('image-classification', model='laion/CLIP-ViT-B-16-laion2B-s34B-b88K')\", \"api_arguments\": {\"image\": \"Path to image file or URL\", \"class_names\": \"List of possible class names (comma-separated)\"}, \"python_environment_requirements\": {\"transformers\": \">=4.11.0\"}, \"example_code\": \"from transformers import pipeline; classify = pipeline('image-classification', model='laion/CLIP-ViT-B-16-laion2B-s34B-b88K'); classify('/path/to/image.jpg', ['cat', 'dog'])\", \"performance\": {\"dataset\": \"ImageNet-1k\", \"accuracy\": \"70.2%\"}, \"description\": \"A CLIP ViT-B/16 model trained with the LAION-2B English subset of LAION-5B using OpenCLIP. This model is intended for research purposes and can be used for zero-shot image classification, image and text retrieval, and other related tasks.\"}}\nWrite a python program in 1 to 2 lines to call API in huggingface.\n\nThe answer should follow the format: {<<<domain>>> $DOMAIN, <<<api_call>>>: $API_CALL, <<<api_provider>>>: $API_PROVIDER, <<<explanation>>>: $EXPLANATION, <<<code>>>: $CODE}. Here are the requirements:\n1. The $API_CALL should have only 1 line of code that calls api.\n2. The $API_PROVIDER should be the programming framework used.\n3. $EXPLANATION should be a step-by-step explanation.\n4. The $CODE is the python code.\n5. Do not repeat the format in your answer.", "category": "generic"}
{"question_id": 32, "text": " I want to analyze a medical image to find out if it's an X-ray, an MRI scan, or a CT scan.\n \n Use this API documentation for reference:  {\"domain\": \"Multimodal Zero-Shot Image Classification\", \"framework\": \"Hugging Face\", \"functionality\": \"Zero-Shot Image Classification\", \"api_name\": \"microsoft/BiomedCLIP-PubMedBERT_256-vit_base_patch16_224\", \"api_call\": \"pipeline('zero-shot-image-classification', model='microsoft/BiomedCLIP-PubMedBERT_256-vit_base_patch16_224')\", \"api_arguments\": \"image, possible_class_names\", \"python_environment_requirements\": \"transformers, torch, torchvision\", \"example_code\": \"from transformers import pipeline\nclip = pipeline('zero-shot-image-classification', model='microsoft/BiomedCLIP-PubMedBERT_256-vit_base_patch16_224')\nimage = 'path/to/image.png'\npossible_class_names = ['class1', 'class2', 'class3']\nresult = clip(image, possible_class_names)\", \"performance\": {\"dataset\": \"PMC-15M\", \"accuracy\": \"State of the art\"}, \"description\": \"BiomedCLIP is a biomedical vision-language foundation model pretrained on PMC-15M, a dataset of 15 million figure-caption pairs extracted from biomedical research articles in PubMed Central, using contrastive learning. It uses PubMedBERT as the text encoder and Vision Transformer as the image encoder, with domain-specific adaptations. It can perform various vision-language processing (VLP) tasks such as cross-modal retrieval, image classification, and visual question answering.\"}}\nWrite a python program in 1 to 2 lines to call API in huggingface.\n\nThe answer should follow the format: {<<<domain>>> $DOMAIN, <<<api_call>>>: $API_CALL, <<<api_provider>>>: $API_PROVIDER, <<<explanation>>>: $EXPLANATION, <<<code>>>: $CODE}. Here are the requirements:\n1. The $API_CALL should have only 1 line of code that calls api.\n2. The $API_PROVIDER should be the programming framework used.\n3. $EXPLANATION should be a step-by-step explanation.\n4. The $CODE is the python code.\n5. Do not repeat the format in your answer.", "category": "generic"}
{"question_id": 33, "text": " We are building a quiz application where the image will be shown, and we have to choose a dressings matching that image. Please help in classifying the image.\n \n Use this API documentation for reference:  {\"domain\": \"Natural Language Processing Zero-Shot Classification\", \"framework\": \"Hugging Face Transformers\", \"functionality\": \"Zero-Shot Image Classification\", \"api_name\": \"kakaobrain/align-base\", \"api_call\": \"AlignModel.from_pretrained('kakaobrain/align-base')\", \"api_arguments\": [\"text\", \"images\", \"return_tensors\"], \"python_environment_requirements\": [\"requests\", \"torch\", \"PIL\", \"transformers\"], \"example_code\": \"import requests\nimport torch\nfrom PIL import Image\nfrom transformers import AlignProcessor, AlignModel\nprocessor = AlignProcessor.from_pretrained(kakaobrain/align-base)\nmodel = AlignModel.from_pretrained(kakaobrain/align-base)\nurl = http://images.cocodataset.org/val2017/000000039769.jpg\nimage = Image.open(requests.get(url, stream=True).raw)\ncandidate_labels = [an image of a cat, an image of a dog]\ninputs = processor(text=candidate_labels, images=image, return_tensors=pt)\nwith torch.no_grad():\n outputs = model(**inputs)\nlogits_per_image = outputs.logits_per_image\nprobs = logits_per_image.softmax(dim=1)\nprint(probs)\", \"performance\": {\"dataset\": \"COYO-700M\", \"accuracy\": \"on-par or outperforms Google ALIGN's reported metrics\"}, \"description\": \"The ALIGN model is a dual-encoder architecture with EfficientNet as its vision encoder and BERT as its text encoder. It learns to align visual and text representations with contrastive learning. This implementation is trained on the open source COYO dataset and can be used for zero-shot image classification and multi-modal embedding retrieval.\"}}\nWrite a python program in 1 to 2 lines to call API in huggingface.\n\nThe answer should follow the format: {<<<domain>>> $DOMAIN, <<<api_call>>>: $API_CALL, <<<api_provider>>>: $API_PROVIDER, <<<explanation>>>: $EXPLANATION, <<<code>>>: $CODE}. Here are the requirements:\n1. The $API_CALL should have only 1 line of code that calls api.\n2. The $API_PROVIDER should be the programming framework used.\n3. $EXPLANATION should be a step-by-step explanation.\n4. The $CODE is the python code.\n5. Do not repeat the format in your answer.", "category": "generic"}
{"question_id": 34, "text": " We're developing a chatbot that can quickly identify and describe images for our Chinese-speaking users.\n \n Use this API documentation for reference:  {\"domain\": \"Natural Language Processing Zero-Shot Classification\", \"framework\": \"Hugging Face Transformers\", \"functionality\": \"Zero-Shot Image Classification\", \"api_name\": \"OFA-Sys/chinese-clip-vit-large-patch14-336px\", \"api_call\": \"ChineseCLIPModel.from_pretrained('OFA-Sys/chinese-clip-vit-large-patch14-336px')\", \"api_arguments\": {\"images\": \"image\", \"text\": \"texts\", \"return_tensors\": \"pt\", \"padding\": \"True\"}, \"python_environment_requirements\": [\"PIL\", \"requests\", \"transformers\"], \"example_code\": \"from PIL import Image\nimport requests\nfrom transformers import ChineseCLIPProcessor, ChineseCLIPModel\nmodel = ChineseCLIPModel.from_pretrained(OFA-Sys/chinese-clip-vit-large-patch14-336px)\nprocessor = ChineseCLIPProcessor.from_pretrained(OFA-Sys/chinese-clip-vit-large-patch14-336px)\nurl = https://clip-cn-beijing.oss-cn-beijing.aliyuncs.com/pokemon.jpeg\nimage = Image.open(requests.get(url, stream=True).raw)\ntexts = []\ninputs = processor(images=image, return_tensors=pt)\nimage_features = model.get_image_features(**inputs)\nimage_features = image_features / image_features.norm(p=2, dim=-1, keepdim=True)\ninputs = processor(text=texts, padding=True, return_tensors=pt)\ntext_features = model.get_text_features(**inputs)\ntext_features = text_features / text_features.norm(p=2, dim=-1, keepdim=True)\ninputs = processor(text=texts, images=image, return_tensors=pt, padding=True)\noutputs = model(**inputs)\nlogits_per_image = outputs.logits_per_image\nprobs = logits_per_image.softmax(dim=1)\", \"performance\": {\"dataset\": {\"CIFAR10\": 96.0, \"CIFAR100\": 79.75, \"DTD\": 51.2, \"EuroSAT\": 52.0, \"FER\": 55.1, \"FGVC\": 26.2, \"KITTI\": 49.9, \"MNIST\": 79.4, \"PC\": 63.5, \"VOC\": 84.9}, \"accuracy\": \"various\"}, \"description\": \"Chinese CLIP is a simple implementation of CLIP on a large-scale dataset of around 200 million Chinese image-text pairs. It uses ViT-L/14@336px as the image encoder and RoBERTa-wwm-base as the text encoder.\"}}\nWrite a python program in 1 to 2 lines to call API in huggingface.\n\nThe answer should follow the format: {<<<domain>>> $DOMAIN, <<<api_call>>>: $API_CALL, <<<api_provider>>>: $API_PROVIDER, <<<explanation>>>: $EXPLANATION, <<<code>>>: $CODE}. Here are the requirements:\n1. The $API_CALL should have only 1 line of code that calls api.\n2. The $API_PROVIDER should be the programming framework used.\n3. $EXPLANATION should be a step-by-step explanation.\n4. The $CODE is the python code.\n5. Do not repeat the format in your answer.", "category": "generic"}
{"question_id": 35, "text": " We would like to understand the sentiment of user's messages in a customer support chat system.\n \n Use this API documentation for reference:  {\"domain\": \"Natural Language Processing Text Classification\", \"framework\": \"Hugging Face\", \"functionality\": \"Sentiment Analysis\", \"api_name\": \"cardiffnlp/twitter-xlm-roberta-base-sentiment\", \"api_call\": \"pipeline(sentiment-analysis, model='cardiffnlp/twitter-xlm-roberta-base-sentiment')\", \"api_arguments\": [\"model_path\"], \"python_environment_requirements\": [\"transformers\"], \"example_code\": \"from transformers import pipeline\nmodel_path = cardiffnlp/twitter-xlm-roberta-base-sentiment\nsentiment_task = pipeline(sentiment-analysis, model=model_path, tokenizer=model_path)\nsentiment_task(T'estimo!)\", \"performance\": {\"dataset\": \"Twitter\", \"accuracy\": \"Not provided\"}, \"description\": \"This is a multilingual XLM-roBERTa-base model trained on ~198M tweets and finetuned for sentiment analysis. The sentiment fine-tuning was done on 8 languages (Ar, En, Fr, De, Hi, It, Sp, Pt) but it can be used for more languages (see paper for details).\"}}\nWrite a python program in 1 to 2 lines to call API in huggingface.\n\nThe answer should follow the format: {<<<domain>>> $DOMAIN, <<<api_call>>>: $API_CALL, <<<api_provider>>>: $API_PROVIDER, <<<explanation>>>: $EXPLANATION, <<<code>>>: $CODE}. Here are the requirements:\n1. The $API_CALL should have only 1 line of code that calls api.\n2. The $API_PROVIDER should be the programming framework used.\n3. $EXPLANATION should be a step-by-step explanation.\n4. The $CODE is the python code.\n5. Do not repeat the format in your answer.", "category": "generic"}
{"question_id": 36, "text": " As a book store owner, I want to classify customer reviews into positive and negative sentiments.\n \n Use this API documentation for reference:  {\"domain\": \"Natural Language Processing Text Classification\", \"framework\": \"Transformers\", \"functionality\": \"Text Classification\", \"api_name\": \"distilbert-base-uncased-finetuned-sst-2-english\", \"api_call\": \"DistilBertForSequenceClassification.from_pretrained('distilbert-base-uncased-finetuned-sst-2-english')\", \"api_arguments\": [\"inputs\"], \"python_environment_requirements\": [\"torch\", \"transformers\"], \"example_code\": \"import torch\nfrom transformers import DistilBertTokenizer, DistilBertForSequenceClassification\ntokenizer = DistilBertTokenizer.from_pretrained('distilbert-base-uncased')\nmodel = DistilBertForSequenceClassification.from_pretrained('distilbert-base-uncased-finetuned-sst-2-english')\ninputs = tokenizer('Hello, my dog is cute', return_tensors='pt')\nwith torch.no_grad():\n    logits = model(**inputs).logits\npredicted_class_id = logits.argmax().item()\nmodel.config.id2label[predicted_class_id]\", \"performance\": {\"dataset\": \"glue\", \"accuracy\": 0.911}, \"description\": \"This model is a fine-tune checkpoint of DistilBERT-base-uncased, fine-tuned on SST-2. It reaches an accuracy of 91.3 on the dev set (for comparison, Bert bert-base-uncased version reaches an accuracy of 92.7). This model can be used for topic classification.\"}}\nWrite a python program in 1 to 2 lines to call API in huggingface.\n\nThe answer should follow the format: {<<<domain>>> $DOMAIN, <<<api_call>>>: $API_CALL, <<<api_provider>>>: $API_PROVIDER, <<<explanation>>>: $EXPLANATION, <<<code>>>: $CODE}. Here are the requirements:\n1. The $API_CALL should have only 1 line of code that calls api.\n2. The $API_PROVIDER should be the programming framework used.\n3. $EXPLANATION should be a step-by-step explanation.\n4. The $CODE is the python code.\n5. Do not repeat the format in your answer.", "category": "generic"}
{"question_id": 37, "text": " I am the owner of a news website. I have several consumers' comments about our publishing news. I want to analyze the sentiments of these comments.\n \n Use this API documentation for reference:  {\"domain\": \"Natural Language Processing Text Classification\", \"framework\": \"Transformers\", \"functionality\": \"Sentiment Analysis\", \"api_name\": \"cardiffnlp/twitter-roberta-base-sentiment-latest\", \"api_call\": \"pipeline(sentiment-analysis, model=AutoModel.from_pretrained('cardiffnlp/twitter-roberta-base-sentiment-latest'), tokenizer=AutoTokenizer.from_pretrained('cardiffnlp/twitter-roberta-base-sentiment-latest'))\", \"api_arguments\": {\"model\": \"model_path\", \"tokenizer\": \"model_path\"}, \"python_environment_requirements\": [\"transformers\", \"numpy\", \"scipy\"], \"example_code\": \"from transformers import pipeline\nsentiment_task = pipeline(sentiment-analysis, model=model_path, tokenizer=model_path)\nsentiment_task(Covid cases are increasing fast!)\", \"performance\": {\"dataset\": \"tweet_eval\", \"accuracy\": \"Not provided\"}, \"description\": \"This is a RoBERTa-base model trained on ~124M tweets from January 2018 to December 2021, and finetuned for sentiment analysis with the TweetEval benchmark. The model is suitable for English.\"}}\nWrite a python program in 1 to 2 lines to call API in huggingface.\n\nThe answer should follow the format: {<<<domain>>> $DOMAIN, <<<api_call>>>: $API_CALL, <<<api_provider>>>: $API_PROVIDER, <<<explanation>>>: $EXPLANATION, <<<code>>>: $CODE}. Here are the requirements:\n1. The $API_CALL should have only 1 line of code that calls api.\n2. The $API_PROVIDER should be the programming framework used.\n3. $EXPLANATION should be a step-by-step explanation.\n4. The $CODE is the python code.\n5. Do not repeat the format in your answer.", "category": "generic"}
{"question_id": 38, "text": " Our business is expanding to international markets. Analyze the sentiment of the following customer review to better understand their satisfaction with our product: \\\"\\u00a1Esto es maravilloso! Me encanta.\\\"\n###Input: \\\"\\u00a1Esto es maravilloso! Me encanta.\\\"\n \n Use this API documentation for reference:  {\"domain\": \"Natural Language Processing Text Classification\", \"framework\": \"Hugging Face Transformers\", \"functionality\": \"Sentiment Analysis\", \"api_name\": \"bert-base-multilingual-uncased-sentiment\", \"api_call\": \"pipeline('sentiment-analysis', model='nlptown/bert-base-multilingual-uncased-sentiment')\", \"api_arguments\": [\"text\"], \"python_environment_requirements\": [\"transformers\"], \"example_code\": \"result = sentiment_pipeline('I love this product!')\", \"performance\": {\"dataset\": [{\"language\": \"English\", \"accuracy\": {\"exact\": \"67%\", \"off-by-1\": \"95%\"}}, {\"language\": \"Dutch\", \"accuracy\": {\"exact\": \"57%\", \"off-by-1\": \"93%\"}}, {\"language\": \"German\", \"accuracy\": {\"exact\": \"61%\", \"off-by-1\": \"94%\"}}, {\"language\": \"French\", \"accuracy\": {\"exact\": \"59%\", \"off-by-1\": \"94%\"}}, {\"language\": \"Italian\", \"accuracy\": {\"exact\": \"59%\", \"off-by-1\": \"95%\"}}, {\"language\": \"Spanish\", \"accuracy\": {\"exact\": \"58%\", \"off-by-1\": \"95%\"}}]}, \"description\": \"This a bert-base-multilingual-uncased model finetuned for sentiment analysis on product reviews in six languages: English, Dutch, German, French, Spanish and Italian. It predicts the sentiment of the review as a number of stars (between 1 and 5).\"}}\nWrite a python program in 1 to 2 lines to call API in huggingface.\n\nThe answer should follow the format: {<<<domain>>> $DOMAIN, <<<api_call>>>: $API_CALL, <<<api_provider>>>: $API_PROVIDER, <<<explanation>>>: $EXPLANATION, <<<code>>>: $CODE}. Here are the requirements:\n1. The $API_CALL should have only 1 line of code that calls api.\n2. The $API_PROVIDER should be the programming framework used.\n3. $EXPLANATION should be a step-by-step explanation.\n4. The $CODE is the python code.\n5. Do not repeat the format in your answer.", "category": "generic"}
{"question_id": 39, "text": " We are a forum moderator team looking for a solution to classify comments into toxic or non-toxic categories.\n \n Use this API documentation for reference:  {\"domain\": \"Natural Language Processing Text Classification\", \"framework\": \"Hugging Face Transformers\", \"functionality\": \"Transformers\", \"api_name\": \"martin-ha/toxic-comment-model\", \"api_call\": \"pipeline(model='martin-ha/toxic-comment-model')\", \"api_arguments\": {\"model_path\": \"martin-ha/toxic-comment-model\"}, \"python_environment_requirements\": [\"transformers\"], \"example_code\": \"from transformers import AutoModelForSequenceClassification, AutoTokenizer, TextClassificationPipeline\nmodel_path = martin-ha/toxic-comment-model\ntokenizer = AutoTokenizer.from_pretrained(model_path)\nmodel = AutoModelForSequenceClassification.from_pretrained(model_path)\npipeline = TextClassificationPipeline(model=model, tokenizer=tokenizer)\nprint(pipeline('This is a test text.'))\", \"performance\": {\"dataset\": \"held-out test set\", \"accuracy\": 0.94, \"f1-score\": 0.59}, \"description\": \"This model is a fine-tuned version of the DistilBERT model to classify toxic comments.\"}}\nWrite a python program in 1 to 2 lines to call API in huggingface.\n\nThe answer should follow the format: {<<<domain>>> $DOMAIN, <<<api_call>>>: $API_CALL, <<<api_provider>>>: $API_PROVIDER, <<<explanation>>>: $EXPLANATION, <<<code>>>: $CODE}. Here are the requirements:\n1. The $API_CALL should have only 1 line of code that calls api.\n2. The $API_PROVIDER should be the programming framework used.\n3. $EXPLANATION should be a step-by-step explanation.\n4. The $CODE is the python code.\n5. Do not repeat the format in your answer.", "category": "generic"}
{"question_id": 40, "text": " My company is launching a social media campaign. We need an AI-based system that would automatically analyze the sentiment of any user-generated reviews or tweets concerning our product.\n \n Use this API documentation for reference:  {\"domain\": \"Natural Language Processing Text Classification\", \"framework\": \"Transformers\", \"functionality\": \"Sentiment Analysis\", \"api_name\": \"siebert/sentiment-roberta-large-english\", \"api_call\": \"pipeline('sentiment-analysis', model='siebert/sentiment-roberta-large-english')\", \"api_arguments\": [\"text\"], \"python_environment_requirements\": [\"transformers\"], \"example_code\": \"from transformers import pipeline\nsentiment_analysis = pipeline(sentiment-analysis, model=siebert/sentiment-roberta-large-english)\nprint(sentiment_analysis(I love this!))\", \"performance\": {\"dataset\": [{\"name\": \"McAuley and Leskovec (2013) (Reviews)\", \"accuracy\": 98.0}, {\"name\": \"McAuley and Leskovec (2013) (Review Titles)\", \"accuracy\": 87.0}, {\"name\": \"Yelp Academic Dataset\", \"accuracy\": 96.5}, {\"name\": \"Maas et al. (2011)\", \"accuracy\": 96.0}, {\"name\": \"Kaggle\", \"accuracy\": 96.0}, {\"name\": \"Pang and Lee (2005)\", \"accuracy\": 91.0}, {\"name\": \"Nakov et al. (2013)\", \"accuracy\": 88.5}, {\"name\": \"Shamma (2009)\", \"accuracy\": 87.0}, {\"name\": \"Blitzer et al. (2007) (Books)\", \"accuracy\": 92.5}, {\"name\": \"Blitzer et al. (2007) (DVDs)\", \"accuracy\": 92.5}, {\"name\": \"Blitzer et al. (2007) (Electronics)\", \"accuracy\": 95.0}, {\"name\": \"Blitzer et al. (2007) (Kitchen devices)\", \"accuracy\": 98.5}, {\"name\": \"Pang et al. (2002)\", \"accuracy\": 95.5}, {\"name\": \"Speriosu et al. (2011)\", \"accuracy\": 85.5}, {\"name\": \"Hartmann et al. (2019)\", \"accuracy\": 98.0}], \"average_accuracy\": 93.2}, \"description\": \"This model ('SiEBERT', prefix for 'Sentiment in English') is a fine-tuned checkpoint of RoBERTa-large (Liu et al. 2019). It enables reliable binary sentiment analysis for various types of English-language text. For each instance, it predicts either positive (1) or negative (0) sentiment. The model was fine-tuned and evaluated on 15 data sets from diverse text sources to enhance generalization across different types of texts (reviews, tweets, etc.). Consequently, it outperforms models trained on only one type of text (e.g., movie reviews from the popular SST-2 benchmark) when used on new data as shown below.\"}}\nWrite a python program in 1 to 2 lines to call API in huggingface.\n\nThe answer should follow the format: {<<<domain>>> $DOMAIN, <<<api_call>>>: $API_CALL, <<<api_provider>>>: $API_PROVIDER, <<<explanation>>>: $EXPLANATION, <<<code>>>: $CODE}. Here are the requirements:\n1. The $API_CALL should have only 1 line of code that calls api.\n2. The $API_PROVIDER should be the programming framework used.\n3. $EXPLANATION should be a step-by-step explanation.\n4. The $CODE is the python code.\n5. Do not repeat the format in your answer.", "category": "generic"}
{"question_id": 41, "text": " I have jobs descriptions in French for sales manager, please highlight names of organizations or cities within the text.\n###Input: \\\"La soci\\u00e9t\\u00e9 de Paris est sp\\u00e9cialis\\u00e9e dans la vente de v\\u00e9hicules \\u00e9lectriques. Responsable des ventes, vous travaillerez au sein d'une \\u00e9quipe dynamique dans l'agence de Lyon. Vous \\u00eates charg\\u00e9(e) de d\\u00e9velopper le portefeuille client et d'assurer la satisfaction des clients existants. Dans ce contexte, vous devrez travailler en lien \\u00e9troit avec le directeur commercial et les autres \\u00e9quipes de l'entreprise. Une exp\\u00e9rience pr\\u00e9alable chez Renault est un atout.\\\"\n \n Use this API documentation for reference:  {\"domain\": \"Natural Language Processing Token Classification\", \"framework\": \"Hugging Face Transformers\", \"functionality\": \"Named Entity Recognition\", \"api_name\": \"Jean-Baptiste/camembert-ner\", \"api_call\": \"AutoModelForTokenClassification.from_pretrained('Jean-Baptiste/camembert-ner')\", \"api_arguments\": {\"model\": \"model\", \"tokenizer\": \"tokenizer\", \"aggregation_strategy\": \"simple\"}, \"python_environment_requirements\": {\"transformers\": \"AutoTokenizer, AutoModelForTokenClassification, pipeline\"}, \"example_code\": \"from transformers import AutoTokenizer, AutoModelForTokenClassification\ntokenizer = AutoTokenizer.from_pretrained(Jean-Baptiste/camembert-ner)\nmodel = AutoModelForTokenClassification.from_pretrained(Jean-Baptiste/camembert-ner)\nfrom transformers import pipeline\nnlp = pipeline('ner', model=model, tokenizer=tokenizer, aggregation_strategy=simple)\nnlp(Apple est cre le 1er avril 1976 dans le garage de la maison d'enfance de Steve Jobs  Los Altos en Californie par Steve Jobs, Steve Wozniak et Ronald Wayne14, puis constitue sous forme de socit le 3 janvier 1977  l'origine sous le nom d'Apple Computer, mais pour ses 30 ans et pour reflter la diversification de ses produits, le mot \\u00ab computer  est retir le 9 janvier 2015.)\", \"performance\": {\"dataset\": \"wikiner-fr\", \"accuracy\": {\"overall_f1\": 0.8914, \"PER_f1\": 0.9483, \"ORG_f1\": 0.8181, \"LOC_f1\": 0.8955, \"MISC_f1\": 0.8146}}, \"description\": \"camembert-ner is a Named Entity Recognition (NER) model fine-tuned from camemBERT on the wikiner-fr dataset. It can recognize entities such as persons, organizations, locations, and miscellaneous entities.\"}}\nWrite a python program in 1 to 2 lines to call API in huggingface.\n\nThe answer should follow the format: {<<<domain>>> $DOMAIN, <<<api_call>>>: $API_CALL, <<<api_provider>>>: $API_PROVIDER, <<<explanation>>>: $EXPLANATION, <<<code>>>: $CODE}. Here are the requirements:\n1. The $API_CALL should have only 1 line of code that calls api.\n2. The $API_PROVIDER should be the programming framework used.\n3. $EXPLANATION should be a step-by-step explanation.\n4. The $CODE is the python code.\n5. Do not repeat the format in your answer.", "category": "generic"}
{"question_id": 42, "text": " In order to have a better understanding of our clients, I'd like to identify the names of people and organizations mentioned in the following customer review.\n###Input: \\\"I recently purchased a MacBook Pro from Apple Inc. and had a fantastic customer support experience. John from their tech support team was incredibly helpful and professional.\\\"\n \n Use this API documentation for reference:  {\"domain\": \"Natural Language Processing Token Classification\", \"framework\": \"Transformers\", \"functionality\": \"Named Entity Recognition\", \"api_name\": \"dslim/bert-large-NER\", \"api_call\": \"AutoModelForTokenClassification.from_pretrained('dslim/bert-large-NER')\", \"api_arguments\": {\"model\": \"AutoModelForTokenClassification.from_pretrained('dslim/bert-large-NER')\", \"tokenizer\": \"AutoTokenizer.from_pretrained('dslim/bert-large-NER')\"}, \"python_environment_requirements\": {\"transformers\": \"4.0.1\"}, \"example_code\": {\"example\": \"My name is Wolfgang and I live in Berlin\", \"ner_results\": \"nlp(example)\"}, \"performance\": {\"dataset\": \"conll2003\", \"accuracy\": {\"f1\": 0.92, \"precision\": 0.92, \"recall\": 0.919}}, \"description\": \"bert-large-NER is a fine-tuned BERT model that is ready to use for Named Entity Recognition and achieves state-of-the-art performance for the NER task. It has been trained to recognize four types of entities: location (LOC), organizations (ORG), person (PER) and Miscellaneous (MISC).\"}}\nWrite a python program in 1 to 2 lines to call API in huggingface.\n\nThe answer should follow the format: {<<<domain>>> $DOMAIN, <<<api_call>>>: $API_CALL, <<<api_provider>>>: $API_PROVIDER, <<<explanation>>>: $EXPLANATION, <<<code>>>: $CODE}. Here are the requirements:\n1. The $API_CALL should have only 1 line of code that calls api.\n2. The $API_PROVIDER should be the programming framework used.\n3. $EXPLANATION should be a step-by-step explanation.\n4. The $CODE is the python code.\n5. Do not repeat the format in your answer.", "category": "generic"}
{"question_id": 43, "text": " I am building a social media app that requires people to write fascinating stories rather than boring sentences. Detect named entities in a sentence by using an NER model.\n \n Use this API documentation for reference:  {\"domain\": \"Natural Language Processing Token Classification\", \"framework\": \"Flair\", \"functionality\": \"Named Entity Recognition\", \"api_name\": \"flair/ner-english-ontonotes-large\", \"api_call\": \"SequenceTagger.load('flair/ner-english-ontonotes-large')\", \"api_arguments\": [\"sentence\"], \"python_environment_requirements\": [\"flair\"], \"example_code\": \"from flair.data import Sentence\nfrom flair.models import SequenceTagger\n# load tagger\ntagger = SequenceTagger.load('flair/ner-english-ontonotes-large')\n# make example sentence\nsentence = Sentence('On September 1st George won 1 dollar while watching Game of Thrones.')\n# predict NER tags\ntagger.predict(sentence)\n# print sentence\nprint(sentence)\n# print predicted NER spans\nprint('The following NER tags are found:')\n# iterate over entities and print\nfor entity in sentence.get_spans('ner'):\n    print(entity)\", \"performance\": {\"dataset\": \"Ontonotes\", \"accuracy\": 90.93}, \"description\": \"English NER in Flair (Ontonotes large model). This is the large 18-class NER model for English that ships with Flair. It predicts 18 tags such as cardinal value, date value, event name, building name, geo-political entity, language name, law name, location name, money name, affiliation, ordinal value, organization name, percent value, person name, product name, quantity value, time value, and name of work of art. The model is based on document-level XLM-R embeddings and FLERT.\"}}\nWrite a python program in 1 to 2 lines to call API in huggingface.\n\nThe answer should follow the format: {<<<domain>>> $DOMAIN, <<<api_call>>>: $API_CALL, <<<api_provider>>>: $API_PROVIDER, <<<explanation>>>: $EXPLANATION, <<<code>>>: $CODE}. Here are the requirements:\n1. The $API_CALL should have only 1 line of code that calls api.\n2. The $API_PROVIDER should be the programming framework used.\n3. $EXPLANATION should be a step-by-step explanation.\n4. The $CODE is the python code.\n5. Do not repeat the format in your answer.", "category": "generic"}
{"question_id": 44, "text": " We have a large dataset of customer orders in the form of a table. Help us answer questions about this data.\n \n Use this API documentation for reference:  {\"domain\": \"Natural Language Processing Table Question Answering\", \"framework\": \"Transformers\", \"functionality\": \"Table Question Answering\", \"api_name\": \"google/tapas-small-finetuned-sqa\", \"api_call\": \"pipeline('table-question-answering', model='google/tapas-small-finetuned-sqa')\", \"api_arguments\": \"\", \"python_environment_requirements\": \"transformers\", \"example_code\": \"\", \"performance\": {\"dataset\": \"msr_sqa\", \"accuracy\": 0.6155}, \"description\": \"TAPAS small model fine-tuned on Sequential Question Answering (SQA). It uses relative position embeddings (i.e. resetting the position index at every cell of the table).\"}}\nWrite a python program in 1 to 2 lines to call API in huggingface.\n\nThe answer should follow the format: {<<<domain>>> $DOMAIN, <<<api_call>>>: $API_CALL, <<<api_provider>>>: $API_PROVIDER, <<<explanation>>>: $EXPLANATION, <<<code>>>: $CODE}. Here are the requirements:\n1. The $API_CALL should have only 1 line of code that calls api.\n2. The $API_PROVIDER should be the programming framework used.\n3. $EXPLANATION should be a step-by-step explanation.\n4. The $CODE is the python code.\n5. Do not repeat the format in your answer.", "category": "generic"}
{"question_id": 45, "text": " Gather information about annual income and age demographics of employees to predict retirement patterns. Make sure to identify top employees for potential promotions.\n \n Use this API documentation for reference:  {\"domain\": \"Natural Language Processing Table Question Answering\", \"framework\": \"Transformers\", \"functionality\": \"Table Question Answering\", \"api_name\": \"google/tapas-large-finetuned-sqa\", \"api_call\": \"TapasForQuestionAnswering.from_pretrained('google/tapas-large-finetuned-sqa')\", \"api_arguments\": [\"question\", \"table\"], \"python_environment_requirements\": [\"transformers\"], \"example_code\": \"https://huggingface.co/google/tapas-large-finetuned-sqa\", \"performance\": {\"dataset\": \"msr_sqa\", \"accuracy\": 0.7289}, \"description\": \"TAPAS large model fine-tuned on Sequential Question Answering (SQA). This model was pre-trained on MLM and an additional step which the authors call intermediate pre-training, and then fine-tuned on SQA. It uses relative position embeddings (i.e. resetting the position index at every cell of the table).\"}}\nWrite a python program in 1 to 2 lines to call API in huggingface.\n\nThe answer should follow the format: {<<<domain>>> $DOMAIN, <<<api_call>>>: $API_CALL, <<<api_provider>>>: $API_PROVIDER, <<<explanation>>>: $EXPLANATION, <<<code>>>: $CODE}. Here are the requirements:\n1. The $API_CALL should have only 1 line of code that calls api.\n2. The $API_PROVIDER should be the programming framework used.\n3. $EXPLANATION should be a step-by-step explanation.\n4. The $CODE is the python code.\n5. Do not repeat the format in your answer.", "category": "generic"}
{"question_id": 46, "text": " To track our sales data, we need to find total sales of a specific product based on a table containing sales information per week.\n \n Use this API documentation for reference:  {\"domain\": \"Natural Language Processing Table Question Answering\", \"framework\": \"Hugging Face Transformers\", \"functionality\": \"Table Question Answering\", \"api_name\": \"lysandre/tapas-temporary-repo\", \"api_call\": \"TapasForQuestionAnswering.from_pretrained('lysandre/tapas-temporary-repo')\", \"api_arguments\": [\"model_name\", \"question\", \"table\"], \"python_environment_requirements\": [\"transformers\"], \"example_code\": \"tokenizer = TapasTokenizer.from_pretrained('lysandre/tapas-temporary-repo')\nmodel = TapasForQuestionAnswering.from_pretrained('lysandre/tapas-temporary-repo')\ninputs = tokenizer(table=table, queries=question, return_tensors='pt')\noutputs = model(**inputs)\npredicted_answer_coordinates, predicted_aggregation_indices = tokenizer.convert_logits_to_predictions(inputs, outputs.logits.detach(), outputs.logits_aggregation.detach())\", \"performance\": {\"dataset\": \"SQA\", \"accuracy\": \"Not provided\"}, \"description\": \"TAPAS base model fine-tuned on Sequential Question Answering (SQA). This model is pretrained on a large corpus of English data from Wikipedia in a self-supervised fashion and can be used for answering questions related to a table in a conversational set-up.\"}}\nWrite a python program in 1 to 2 lines to call API in huggingface.\n\nThe answer should follow the format: {<<<domain>>> $DOMAIN, <<<api_call>>>: $API_CALL, <<<api_provider>>>: $API_PROVIDER, <<<explanation>>>: $EXPLANATION, <<<code>>>: $CODE}. Here are the requirements:\n1. The $API_CALL should have only 1 line of code that calls api.\n2. The $API_PROVIDER should be the programming framework used.\n3. $EXPLANATION should be a step-by-step explanation.\n4. The $CODE is the python code.\n5. Do not repeat the format in your answer.", "category": "generic"}
{"question_id": 47, "text": " I have a table containing information about various animals and their important characteristics. I need the system to answer a query to provide information about the tallest animal in the table.\n \n Use this API documentation for reference:  {\"domain\": \"Natural Language Processing Table Question Answering\", \"framework\": \"Transformers\", \"functionality\": \"Table Question Answering\", \"api_name\": \"google/tapas-mini-finetuned-sqa\", \"api_call\": \"TapasForQuestionAnswering.from_pretrained('google/tapas-mini-finetuned-sqa')\", \"api_arguments\": [\"model_name\", \"table\", \"queries\"], \"python_environment_requirements\": [\"transformers\"], \"example_code\": \"N/A\", \"performance\": {\"dataset\": \"msr_sqa\", \"accuracy\": 0.5148}, \"description\": \"TAPAS mini model fine-tuned on Sequential Question Answering (SQA)\"}}\nWrite a python program in 1 to 2 lines to call API in huggingface.\n\nThe answer should follow the format: {<<<domain>>> $DOMAIN, <<<api_call>>>: $API_CALL, <<<api_provider>>>: $API_PROVIDER, <<<explanation>>>: $EXPLANATION, <<<code>>>: $CODE}. Here are the requirements:\n1. The $API_CALL should have only 1 line of code that calls api.\n2. The $API_PROVIDER should be the programming framework used.\n3. $EXPLANATION should be a step-by-step explanation.\n4. The $CODE is the python code.\n5. Do not repeat the format in your answer.", "category": "generic"}
{"question_id": 48, "text": " You are building an app that allows users to find quick answers to textbook questions. Users will send a message with the question, and the answer should be detected directly from the textbook content.\n \n Use this API documentation for reference:  {\"domain\": \"Natural Language Processing Question Answering\", \"framework\": \"Transformers\", \"functionality\": \"Question Answering\", \"api_name\": \"distilbert-base-uncased-distilled-squad\", \"api_call\": \"pipeline('question-answering', model='distilbert-base-uncased-distilled-squad')\", \"api_arguments\": [\"question\", \"context\"], \"python_environment_requirements\": [\"transformers\"], \"example_code\": \"from transformers import pipeline\nquestion_answerer = pipeline(question-answering, model='distilbert-base-uncased-distilled-squad')\ncontext = r\n... Extractive Question Answering is the task of extracting an answer from a text given a question. An example of a\n... question answering dataset is the SQuAD dataset, which is entirely based on that task. If you would like to fine-tune\n... a model on a SQuAD task, you may leverage the examples/pytorch/question-answering/run_squad.py script.\n... \nresult = question_answerer(question=What is a good example of a question answering dataset?, context=context)\nprint(\n... fAnswer: '{result['answer']}', score: {round(result['score'], 4)}, start: {result['start']}, end: {result['end']}\n...)\", \"performance\": {\"dataset\": \"SQuAD v1.1\", \"accuracy\": \"86.9 F1 score\"}, \"description\": \"DistilBERT base uncased distilled SQuAD is a fine-tuned version of DistilBERT-base-uncased for the task of question answering. It has 40% less parameters than bert-base-uncased, runs 60% faster while preserving over 95% of BERT's performances as measured on the GLUE language understanding benchmark.\"}}\nWrite a python program in 1 to 2 lines to call API in huggingface.\n\nThe answer should follow the format: {<<<domain>>> $DOMAIN, <<<api_call>>>: $API_CALL, <<<api_provider>>>: $API_PROVIDER, <<<explanation>>>: $EXPLANATION, <<<code>>>: $CODE}. Here are the requirements:\n1. The $API_CALL should have only 1 line of code that calls api.\n2. The $API_PROVIDER should be the programming framework used.\n3. $EXPLANATION should be a step-by-step explanation.\n4. The $CODE is the python code.\n5. Do not repeat the format in your answer.", "category": "generic"}
{"question_id": 49, "text": " We want to make sure clarify some questions about the legal implications of a new partnership contract for a real estate development project.\n###Input: We hereby grant the Licensee the exclusive right to develop, construct, operate and promote the Project, as well as to manage the daily operations of the Licensed Facilities during the Term. In consideration for the grant of the License, the Licensee shall pay to the Licensor the full amount of Ten Million (10,000,000) Dollars within thirty (30) days after the execution hereof.\n \n Use this API documentation for reference:  {\"domain\": \"Natural Language Processing Question Answering\", \"framework\": \"Hugging Face Transformers\", \"functionality\": \"Question Answering\", \"api_name\": \"Rakib/roberta-base-on-cuad\", \"api_call\": \"AutoModelForQuestionAnswering.from_pretrained('Rakib/roberta-base-on-cuad')\", \"api_arguments\": {\"tokenizer\": \"AutoTokenizer.from_pretrained(Rakib/roberta-base-on-cuad)\"}, \"python_environment_requirements\": {\"transformers\": \"latest\"}, \"example_code\": {\"import\": \"from transformers import AutoTokenizer, AutoModelForQuestionAnswering\", \"tokenizer\": \"tokenizer = AutoTokenizer.from_pretrained(Rakib/roberta-base-on-cuad)\", \"model\": \"model = AutoModelForQuestionAnswering.from_pretrained(Rakib/roberta-base-on-cuad)\"}, \"performance\": {\"dataset\": \"cuad\", \"accuracy\": \"46.6%\"}, \"description\": \"This model is trained for the task of Question Answering on Legal Documents using the CUAD dataset. It is based on the RoBERTa architecture and can be used to extract answers from legal contracts and documents.\"}}\nWrite a python program in 1 to 2 lines to call API in huggingface.\n\nThe answer should follow the format: {<<<domain>>> $DOMAIN, <<<api_call>>>: $API_CALL, <<<api_provider>>>: $API_PROVIDER, <<<explanation>>>: $EXPLANATION, <<<code>>>: $CODE}. Here are the requirements:\n1. The $API_CALL should have only 1 line of code that calls api.\n2. The $API_PROVIDER should be the programming framework used.\n3. $EXPLANATION should be a step-by-step explanation.\n4. The $CODE is the python code.\n5. Do not repeat the format in your answer.", "category": "generic"}
{"question_id": 50, "text": " Help me setup a tinyroberta model from deepset for Question and Answer. Provide a sample input and output.\n \n Use this API documentation for reference:  {\"domain\": \"Natural Language Processing Question Answering\", \"framework\": \"Transformers\", \"functionality\": \"Question Answering\", \"api_name\": \"deepset/tinyroberta-squad2\", \"api_call\": \"AutoModelForQuestionAnswering.from_pretrained('deepset/tinyroberta-squad2')\", \"api_arguments\": {\"model_name_or_path\": \"deepset/tinyroberta-squad2\", \"question\": \"Why is model conversion important?\", \"context\": \"The option to convert models between FARM and transformers gives freedom to the user and let people easily switch between frameworks.\"}, \"python_environment_requirements\": [\"transformers\"], \"example_code\": \"from transformers import AutoModelForQuestionAnswering, AutoTokenizer, pipeline\nmodel_name = deepset/tinyroberta-squad2\nnlp = pipeline('question-answering', model=model_name, tokenizer=model_name)\nQA_input = {\n 'question': 'Why is model conversion important?',\n 'context': 'The option to convert models between FARM and transformers gives freedom to the user and let people easily switch between frameworks.'\n}\nres = nlp(QA_input)\", \"performance\": {\"dataset\": \"squad_v2\", \"accuracy\": {\"exact\": 78.69114798281817, \"f1\": 81.9198998536977}}, \"description\": \"This is the distilled version of the deepset/roberta-base-squad2 model. This model has a comparable prediction quality and runs at twice the speed of the base model.\"}}\nWrite a python program in 1 to 2 lines to call API in huggingface.\n\nThe answer should follow the format: {<<<domain>>> $DOMAIN, <<<api_call>>>: $API_CALL, <<<api_provider>>>: $API_PROVIDER, <<<explanation>>>: $EXPLANATION, <<<code>>>: $CODE}. Here are the requirements:\n1. The $API_CALL should have only 1 line of code that calls api.\n2. The $API_PROVIDER should be the programming framework used.\n3. $EXPLANATION should be a step-by-step explanation.\n4. The $CODE is the python code.\n5. Do not repeat the format in your answer.", "category": "generic"}
{"question_id": 51, "text": " I want to build a tool to answer questions automatically from a given document. Which model do you recommend for this task?\n \n Use this API documentation for reference:  {\"domain\": \"Natural Language Processing Question Answering\", \"framework\": \"Hugging Face Transformers\", \"functionality\": \"Question Answering\", \"api_name\": \"deepset/roberta-base-squad2-distilled\", \"api_call\": \"AutoModel.from_pretrained('deepset/roberta-base-squad2-distilled')\", \"api_arguments\": {\"context\": \"string\", \"question\": \"string\"}, \"python_environment_requirements\": \"transformers\", \"example_code\": \"from transformers import pipeline\nqa_pipeline = pipeline('question-answering', model='deepset/roberta-base-squad2-distilled')\nresult = qa_pipeline({'context': 'This is a context.', 'question': 'What is this?'})\nprint(result)\", \"performance\": {\"dataset\": \"squad_v2\", \"exact\": 79.8366040596311, \"f1\": 83.916407079888}, \"description\": \"This model is a distilled version of deepset/roberta-large-squad2, trained on SQuAD 2.0 dataset for question answering tasks. It is based on the Roberta architecture and has been fine-tuned using Haystack's distillation feature.\"}}\nWrite a python program in 1 to 2 lines to call API in huggingface.\n\nThe answer should follow the format: {<<<domain>>> $DOMAIN, <<<api_call>>>: $API_CALL, <<<api_provider>>>: $API_PROVIDER, <<<explanation>>>: $EXPLANATION, <<<code>>>: $CODE}. Here are the requirements:\n1. The $API_CALL should have only 1 line of code that calls api.\n2. The $API_PROVIDER should be the programming framework used.\n3. $EXPLANATION should be a step-by-step explanation.\n4. The $CODE is the python code.\n5. Do not repeat the format in your answer.", "category": "generic"}
{"question_id": 52, "text": " We have a French news agency and we want to categorize the news articles based on sports, politics, and science.\n \n Use this API documentation for reference:  {\"domain\": \"Natural Language Processing Zero-Shot Classification\", \"framework\": \"Hugging Face Transformers\", \"functionality\": \"Zero-Shot Classification\", \"api_name\": \"BaptisteDoyen/camembert-base-xnli\", \"api_call\": \"pipeline('zero-shot-classification', model='BaptisteDoyen/camembert-base-xnli')\", \"api_arguments\": {\"sequence\": \"str\", \"candidate_labels\": \"List[str]\", \"hypothesis_template\": \"str\"}, \"python_environment_requirements\": [\"transformers\"], \"example_code\": \"sequence = L'\\u00e9quipe de France joue aujourd'hui au Parc des Princes\ncandidate_labels = [sport,politique,science]\nhypothesis_template = Ce texte parle de {}.\nclassifier(sequence, candidate_labels, hypothesis_template=hypothesis_template)\", \"performance\": {\"dataset\": \"xnli\", \"accuracy\": {\"validation\": 81.4, \"test\": 81.7}}, \"description\": \"Camembert-base model fine-tuned on french part of XNLI dataset. One of the few Zero-Shot classification models working on French.\"}}\nWrite a python program in 1 to 2 lines to call API in huggingface.\n\nThe answer should follow the format: {<<<domain>>> $DOMAIN, <<<api_call>>>: $API_CALL, <<<api_provider>>>: $API_PROVIDER, <<<explanation>>>: $EXPLANATION, <<<code>>>: $CODE}. Here are the requirements:\n1. The $API_CALL should have only 1 line of code that calls api.\n2. The $API_PROVIDER should be the programming framework used.\n3. $EXPLANATION should be a step-by-step explanation.\n4. The $CODE is the python code.\n5. Do not repeat the format in your answer.", "category": "generic"}
{"question_id": 53, "text": " I need a solution to detect whether a piece of news is talking about technology, sports, or politics.\n \n Use this API documentation for reference:  {\"domain\": \"Natural Language Processing Zero-Shot Classification\", \"framework\": \"Transformers\", \"functionality\": \"Zero-Shot Classification\", \"api_name\": \"cross-encoder/nli-roberta-base\", \"api_call\": \"CrossEncoder('cross-encoder/nli-roberta-base')\", \"api_arguments\": [\"sentence1\", \"sentence2\"], \"python_environment_requirements\": [\"sentence_transformers\", \"transformers\"], \"example_code\": \"from transformers import pipeline\nclassifier = pipeline(zero-shot-classification, model='cross-encoder/nli-roberta-base')\nsent = Apple just announced the newest iPhone X\ncandidate_labels = [technology, sports, politics]\nres = classifier(sent, candidate_labels)\nprint(res)\", \"performance\": {\"dataset\": [\"SNLI\", \"MultiNLI\"], \"accuracy\": \"See SBERT.net - Pretrained Cross-Encoder\"}, \"description\": \"Cross-Encoder for Natural Language Inference trained on the SNLI and MultiNLI datasets. Outputs three scores corresponding to the labels: contradiction, entailment, neutral.\"}}\nWrite a python program in 1 to 2 lines to call API in huggingface.\n\nThe answer should follow the format: {<<<domain>>> $DOMAIN, <<<api_call>>>: $API_CALL, <<<api_provider>>>: $API_PROVIDER, <<<explanation>>>: $EXPLANATION, <<<code>>>: $CODE}. Here are the requirements:\n1. The $API_CALL should have only 1 line of code that calls api.\n2. The $API_PROVIDER should be the programming framework used.\n3. $EXPLANATION should be a step-by-step explanation.\n4. The $CODE is the python code.\n5. Do not repeat the format in your answer.", "category": "generic"}
{"question_id": 54, "text": " I want to build a chatbot that is used by language learners who want to communicate in French while they only know English. Generate a response for an English message.\n \n Use this API documentation for reference:  {\"domain\": \"Natural Language Processing Translation\", \"framework\": \"Hugging Face\", \"functionality\": \"Translation\", \"api_name\": \"Helsinki-NLP/opus-mt-en-fr\", \"api_call\": \"translate('input_text', model='Helsinki-NLP/opus-mt-en-fr')\", \"api_arguments\": [\"input_text\"], \"python_environment_requirements\": [\"transformers\"], \"example_code\": \"\", \"performance\": {\"dataset\": \"opus\", \"accuracy\": {\"BLEU\": {\"newsdiscussdev2015-enfr.en.fr\": 33.8, \"newsdiscusstest2015-enfr.en.fr\": 40.0, \"newssyscomb2009.en.fr\": 29.8, \"news-test2008.en.fr\": 27.5, \"newstest2009.en.fr\": 29.4, \"newstest2010.en.fr\": 32.7, \"newstest2011.en.fr\": 34.3, \"newstest2012.en.fr\": 31.8, \"newstest2013.en.fr\": 33.2, \"Tatoeba.en.fr\": 50.5}}}, \"description\": \"Helsinki-NLP/opus-mt-en-fr is a translation model that translates English text to French using the Hugging Face Transformers library. It is based on the OPUS dataset and uses a transformer-align architecture with normalization and SentencePiece pre-processing.\"}}\nWrite a python program in 1 to 2 lines to call API in huggingface.\n\nThe answer should follow the format: {<<<domain>>> $DOMAIN, <<<api_call>>>: $API_CALL, <<<api_provider>>>: $API_PROVIDER, <<<explanation>>>: $EXPLANATION, <<<code>>>: $CODE}. Here are the requirements:\n1. The $API_CALL should have only 1 line of code that calls api.\n2. The $API_PROVIDER should be the programming framework used.\n3. $EXPLANATION should be a step-by-step explanation.\n4. The $CODE is the python code.\n5. Do not repeat the format in your answer.", "category": "generic"}
{"question_id": 55, "text": " Translate the following text from French to English: \\u201cLe syst\\u00e8me \\u00e9ducatif fran\\u00e7ais est compos\\u00e9 d'\\u00e9coles maternelles, d'\\u00e9coles \\u00e9l\\u00e9mentaires, de coll\\u00e8ges et de lyc\\u00e9es.\\u201d\n###Input: Le syst\\u00e8me \\u00e9ducatif fran\\u00e7ais est compos\\u00e9 d'\\u00e9coles maternelles, d'\\u00e9coles \\u00e9l\\u00e9mentaires, de coll\\u00e8ges et de lyc\\u00e9es.\n \n Use this API documentation for reference:  {\"domain\": \"Natural Language Processing Translation\", \"framework\": \"Transformers\", \"functionality\": \"Translation\", \"api_name\": \"opus-mt-fr-en\", \"api_call\": \"pipeline('translation_fr_to_en', model='Helsinki-NLP/opus-mt-fr-en')\", \"api_arguments\": [\"text\"], \"python_environment_requirements\": [\"transformers\", \"torch\"], \"example_code\": \"translation_pipeline('Bonjour, comment \\u00e7a va?')\", \"performance\": {\"dataset\": \"opus\", \"accuracy\": {\"BLEU\": {\"newsdiscussdev2015-enfr.fr.en\": 33.1, \"newsdiscusstest2015-enfr.fr.en\": 38.7, \"newssyscomb2009.fr.en\": 30.3, \"news-test2008.fr.en\": 26.2, \"newstest2009.fr.en\": 30.2, \"newstest2010.fr.en\": 32.2, \"newstest2011.fr.en\": 33.0, \"newstest2012.fr.en\": 32.8, \"newstest2013.fr.en\": 33.9, \"newstest2014-fren.fr.en\": 37.8, \"Tatoeba.fr.en\": 57.5}}}, \"description\": \"Helsinki-NLP/opus-mt-fr-en is a machine translation model trained to translate from French to English. It is based on the Marian NMT framework and trained on the OPUS dataset.\"}}\nWrite a python program in 1 to 2 lines to call API in huggingface.\n\nThe answer should follow the format: {<<<domain>>> $DOMAIN, <<<api_call>>>: $API_CALL, <<<api_provider>>>: $API_PROVIDER, <<<explanation>>>: $EXPLANATION, <<<code>>>: $CODE}. Here are the requirements:\n1. The $API_CALL should have only 1 line of code that calls api.\n2. The $API_PROVIDER should be the programming framework used.\n3. $EXPLANATION should be a step-by-step explanation.\n4. The $CODE is the python code.\n5. Do not repeat the format in your answer.", "category": "generic"}
{"question_id": 56, "text": " I want to translate a text from one language to another.\n \n Use this API documentation for reference:  {\"domain\": \"Natural Language Processing Translation\", \"framework\": \"PyTorch Transformers\", \"functionality\": \"text2text-generation\", \"api_name\": \"facebook/nllb-200-distilled-600M\", \"api_call\": \"pipeline('translation_xx_to_yy', model='facebook/nllb-200-distilled-600M')\", \"api_arguments\": [\"model\", \"text\"], \"python_environment_requirements\": [\"transformers\", \"torch\"], \"example_code\": \"from transformers import pipeline; translator = pipeline('translation_xx_to_yy', model='facebook/nllb-200-distilled-600M'); translator('Hello World')\", \"performance\": {\"dataset\": \"Flores-200\", \"accuracy\": \"BLEU, spBLEU, chrF++\"}, \"description\": \"NLLB-200 is a machine translation model primarily intended for research in machine translation, especially for low-resource languages. It allows for single sentence translation among 200 languages. The model was trained on general domain text data and is not intended to be used with domain specific texts, such as medical domain or legal domain. The model is not intended to be used for document translation.\"}}\nWrite a python program in 1 to 2 lines to call API in huggingface.\n\nThe answer should follow the format: {<<<domain>>> $DOMAIN, <<<api_call>>>: $API_CALL, <<<api_provider>>>: $API_PROVIDER, <<<explanation>>>: $EXPLANATION, <<<code>>>: $CODE}. Here are the requirements:\n1. The $API_CALL should have only 1 line of code that calls api.\n2. The $API_PROVIDER should be the programming framework used.\n3. $EXPLANATION should be a step-by-step explanation.\n4. The $CODE is the python code.\n5. Do not repeat the format in your answer.", "category": "generic"}
{"question_id": 57, "text": " Our team member has written a long article that needs to be published on a company blog. Can you provide a shorter summary to be used as a snippet on the landing page?\n###Input: \\\"Apple Inc. reported its quarterly earnings results yesterday. The company posted a record-breaking revenue of $123.9 billion for the first quarter of 2022, up by 11% from the same period last year. The increase was fueled by stronger demand for iPhones, iPads, and Macs, as well as continued growth in its services segment. Apple's operating profit for the quarter came in at $38.3 billion, up 17% from a year earlier. The results surpassed analysts' expectations, who had anticipated revenue of around $118 billion. This strong performance is largely attributed to the successful launch of the iPhone 13, which has enjoyed robust sales since its debut in September. Apple CEO Tim Cook said in a statement, \\\"Our record-breaking quarter reflects the strength of our entire ecosystem, from our innovative products and services to the unmatched dedication of our teams around the world.\\\" Despite the ongoing global supply chain disruptions, Apple has managed to maintain its growth trajectory, thanks in part to its vertically integrated operations and nimble supply chain management. The company is expected to face stiffer competition going forward, particularly in the smartphone market, as rivals introduce new devices and increased pricing pressures.\\\"\n \n Use this API documentation for reference:  {\"domain\": \"Natural Language Processing Summarization\", \"framework\": \"Hugging Face Transformers\", \"functionality\": \"Text Summarization\", \"api_name\": \"facebook/bart-large-cnn\", \"api_call\": \"pipeline('summarization', model='facebook/bart-large-cnn')\", \"api_arguments\": [\"ARTICLE\", \"max_length\", \"min_length\", \"do_sample\"], \"python_environment_requirements\": [\"transformers\"], \"example_code\": \"from transformers import pipeline\nsummarizer = pipeline(summarization, model=facebook/bart-large-cnn)\nARTICLE = ...\nprint(summarizer(ARTICLE, max_length=130, min_length=30, do_sample=False))\", \"performance\": {\"dataset\": \"cnn_dailymail\", \"accuracy\": {\"ROUGE-1\": 42.949, \"ROUGE-2\": 20.815, \"ROUGE-L\": 30.619, \"ROUGE-LSUM\": 40.038}}, \"description\": \"BART (large-sized model), fine-tuned on CNN Daily Mail. BART is a transformer encoder-encoder (seq2seq) model with a bidirectional (BERT-like) encoder and an autoregressive (GPT-like) decoder. BART is pre-trained by (1) corrupting text with an arbitrary noising function, and (2) learning a model to reconstruct the original text. BART is particularly effective when fine-tuned for text generation (e.g. summarization, translation) but also works well for comprehension tasks (e.g. text classification, question answering). This particular checkpoint has been fine-tuned on CNN Daily Mail, a large collection of text-summary pairs.\"}}\nWrite a python program in 1 to 2 lines to call API in huggingface.\n\nThe answer should follow the format: {<<<domain>>> $DOMAIN, <<<api_call>>>: $API_CALL, <<<api_provider>>>: $API_PROVIDER, <<<explanation>>>: $EXPLANATION, <<<code>>>: $CODE}. Here are the requirements:\n1. The $API_CALL should have only 1 line of code that calls api.\n2. The $API_PROVIDER should be the programming framework used.\n3. $EXPLANATION should be a step-by-step explanation.\n4. The $CODE is the python code.\n5. Do not repeat the format in your answer.", "category": "generic"}
{"question_id": 58, "text": " Write a summary of a conference held by the World Health Organization discussing the impacts of climate change on human health.\n###Input: Over the past week, the World Health Organization held a conference discussing the impacts of climate change on human health. The conference brought together leading experts from around the world to examine the current problems affecting people's health due to changing environmental conditions. The topics of discussion included increased occurrence of heat-related illnesses, heightened rates of vector-borne diseases, and the growing problem of air pollution. The conference concluded with a call to action for governments and organizations to invest in mitigating and adapting to the negative consequences of climate change for the sake of public health.\n \n Use this API documentation for reference:  {\"domain\": \"Natural Language Processing Summarization\", \"framework\": \"Hugging Face Transformers\", \"functionality\": \"Summarization\", \"api_name\": \"google/pegasus-xsum\", \"api_call\": \"pipeline('summarization', model='google/pegasus-xsum')\", \"api_arguments\": \"\", \"python_environment_requirements\": \"transformers\", \"example_code\": \"\", \"performance\": {\"dataset\": [{\"name\": \"xsum\", \"accuracy\": {\"ROUGE-1\": 46.862, \"ROUGE-2\": 24.453, \"ROUGE-L\": 39.055, \"ROUGE-LSUM\": 39.099}}, {\"name\": \"cnn_dailymail\", \"accuracy\": {\"ROUGE-1\": 22.206, \"ROUGE-2\": 7.67, \"ROUGE-L\": 15.405, \"ROUGE-LSUM\": 19.218}}, {\"name\": \"samsum\", \"accuracy\": {\"ROUGE-1\": 21.81, \"ROUGE-2\": 4.253, \"ROUGE-L\": 17.447, \"ROUGE-LSUM\": 18.891}}]}, \"description\": \"PEGASUS is a pre-trained model for abstractive summarization, developed by Google. It is based on the Transformer architecture and trained on both C4 and HugeNews datasets. The model is designed to extract gap sentences and generate summaries by stochastically sampling important sentences.\"}}\nWrite a python program in 1 to 2 lines to call API in huggingface.\n\nThe answer should follow the format: {<<<domain>>> $DOMAIN, <<<api_call>>>: $API_CALL, <<<api_provider>>>: $API_PROVIDER, <<<explanation>>>: $EXPLANATION, <<<code>>>: $CODE}. Here are the requirements:\n1. The $API_CALL should have only 1 line of code that calls api.\n2. The $API_PROVIDER should be the programming framework used.\n3. $EXPLANATION should be a step-by-step explanation.\n4. The $CODE is the python code.\n5. Do not repeat the format in your answer.", "category": "generic"}
{"question_id": 59, "text": " Please provide a brief overview of a news article.\n###Input: A new study suggests that eating chocolate at least once a week can lead to better cognition. The study, published in the journal Appetite, analyzed data from over 900 adults and found that individuals who consumed chocolate at least once a week performed better on cognitive tests than those who consumed chocolate less frequently. Researchers believe that the beneficial effects of chocolate on cognition may be due to the presence of flavonoids, which have been shown to be antioxidant-rich and to improve brain blood flow.\n \n Use this API documentation for reference:  {\"domain\": \"Natural Language Processing Summarization\", \"framework\": \"Hugging Face Transformers\", \"functionality\": \"text2text-generation\", \"api_name\": \"google/pegasus-cnn_dailymail\", \"api_call\": \"PegasusForConditionalGeneration.from_pretrained('google/pegasus-cnn_dailymail')\", \"api_arguments\": [\"model_name\"], \"python_environment_requirements\": [\"transformers\"], \"example_code\": \"from transformers import PegasusForConditionalGeneration, PegasusTokenizer\nmodel_name = 'google/pegasus-cnn_dailymail'\ntokenizer = PegasusTokenizer.from_pretrained(model_name)\nmodel = PegasusForConditionalGeneration.from_pretrained(model_name)\ninputs = tokenizer.encode('input_text', return_tensors='pt')\nsummary_ids = model.generate(inputs)\nsummary = tokenizer.decode(summary_ids[0], skip_special_tokens=True)\", \"performance\": {\"dataset\": \"cnn_dailymail\", \"accuracy\": \"44.16/21.56/41.30\"}, \"description\": \"PEGASUS model for abstractive summarization, pretrained on the CNN/DailyMail dataset.\"}}\nWrite a python program in 1 to 2 lines to call API in huggingface.\n\nThe answer should follow the format: {<<<domain>>> $DOMAIN, <<<api_call>>>: $API_CALL, <<<api_provider>>>: $API_PROVIDER, <<<explanation>>>: $EXPLANATION, <<<code>>>: $CODE}. Here are the requirements:\n1. The $API_CALL should have only 1 line of code that calls api.\n2. The $API_PROVIDER should be the programming framework used.\n3. $EXPLANATION should be a step-by-step explanation.\n4. The $CODE is the python code.\n5. Do not repeat the format in your answer.", "category": "generic"}
{"question_id": 60, "text": " I developed a document generation app, I need to create a summary of a long article given as input to provide to my users before they read the full article.\n \n Use this API documentation for reference:  {\"domain\": \"Natural Language Processing Summarization\", \"framework\": \"Hugging Face Transformers\", \"functionality\": \"text2text-generation\", \"api_name\": \"google/bigbird-pegasus-large-bigpatent\", \"api_call\": \"BigBirdPegasusForConditionalGeneration.from_pretrained('google/bigbird-pegasus-large-bigpatent')\", \"api_arguments\": {\"attention_type\": \"original_full\", \"block_size\": 16, \"num_random_blocks\": 2}, \"python_environment_requirements\": \"transformers\", \"example_code\": \"from transformers import BigBirdPegasusForConditionalGeneration, AutoTokenizer\ntokenizer = AutoTokenizer.from_pretrained(google/bigbird-pegasus-large-bigpatent)\nmodel = BigBirdPegasusForConditionalGeneration.from_pretrained(google/bigbird-pegasus-large-bigpatent)\ntext = Replace me by any text you'd like.\ninputs = tokenizer(text, return_tensors='pt')\nprediction = model.generate(**inputs)\nprediction = tokenizer.batch_decode(prediction)\", \"performance\": {\"dataset\": \"big_patent\", \"accuracy\": \"Not provided\"}, \"description\": \"BigBird, a sparse-attention based transformer, extends Transformer-based models like BERT to much longer sequences. It can handle sequences up to a length of 4096 at a much lower compute cost compared to BERT. BigBird has achieved state-of-the-art results on various tasks involving very long sequences such as long documents summarization and question-answering with long contexts.\"}}\nWrite a python program in 1 to 2 lines to call API in huggingface.\n\nThe answer should follow the format: {<<<domain>>> $DOMAIN, <<<api_call>>>: $API_CALL, <<<api_provider>>>: $API_PROVIDER, <<<explanation>>>: $EXPLANATION, <<<code>>>: $CODE}. Here are the requirements:\n1. The $API_CALL should have only 1 line of code that calls api.\n2. The $API_PROVIDER should be the programming framework used.\n3. $EXPLANATION should be a step-by-step explanation.\n4. The $CODE is the python code.\n5. Do not repeat the format in your answer.", "category": "generic"}
{"question_id": 61, "text": " We need a quick summary of a news article we found online. Can you help us with that?\n###Input: Videos that say approved vaccines are dangerous and cause autism, cancer or infertility are among those that will be taken down, the company said. The policy includes the termination of accounts of anti-vaccine influencers. Tech giants have been criticised for not doing more to counter false health information on their sites. In July, US President Joe Biden said social media platforms were largely responsible for people's scepticism in getting vaccinated by spreading misinformation, and appealed for them to address the issue. YouTube, which is owned by Google, said 130,000 videos were removed from its platform since last year, when it implemented a ban on content spreading misinformation about Covid vaccines. In a blog post, the company said it had seen false claims about Covid jabs spill over into misinformation about vaccines in general. The new policy covers long-approved vaccines, such as those against measles or hepatitis B. We're expanding our medical misinformation policies on YouTube with new guidelines on currently administered vaccines that are approved and confirmed to be safe and effective by local health authorities and the WHO, the post said, referring to the World Health Organization.\n \n Use this API documentation for reference:  {\"domain\": \"Natural Language Processing Summarization\", \"framework\": \"Transformers\", \"functionality\": \"text2text-generation\", \"api_name\": \"csebuetnlp/mT5_multilingual_XLSum\", \"api_call\": \"AutoModelForSeq2SeqLM.from_pretrained('csebuetnlp/mT5_multilingual_XLSum')\", \"api_arguments\": [\"model_name\"], \"python_environment_requirements\": [\"transformers==4.11.0.dev0\"], \"example_code\": \"import re\nfrom transformers import AutoTokenizer, AutoModelForSeq2SeqLM\nWHITESPACE_HANDLER = lambda k: re.sub('\\\\s+', ' ', re.sub('\\\n+', ' ', k.strip()))\narticle_text = Videos that say approved vaccines are dangerous and cause autism, cancer or infertility are among those that will be taken down, the company said. The policy includes the termination of accounts of anti-vaccine influencers. Tech giants have been criticised for not doing more to counter false health information on their sites. In July, US President Joe Biden said social media platforms were largely responsible for people's scepticism in getting vaccinated by spreading misinformation, and appealed for them to address the issue. YouTube, which is owned by Google, said 130,000 videos were removed from its platform since last year, when it implemented a ban on content spreading misinformation about Covid vaccines. In a blog post, the company said it had seen false claims about Covid jabs spill over into misinformation about vaccines in general. The new policy covers long-approved vaccines, such as those against measles or hepatitis B. We're expanding our medical misinformation policies on YouTube with new guidelines on currently administered vaccines that are approved and confirmed to be safe and effective by local health authorities and the WHO, the post said, referring to the World Health Organization.\nmodel_name = csebuetnlp/mT5_multilingual_XLSum\ntokenizer = AutoTokenizer.from_pretrained(model_name)\nmodel = AutoModelForSeq2SeqLM.from_pretrained(model_name)\ninput_ids = tokenizer(\n [WHITESPACE_HANDLER(article_text)],\n return_tensors=pt,\n padding=max_length,\n truncation=True,\n max_length=512\n)[input_ids]\noutput_ids = model.generate(\n input_ids=input_ids,\n max_length=84,\n no_repeat_ngram_size=2,\n num_beams=4\n)[0]\nsummary = tokenizer.decode(\n output_ids,\n skip_special_tokens=True,\n clean_up_tokenization_spaces=False\n)\nprint(summary)\", \"performance\": {\"dataset\": \"xsum\", \"accuracy\": {\"ROUGE-1\": 36.5, \"ROUGE-2\": 13.934, \"ROUGE-L\": 28.988, \"ROUGE-LSUM\": 28.996, \"loss\": 2.067, \"gen_len\": 26.973}}, \"description\": \"This repository contains the mT5 checkpoint finetuned on the 45 languages of XL-Sum dataset. It is a multilingual abstractive summarization model that supports text-to-text generation for 43 languages.\"}}\nWrite a python program in 1 to 2 lines to call API in huggingface.\n\nThe answer should follow the format: {<<<domain>>> $DOMAIN, <<<api_call>>>: $API_CALL, <<<api_provider>>>: $API_PROVIDER, <<<explanation>>>: $EXPLANATION, <<<code>>>: $CODE}. Here are the requirements:\n1. The $API_CALL should have only 1 line of code that calls api.\n2. The $API_PROVIDER should be the programming framework used.\n3. $EXPLANATION should be a step-by-step explanation.\n4. The $CODE is the python code.\n5. Do not repeat the format in your answer.", "category": "generic"}
{"question_id": 62, "text": " We'd like our chatbot to act as a fictional character for engaging with our users.\n \n Use this API documentation for reference:  {\"domain\": \"Natural Language Processing Conversational\", \"framework\": \"Hugging Face Transformers\", \"functionality\": \"Text Generation\", \"api_name\": \"pygmalion-6b\", \"api_call\": \"AutoModelForCausalLM.from_pretrained('waifu-workshop/pygmalion-6b')\", \"api_arguments\": [\"input_ids\", \"max_length\", \"num_return_sequences\"], \"python_environment_requirements\": [\"transformers\", \"torch\"], \"example_code\": \"from transformers import AutoTokenizer, AutoModelForCausalLM\ntokenizer = AutoTokenizer.from_pretrained('waifu-workshop/pygmalion-6b')\nmodel = AutoModelForCausalLM.from_pretrained('waifu-workshop/pygmalion-6b')\ninput_text = [CHARACTER]'s Persona: [A few sentences about the character you want the model to play]\\\n<START>\\\n[DIALOGUE HISTORY]\\\nYou: [Your input message here]\\\n[CHARACTER]:\ninput_ids = tokenizer.encode(input_text, return_tensors='pt')\noutput = model.generate(input_ids, max_length=100, num_return_sequences=1)\noutput_text = tokenizer.decode(output[0], skip_special_tokens=True)\", \"performance\": {\"dataset\": \"56MB of dialogue data gathered from multiple sources\", \"accuracy\": \"Not specified\"}, \"description\": \"Pygmalion 6B is a proof-of-concept dialogue model based on EleutherAI's GPT-J-6B. It is fine-tuned on 56MB of dialogue data gathered from multiple sources, which includes both real and partially machine-generated conversations. The model is intended for conversational text generation and can be used to play a character in a dialogue.\"}}\nWrite a python program in 1 to 2 lines to call API in huggingface.\n\nThe answer should follow the format: {<<<domain>>> $DOMAIN, <<<api_call>>>: $API_CALL, <<<api_provider>>>: $API_PROVIDER, <<<explanation>>>: $EXPLANATION, <<<code>>>: $CODE}. Here are the requirements:\n1. The $API_CALL should have only 1 line of code that calls api.\n2. The $API_PROVIDER should be the programming framework used.\n3. $EXPLANATION should be a step-by-step explanation.\n4. The $CODE is the python code.\n5. Do not repeat the format in your answer.", "category": "generic"}
{"question_id": 63, "text": " Write a story about a spaceship journey to a distant planet in search of a new home for humanity.\n \n Use this API documentation for reference:  {\"domain\": \"Natural Language Processing Text Generation\", \"framework\": \"Transformers\", \"functionality\": \"Text Generation\", \"api_name\": \"EleutherAI/gpt-j-6B\", \"api_call\": \"AutoModelForCausalLM.from_pretrained('EleutherAI/gpt-j-6B')\", \"api_arguments\": {\"pretrained_model\": \"EleutherAI/gpt-j-6B\"}, \"python_environment_requirements\": {\"transformers\": \"AutoTokenizer, AutoModelForCausalLM\"}, \"example_code\": {\"loading_model\": \"from transformers import AutoTokenizer, AutoModelForCausalLM\ntokenizer = AutoTokenizer.from_pretrained(EleutherAI/gpt-j-6B)\nmodel = AutoModelForCausalLM.from_pretrained(EleutherAI/gpt-j-6B)\"}, \"performance\": {\"dataset\": \"the_pile\", \"accuracy\": {\"LAMBADA_PPL\": 3.99, \"LAMBADA_Acc\": \"69.7%\", \"Winogrande\": \"65.3%\", \"Hellaswag\": \"66.1%\", \"PIQA\": \"76.5%\"}}, \"description\": \"GPT-J 6B is a transformer model trained using Ben Wang's Mesh Transformer JAX. It consists of 28 layers with a model dimension of 4096, and a feedforward dimension of 16384. The model dimension is split into 16 heads, each with a dimension of 256. Rotary Position Embedding (RoPE) is applied to 64 dimensions of each head. The model is trained with a tokenization vocabulary of 50257, using the same set of BPEs as GPT-2/GPT-3. GPT-J 6B was trained on the Pile, a large-scale curated dataset created by EleutherAI.\"}}\nWrite a python program in 1 to 2 lines to call API in huggingface.\n\nThe answer should follow the format: {<<<domain>>> $DOMAIN, <<<api_call>>>: $API_CALL, <<<api_provider>>>: $API_PROVIDER, <<<explanation>>>: $EXPLANATION, <<<code>>>: $CODE}. Here are the requirements:\n1. The $API_CALL should have only 1 line of code that calls api.\n2. The $API_PROVIDER should be the programming framework used.\n3. $EXPLANATION should be a step-by-step explanation.\n4. The $CODE is the python code.\n5. Do not repeat the format in your answer.", "category": "generic"}
{"question_id": 64, "text": " I want to write a story about a brave knight and a dragon but I'm unable to come up with a good start. Help me with that.\n \n Use this API documentation for reference:  {\"domain\": \"Natural Language Processing Text Generation\", \"framework\": \"Transformers\", \"functionality\": \"Text Generation\", \"api_name\": \"sshleifer/tiny-gpt2\", \"api_call\": \"TinyGPT2LMHeadModel.from_pretrained('sshleifer/tiny-gpt2')\", \"api_arguments\": {\"model\": \"sshleifer/tiny-gpt2\"}, \"python_environment_requirements\": {\"huggingface_transformers\": \">=4.0.0\"}, \"example_code\": \"from transformers import pipeline\nnlp = pipeline('text-generation', model='sshleifer/tiny-gpt2')\nresult = nlp('Once upon a time')\", \"performance\": {\"dataset\": \"N/A\", \"accuracy\": \"N/A\"}, \"description\": \"A tiny GPT-2 model for text generation, suitable for low-resource environments and faster inference. This model is part of the Hugging Face Transformers library and can be used for generating text given a prompt.\"}}\nWrite a python program in 1 to 2 lines to call API in huggingface.\n\nThe answer should follow the format: {<<<domain>>> $DOMAIN, <<<api_call>>>: $API_CALL, <<<api_provider>>>: $API_PROVIDER, <<<explanation>>>: $EXPLANATION, <<<code>>>: $CODE}. Here are the requirements:\n1. The $API_CALL should have only 1 line of code that calls api.\n2. The $API_PROVIDER should be the programming framework used.\n3. $EXPLANATION should be a step-by-step explanation.\n4. The $CODE is the python code.\n5. Do not repeat the format in your answer.", "category": "generic"}
{"question_id": 65, "text": " I need a text analysis tool that can automatically predict the most plausible missing text in a given sentence.\n \n Use this API documentation for reference:  {\"domain\": \"Natural Language Processing Fill-Mask\", \"framework\": \"Transformers\", \"functionality\": \"Masked Language Modeling\", \"api_name\": \"albert-base-v2\", \"api_call\": \"pipeline('fill-mask', model='albert-base-v2')\", \"api_arguments\": [\"text\"], \"python_environment_requirements\": [\"transformers\"], \"example_code\": \"from transformers import pipeline\nunmasker = pipeline('fill-mask', model='albert-base-v2')\nunmasker(Hello I'm a [MASK] model.)\", \"performance\": {\"dataset\": {\"SQuAD1.1\": \"90.2/83.2\", \"SQuAD2.0\": \"82.1/79.3\", \"MNLI\": \"84.6\", \"SST-2\": \"92.9\", \"RACE\": \"66.8\"}, \"accuracy\": \"82.3\"}, \"description\": \"ALBERT Base v2 is a transformers model pretrained on a large corpus of English data in a self-supervised fashion using a masked language modeling (MLM) objective. It was introduced in this paper and first released in this repository. This model, as all ALBERT models, is uncased: it does not make a difference between english and English.\"}}\nWrite a python program in 1 to 2 lines to call API in huggingface.\n\nThe answer should follow the format: {<<<domain>>> $DOMAIN, <<<api_call>>>: $API_CALL, <<<api_provider>>>: $API_PROVIDER, <<<explanation>>>: $EXPLANATION, <<<code>>>: $CODE}. Here are the requirements:\n1. The $API_CALL should have only 1 line of code that calls api.\n2. The $API_PROVIDER should be the programming framework used.\n3. $EXPLANATION should be a step-by-step explanation.\n4. The $CODE is the python code.\n5. Do not repeat the format in your answer.", "category": "generic"}
{"question_id": 66, "text": " Help me fill in the blanks in the following Chinese sentence: \\\"\\u4e0a\\u6d77\\u662f\\u4e2d\\u56fd\\u7684[MASK]\\u5927\\u57ce\\u5e02\\u3002\\\"\n###Input: \\u4e0a\\u6d77\\u662f\\u4e2d\\u56fd\\u7684[MASK]\\u5927\\u57ce\\u5e02\\u3002\n \n Use this API documentation for reference:  {\"domain\": \"Natural Language Processing Fill-Mask\", \"framework\": \"Transformers\", \"functionality\": \"Fill-Mask\", \"api_name\": \"uer/albert-base-chinese-cluecorpussmall\", \"api_call\": \"AlbertForMaskedLM.from_pretrained('uer/albert-base-chinese-cluecorpussmall')\", \"api_arguments\": [\"model\", \"tokenizer\"], \"python_environment_requirements\": [\"transformers\"], \"example_code\": \"from transformers import BertTokenizer, AlbertForMaskedLM, FillMaskPipeline\ntokenizer = BertTokenizer.from_pretrained(uer/albert-base-chinese-cluecorpussmall)\nmodel = AlbertForMaskedLM.from_pretrained(uer/albert-base-chinese-cluecorpussmall)\nunmasker = FillMaskPipeline(model, tokenizer)\nunmasker(\\u4e2d\\u56fd\\u7684\\u9996\\u90fd\\u662f[MASK]\\u4eac\\u3002)\", \"performance\": {\"dataset\": \"CLUECorpusSmall\", \"accuracy\": \"Not provided\"}, \"description\": \"This is the set of Chinese ALBERT models pre-trained by UER-py on the CLUECorpusSmall dataset. The model can be used for tasks like text generation and feature extraction.\"}}\nWrite a python program in 1 to 2 lines to call API in huggingface.\n\nThe answer should follow the format: {<<<domain>>> $DOMAIN, <<<api_call>>>: $API_CALL, <<<api_provider>>>: $API_PROVIDER, <<<explanation>>>: $EXPLANATION, <<<code>>>: $CODE}. Here are the requirements:\n1. The $API_CALL should have only 1 line of code that calls api.\n2. The $API_PROVIDER should be the programming framework used.\n3. $EXPLANATION should be a step-by-step explanation.\n4. The $CODE is the python code.\n5. Do not repeat the format in your answer.", "category": "generic"}
{"question_id": 67, "text": " We are building a source code autocompletion tool which will complete the code snippet containing a masked token.\n \n Use this API documentation for reference:  {\"domain\": \"Natural Language Processing Fill-Mask\", \"framework\": \"Transformers\", \"functionality\": \"Masked Language Modeling Prediction\", \"api_name\": \"CodeBERTa-small-v1\", \"api_call\": \"pipeline('fill-mask', model='huggingface/CodeBERTa-small-v1')\", \"api_arguments\": [\"task\", \"model\", \"tokenizer\"], \"python_environment_requirements\": [\"transformers\"], \"example_code\": \"fill_mask(PHP_CODE)\", \"performance\": {\"dataset\": \"code_search_net\", \"accuracy\": null}, \"description\": \"CodeBERTa is a RoBERTa-like model trained on the CodeSearchNet dataset from GitHub. It supports languages like Go, Java, JavaScript, PHP, Python, and Ruby. The tokenizer is a Byte-level BPE tokenizer trained on the corpus using Hugging Face tokenizers. The small model is a 6-layer, 84M parameters, RoBERTa-like Transformer model.\"}}\nWrite a python program in 1 to 2 lines to call API in huggingface.\n\nThe answer should follow the format: {<<<domain>>> $DOMAIN, <<<api_call>>>: $API_CALL, <<<api_provider>>>: $API_PROVIDER, <<<explanation>>>: $EXPLANATION, <<<code>>>: $CODE}. Here are the requirements:\n1. The $API_CALL should have only 1 line of code that calls api.\n2. The $API_PROVIDER should be the programming framework used.\n3. $EXPLANATION should be a step-by-step explanation.\n4. The $CODE is the python code.\n5. Do not repeat the format in your answer.", "category": "generic"}
{"question_id": 68, "text": " I work for a Japanese company, and my manager needs me to take a look at a request from a client. I can understand fluent Japanese, but I need a little help filling in missing words from the text.\n \n Use this API documentation for reference:  {\"domain\": \"Natural Language Processing Fill-Mask\", \"framework\": \"Transformers\", \"functionality\": \"Fill-Mask\", \"api_name\": \"cl-tohoku/bert-base-japanese\", \"api_call\": \"AutoModelForMaskedLM.from_pretrained('cl-tohoku/bert-base-japanese')\", \"api_arguments\": [\"text\"], \"python_environment_requirements\": [\"transformers\"], \"example_code\": \"fill_mask('[MASK]')\", \"performance\": {\"dataset\": \"wikipedia\", \"accuracy\": \"N/A\"}, \"description\": \"This is a BERT model pretrained on texts in the Japanese language. This version of the model processes input texts with word-level tokenization based on the IPA dictionary, followed by the WordPiece subword tokenization.\"}}\nWrite a python program in 1 to 2 lines to call API in huggingface.\n\nThe answer should follow the format: {<<<domain>>> $DOMAIN, <<<api_call>>>: $API_CALL, <<<api_provider>>>: $API_PROVIDER, <<<explanation>>>: $EXPLANATION, <<<code>>>: $CODE}. Here are the requirements:\n1. The $API_CALL should have only 1 line of code that calls api.\n2. The $API_PROVIDER should be the programming framework used.\n3. $EXPLANATION should be a step-by-step explanation.\n4. The $CODE is the python code.\n5. Do not repeat the format in your answer.", "category": "generic"}
{"question_id": 69, "text": " We are building a platform to compare and contrast user input sentences with existing sentences in our database. It should return similar results.\n \n Use this API documentation for reference:  {\"domain\": \"Natural Language Processing Sentence Similarity\", \"framework\": \"Hugging Face Transformers\", \"functionality\": \"Sentence Transformers\", \"api_name\": \"sentence-transformers/paraphrase-distilroberta-base-v2\", \"api_call\": \"SentenceTransformer('sentence-transformers/paraphrase-distilroberta-base-v2')\", \"api_arguments\": [\"sentences\"], \"python_environment_requirements\": \"pip install -U sentence-transformers\", \"example_code\": \"from sentence_transformers import SentenceTransformer\nsentences = [This is an example sentence, Each sentence is converted]\nmodel = SentenceTransformer('sentence-transformers/paraphrase-distilroberta-base-v2')\nembeddings = model.encode(sentences)\nprint(embeddings)\", \"performance\": {\"dataset\": \"https://seb.sbert.net\", \"accuracy\": \"Automated evaluation\"}, \"description\": \"This is a sentence-transformers model: It maps sentences & paragraphs to a 768 dimensional dense vector space and can be used for tasks like clustering or semantic search.\"}}\nWrite a python program in 1 to 2 lines to call API in huggingface.\n\nThe answer should follow the format: {<<<domain>>> $DOMAIN, <<<api_call>>>: $API_CALL, <<<api_provider>>>: $API_PROVIDER, <<<explanation>>>: $EXPLANATION, <<<code>>>: $CODE}. Here are the requirements:\n1. The $API_CALL should have only 1 line of code that calls api.\n2. The $API_PROVIDER should be the programming framework used.\n3. $EXPLANATION should be a step-by-step explanation.\n4. The $CODE is the python code.\n5. Do not repeat the format in your answer.", "category": "generic"}
{"question_id": 70, "text": " I need a method to compare the similarity between two sentences to be used within a meme generator, so we can produce a meme with a similar caption.\n \n Use this API documentation for reference:  {\"domain\": \"Natural Language Processing Sentence Similarity\", \"framework\": \"Hugging Face Transformers\", \"functionality\": \"Sentence Embeddings\", \"api_name\": \"sentence-transformers/paraphrase-MiniLM-L3-v2\", \"api_call\": \"SentenceTransformer('sentence-transformers/paraphrase-MiniLM-L3-v2')\", \"api_arguments\": [\"sentences\"], \"python_environment_requirements\": \"pip install -U sentence-transformers\", \"example_code\": \"from sentence_transformers import SentenceTransformer\nsentences = [This is an example sentence, Each sentence is converted]\nmodel = SentenceTransformer('sentence-transformers/paraphrase-MiniLM-L3-v2')\nembeddings = model.encode(sentences)\nprint(embeddings)\", \"performance\": {\"dataset\": \"snli, multi_nli, ms_marco\", \"accuracy\": \"Not provided\"}, \"description\": \"This is a sentence-transformers model: It maps sentences & paragraphs to a 384 dimensional dense vector space and can be used for tasks like clustering or semantic search.\"}}\nWrite a python program in 1 to 2 lines to call API in huggingface.\n\nThe answer should follow the format: {<<<domain>>> $DOMAIN, <<<api_call>>>: $API_CALL, <<<api_provider>>>: $API_PROVIDER, <<<explanation>>>: $EXPLANATION, <<<code>>>: $CODE}. Here are the requirements:\n1. The $API_CALL should have only 1 line of code that calls api.\n2. The $API_PROVIDER should be the programming framework used.\n3. $EXPLANATION should be a step-by-step explanation.\n4. The $CODE is the python code.\n5. Do not repeat the format in your answer.", "category": "generic"}
{"question_id": 71, "text": " A student is writing a research paper and needs help with finding similar articles in order to include them in the literature review section.\n \n Use this API documentation for reference:  {\"domain\": \"Natural Language Processing Sentence Similarity\", \"framework\": \"Hugging Face Transformers\", \"functionality\": \"Sentence Embeddings\", \"api_name\": \"sentence-transformers/nli-mpnet-base-v2\", \"api_call\": \"SentenceTransformer('sentence-transformers/nli-mpnet-base-v2')\", \"api_arguments\": [\"sentences\"], \"python_environment_requirements\": \"pip install -U sentence-transformers\", \"example_code\": \"from sentence_transformers import SentenceTransformer\nsentences = [This is an example sentence, Each sentence is converted]\nmodel = SentenceTransformer('sentence-transformers/nli-mpnet-base-v2')\nembeddings = model.encode(sentences)\nprint(embeddings)\", \"performance\": {\"dataset\": \"https://seb.sbert.net\", \"accuracy\": \"Automated evaluation\"}, \"description\": \"This is a sentence-transformers model: It maps sentences & paragraphs to a 768 dimensional dense vector space and can be used for tasks like clustering or semantic search.\"}}\nWrite a python program in 1 to 2 lines to call API in huggingface.\n\nThe answer should follow the format: {<<<domain>>> $DOMAIN, <<<api_call>>>: $API_CALL, <<<api_provider>>>: $API_PROVIDER, <<<explanation>>>: $EXPLANATION, <<<code>>>: $CODE}. Here are the requirements:\n1. The $API_CALL should have only 1 line of code that calls api.\n2. The $API_PROVIDER should be the programming framework used.\n3. $EXPLANATION should be a step-by-step explanation.\n4. The $CODE is the python code.\n5. Do not repeat the format in your answer.", "category": "generic"}
{"question_id": 72, "text": " Create a solution to convert a given Japanese sentence into a speech audio file.\n \n Use this API documentation for reference:  {\"domain\": \"Audio Text-to-Speech\", \"framework\": \"ESPnet\", \"functionality\": \"Text-to-Speech\", \"api_name\": \"kan-bayashi_jvs_tts_finetune_jvs001_jsut_vits_raw_phn_jaconv_pyopenjta-truncated-178804\", \"api_call\": \"AutoModelForCausalLM.from_pretrained('espnet/kan-bayashi_jvs_tts_finetune_jvs001_jsut_vits_raw_phn_jaconv_pyopenjta-truncated-178804')\", \"api_arguments\": \"text\", \"python_environment_requirements\": \"transformers\", \"example_code\": \"\", \"performance\": {\"dataset\": \"\", \"accuracy\": \"\"}, \"description\": \"A Japanese text-to-speech model trained using the ESPnet framework. It is designed to convert text input into natural-sounding speech.\"}}\nWrite a python program in 1 to 2 lines to call API in huggingface.\n\nThe answer should follow the format: {<<<domain>>> $DOMAIN, <<<api_call>>>: $API_CALL, <<<api_provider>>>: $API_PROVIDER, <<<explanation>>>: $EXPLANATION, <<<code>>>: $CODE}. Here are the requirements:\n1. The $API_CALL should have only 1 line of code that calls api.\n2. The $API_PROVIDER should be the programming framework used.\n3. $EXPLANATION should be a step-by-step explanation.\n4. The $CODE is the python code.\n5. Do not repeat the format in your answer.", "category": "generic"}
{"question_id": 73, "text": " We are working on a transcription service for our customers. We need a way to convert audio files into text.\n \n Use this API documentation for reference:  {\"domain\": \"Audio Automatic Speech Recognition\", \"framework\": \"Hugging Face Transformers\", \"functionality\": \"Speech Recognition\", \"api_name\": \"jonatasgrosman/wav2vec2-large-xlsr-53-english\", \"api_call\": \"Wav2Vec2Model.from_pretrained('jonatasgrosman/wav2vec2-large-xlsr-53-english')\", \"api_arguments\": [\"audio_paths\"], \"python_environment_requirements\": [\"huggingsound\", \"torch\", \"librosa\", \"datasets\", \"transformers\"], \"example_code\": \"from huggingsound import SpeechRecognitionModel\nmodel = SpeechRecognitionModel('jonatasgrosman/wav2vec2-large-xlsr-53-english')\naudio_paths = ['/path/to/file.mp3', '/path/to/another_file.wav']\ntranscriptions = model.transcribe(audio_paths)\", \"performance\": {\"dataset\": \"mozilla-foundation/common_voice_6_0\", \"accuracy\": {\"Test WER\": 19.06, \"Test CER\": 7.69, \"Test WER (+LM)\": 14.81, \"Test CER (+LM)\": 6.84}}, \"description\": \"Fine-tuned facebook/wav2vec2-large-xlsr-53 on English using the train and validation splits of Common Voice 6.1. When using this model, make sure that your speech input is sampled at 16kHz.\"}}\nWrite a python program in 1 to 2 lines to call API in huggingface.\n\nThe answer should follow the format: {<<<domain>>> $DOMAIN, <<<api_call>>>: $API_CALL, <<<api_provider>>>: $API_PROVIDER, <<<explanation>>>: $EXPLANATION, <<<code>>>: $CODE}. Here are the requirements:\n1. The $API_CALL should have only 1 line of code that calls api.\n2. The $API_PROVIDER should be the programming framework used.\n3. $EXPLANATION should be a step-by-step explanation.\n4. The $CODE is the python code.\n5. Do not repeat the format in your answer.", "category": "generic"}
{"question_id": 74, "text": " We are creating an online video conference service, and we need to detect when two or more speakers are speaking at the same time in the audio.\n \n Use this API documentation for reference:  {\"domain\": \"Audio Automatic Speech Recognition\", \"framework\": \"pyannote.audio\", \"functionality\": \"overlapped-speech-detection\", \"api_name\": \"pyannote/overlapped-speech-detection\", \"api_call\": \"pipeline.from_pretrained('pyannote/overlapped-speech-detection', use_auth_token='ACCESS_TOKEN_GOES_HERE')\", \"api_arguments\": [\"audio.wav\"], \"python_environment_requirements\": [\"pyannote.audio 2.1\"], \"example_code\": \"from pyannote.audio import Pipeline\npipeline = Pipeline.from_pretrained(pyannote/overlapped-speech-detection, use_auth_token=ACCESS_TOKEN_GOES_HERE)\noutput = pipeline(audio.wav)\nfor speech in output.get_timeline().support():\n  # two or more speakers are active between speech.start and speech.end\n  ...\", \"performance\": {\"dataset\": \"ami\", \"accuracy\": null}, \"description\": \"Automatic overlapped speech detection using pyannote.audio framework. The model detects when two or more speakers are active in an audio file.\"}}\nWrite a python program in 1 to 2 lines to call API in huggingface.\n\nThe answer should follow the format: {<<<domain>>> $DOMAIN, <<<api_call>>>: $API_CALL, <<<api_provider>>>: $API_PROVIDER, <<<explanation>>>: $EXPLANATION, <<<code>>>: $CODE}. Here are the requirements:\n1. The $API_CALL should have only 1 line of code that calls api.\n2. The $API_PROVIDER should be the programming framework used.\n3. $EXPLANATION should be a step-by-step explanation.\n4. The $CODE is the python code.\n5. Do not repeat the format in your answer.", "category": "generic"}
{"question_id": 75, "text": " Our company develops smart speaker devices that involve interaction with the user. We need to transcribe the input from the users with the maintained accent or language.\n \n Use this API documentation for reference:  {\"domain\": \"Audio Automatic Speech Recognition\", \"framework\": \"Hugging Face Transformers\", \"functionality\": \"Transcription and Translation\", \"api_name\": \"openai/whisper-tiny\", \"api_call\": \"WhisperForConditionalGeneration.from_pretrained('openai/whisper-tiny')\", \"api_arguments\": {\"forced_decoder_ids\": \"WhisperProcessor.get_decoder_prompt_ids(language='english', task='transcribe')\"}, \"python_environment_requirements\": [\"transformers\", \"datasets\"], \"example_code\": [\"from transformers import WhisperProcessor, WhisperForConditionalGeneration\", \"from datasets import load_dataset\", \"processor = WhisperProcessor.from_pretrained('openai/whisper-tiny')\", \"model = WhisperForConditionalGeneration.from_pretrained('openai/whisper-tiny')\", \"model.config.forced_decoder_ids = None\", \"ds = load_dataset('hf-internal-testing/librispeech_asr_dummy', 'clean', split='validation')\", \"sample = ds[0]['audio']\", \"input_features = processor(sample['array'], sampling_rate=sample['sampling_rate'], return_tensors='pt').input_features\", \"predicted_ids = model.generate(input_features)\", \"transcription = processor.batch_decode(predicted_ids, skip_special_tokens=True)\"], \"performance\": {\"dataset\": \"LibriSpeech (clean)\", \"accuracy\": 7.54}, \"description\": \"Whisper is a pre-trained model for automatic speech recognition (ASR) and speech translation. Trained on 680k hours of labelled data, Whisper models demonstrate a strong ability to generalise to many datasets and domains without the need for fine-tuning. It is a Transformer-based encoder-decoder model that can be used for transcription and translation tasks.\"}}\nWrite a python program in 1 to 2 lines to call API in huggingface.\n\nThe answer should follow the format: {<<<domain>>> $DOMAIN, <<<api_call>>>: $API_CALL, <<<api_provider>>>: $API_PROVIDER, <<<explanation>>>: $EXPLANATION, <<<code>>>: $CODE}. Here are the requirements:\n1. The $API_CALL should have only 1 line of code that calls api.\n2. The $API_PROVIDER should be the programming framework used.\n3. $EXPLANATION should be a step-by-step explanation.\n4. The $CODE is the python code.\n5. Do not repeat the format in your answer.", "category": "generic"}
{"question_id": 76, "text": " One of our clients is facing noise issues on their audio recordings. Can you help them to remove the noise from the audio?\n \n Use this API documentation for reference:  {\"domain\": \"Audio Audio-to-Audio\", \"framework\": \"Hugging Face Transformers\", \"functionality\": \"speech-enhancement\", \"api_name\": \"speechbrain/metricgan-plus-voicebank\", \"api_call\": \"SpectralMaskEnhancement.from_hparams(source='speechbrain/metricgan-plus-voicebank', savedir='pretrained_models/metricgan-plus-voicebank')\", \"api_arguments\": {\"source\": \"speechbrain/metricgan-plus-voicebank\", \"savedir\": \"pretrained_models/metricgan-plus-voicebank\"}, \"python_environment_requirements\": \"pip install speechbrain\", \"example_code\": \"import torch\nimport torchaudio\nfrom speechbrain.pretrained import SpectralMaskEnhancement\nenhance_model = SpectralMaskEnhancement.from_hparams(\n source='speechbrain/metricgan-plus-voicebank',\n savedir='pretrained_models/metricgan-plus-voicebank',\n)\nnoisy = enhance_model.load_audio(\n 'speechbrain/metricgan-plus-voicebank/example.wav'\n).unsqueeze(0)\nenhanced = enhance_model.enhance_batch(noisy, lengths=torch.tensor([1.]))\ntorchaudio.save('enhanced.wav', enhanced.cpu(), 16000)\", \"performance\": {\"dataset\": \"Voicebank\", \"accuracy\": {\"Test PESQ\": \"3.15\", \"Test STOI\": \"93.0\"}}, \"description\": \"MetricGAN-trained model for Enhancement\"}}\nWrite a python program in 1 to 2 lines to call API in huggingface.\n\nThe answer should follow the format: {<<<domain>>> $DOMAIN, <<<api_call>>>: $API_CALL, <<<api_provider>>>: $API_PROVIDER, <<<explanation>>>: $EXPLANATION, <<<code>>>: $CODE}. Here are the requirements:\n1. The $API_CALL should have only 1 line of code that calls api.\n2. The $API_PROVIDER should be the programming framework used.\n3. $EXPLANATION should be a step-by-step explanation.\n4. The $CODE is the python code.\n5. Do not repeat the format in your answer.", "category": "generic"}
{"question_id": 77, "text": " We are a media company and we have a large volume of Chinese language audio files. We want to transcribe the audios into chinese text.\n \n Use this API documentation for reference:  {\"domain\": \"Audio Automatic Speech Recognition\", \"framework\": \"Hugging Face Transformers\", \"functionality\": \"Speech Recognition\", \"api_name\": \"jonatasgrosman/wav2vec2-large-xlsr-53-chinese-zh-cn\", \"api_call\": \"Wav2Vec2Model.from_pretrained('jonatasgrosman/wav2vec2-large-xlsr-53-chinese-zh-cn')\", \"api_arguments\": [\"audio_paths\"], \"python_environment_requirements\": [\"huggingsound\", \"torch\", \"librosa\", \"datasets\", \"transformers\"], \"example_code\": \"from huggingsound import SpeechRecognitionModel\nmodel = SpeechRecognitionModel('jonatasgrosman/wav2vec2-large-xlsr-53-chinese-zh-cn')\naudio_paths = ['/path/to/file.mp3', '/path/to/another_file.wav']\ntranscriptions = model.transcribe(audio_paths)\", \"performance\": {\"dataset\": \"Common Voice zh-CN\", \"accuracy\": {\"WER\": 82.37, \"CER\": 19.03}}, \"description\": \"Fine-tuned XLSR-53 large model for speech recognition in Chinese. Fine-tuned facebook/wav2vec2-large-xlsr-53 on Chinese using the train and validation splits of Common Voice 6.1, CSS10 and ST-CMDS.\"}}\nWrite a python program in 1 to 2 lines to call API in huggingface.\n\nThe answer should follow the format: {<<<domain>>> $DOMAIN, <<<api_call>>>: $API_CALL, <<<api_provider>>>: $API_PROVIDER, <<<explanation>>>: $EXPLANATION, <<<code>>>: $CODE}. Here are the requirements:\n1. The $API_CALL should have only 1 line of code that calls api.\n2. The $API_PROVIDER should be the programming framework used.\n3. $EXPLANATION should be a step-by-step explanation.\n4. The $CODE is the python code.\n5. Do not repeat the format in your answer.", "category": "generic"}
{"question_id": 78, "text": " Help us improve the listener experience from our customers by enhancing the audio of noisy recordings.\n \n Use this API documentation for reference:  {\"domain\": \"Audio Audio-to-Audio\", \"framework\": \"Hugging Face Transformers\", \"functionality\": \"Speech Enhancement\", \"api_name\": \"speechbrain/sepformer-whamr-enhancement\", \"api_call\": \"separator.from_hparams(source='speechbrain/sepformer-whamr-enhancement', savedir='pretrained_models/sepformer-whamr-enhancement')\", \"api_arguments\": {\"path\": \"Path to the input audio file.\"}, \"python_environment_requirements\": \"pip install speechbrain\", \"example_code\": \"from speechbrain.pretrained import SepformerSeparation as separator\nimport torchaudio\nmodel = separator.from_hparams(source=speechbrain/sepformer-whamr-enhancement, savedir='pretrained_models/sepformer-whamr-enhancement')\nest_sources = model.separate_file(path='speechbrain/sepformer-whamr-enhancement/example_whamr.wav')\ntorchaudio.save(enhanced_whamr.wav, est_sources[:, :, 0].detach().cpu(), 8000)\", \"performance\": {\"dataset\": \"WHAMR!\", \"accuracy\": \"10.59 dB SI-SNR\"}, \"description\": \"This repository provides all the necessary tools to perform speech enhancement (denoising + dereverberation) with a SepFormer model, implemented with SpeechBrain, and pretrained on WHAMR! dataset with 8k sampling frequency, which is basically a version of WSJ0-Mix dataset with environmental noise and reverberation in 8k.\"}}\nWrite a python program in 1 to 2 lines to call API in huggingface.\n\nThe answer should follow the format: {<<<domain>>> $DOMAIN, <<<api_call>>>: $API_CALL, <<<api_provider>>>: $API_PROVIDER, <<<explanation>>>: $EXPLANATION, <<<code>>>: $CODE}. Here are the requirements:\n1. The $API_CALL should have only 1 line of code that calls api.\n2. The $API_PROVIDER should be the programming framework used.\n3. $EXPLANATION should be a step-by-step explanation.\n4. The $CODE is the python code.\n5. Do not repeat the format in your answer.", "category": "generic"}
{"question_id": 79, "text": " Our company is working on a project to automatically translate spoken English audio to spoken Hokkien audio. We need a speech-to-speech translation model.\n \n Use this API documentation for reference:  {\"domain\": \"Audio Audio-to-Audio\", \"framework\": \"Fairseq\", \"functionality\": \"speech-to-speech-translation\", \"api_name\": \"xm_transformer_s2ut_en-hk\", \"api_call\": \"load_model_ensemble_and_task_from_hf_hub('facebook/xm_transformer_s2ut_en-hk')\", \"api_arguments\": {\"arg_overrides\": {\"config_yaml\": \"config.yaml\", \"task\": \"speech_to_text\"}, \"cache_dir\": \"cache_dir\"}, \"python_environment_requirements\": [\"fairseq\", \"huggingface_hub\", \"torchaudio\"], \"example_code\": {\"import_modules\": [\"import json\", \"import os\", \"from pathlib import Path\", \"import IPython.display as ipd\", \"from fairseq import hub_utils\", \"from fairseq.checkpoint_utils import load_model_ensemble_and_task_from_hf_hub\", \"from fairseq.models.speech_to_text.hub_interface import S2THubInterface\", \"from fairseq.models.text_to_speech import CodeHiFiGANVocoder\", \"from fairseq.models.text_to_speech.hub_interface import VocoderHubInterface\", \"from huggingface_hub import snapshot_download\", \"import torchaudio\"], \"load_model\": [\"cache_dir = os.getenv('HUGGINGFACE_HUB_CACHE')\", \"models, cfg, task = load_model_ensemble_and_task_from_hf_hub('facebook/xm_transformer_s2ut_en-hk', arg_overrides={'config_yaml': 'config.yaml', 'task': 'speech_to_text'}, cache_dir=cache_dir)\", \"model = models[0].cpu()\", \"cfg['task'].cpu = True\"], \"generate_prediction\": [\"generator = task.build_generator([model], cfg)\", \"audio, _ = torchaudio.load('/path/to/an/audio/file')\", \"sample = S2THubInterface.get_model_input(task, audio)\", \"unit = S2THubInterface.get_prediction(task, model, generator, sample)\"], \"speech_synthesis\": [\"library_name = 'fairseq'\", \"cache_dir = (cache_dir or (Path.home() / '.cache' / library_name).as_posix())\", \"cache_dir = snapshot_download('facebook/unit_hifigan_HK_layer12.km2500_frame_TAT-TTS', cache_dir=cache_dir, library_name=library_name)\", \"x = hub_utils.from_pretrained(cache_dir, 'model.pt', '.', archive_map=CodeHiFiGANVocoder.hub_models(), config_yaml='config.json', fp16=False, is_vocoder=True)\", \"with open(f'{x['args']['data']}/config.json') as f:\", \"  vocoder_cfg = json.load(f)\", \"assert (len(x['args']['model_path']) == 1), 'Too many vocoder models in the input'\", \"vocoder = CodeHiFiGANVocoder(x['args']['model_path'][0], vocoder_cfg)\", \"tts_model = VocoderHubInterface(vocoder_cfg, vocoder)\", \"tts_sample = tts_model.get_model_input(unit)\", \"wav, sr = tts_model.get_prediction(tts_sample)\", \"ipd.Audio(wav, rate=sr)\"]}, \"performance\": {\"dataset\": \"MuST-C\", \"accuracy\": \"Not specified\"}, \"description\": \"Speech-to-speech translation model with single-pass decoder (S2UT) from fairseq: English-Hokkien. Trained with supervised data in TED domain, and weakly supervised data in TED and Audiobook domain.\"}}\nWrite a python program in 1 to 2 lines to call API in huggingface.\n\nThe answer should follow the format: {<<<domain>>> $DOMAIN, <<<api_call>>>: $API_CALL, <<<api_provider>>>: $API_PROVIDER, <<<explanation>>>: $EXPLANATION, <<<code>>>: $CODE}. Here are the requirements:\n1. The $API_CALL should have only 1 line of code that calls api.\n2. The $API_PROVIDER should be the programming framework used.\n3. $EXPLANATION should be a step-by-step explanation.\n4. The $CODE is the python code.\n5. Do not repeat the format in your answer.", "category": "generic"}
{"question_id": 80, "text": " We are a startup developing voice assistants. We need a keyword spotting system that can recognize user commands.\n \n Use this API documentation for reference:  {\"domain\": \"Audio Audio Classification\", \"framework\": \"Hugging Face Transformers\", \"functionality\": \"Transformers\", \"api_name\": \"superb/hubert-base-superb-ks\", \"api_call\": \"pipeline('audio-classification', model='superb/hubert-base-superb-ks')\", \"api_arguments\": [\"file\", \"top_k\"], \"python_environment_requirements\": [\"datasets\", \"transformers\", \"torchaudio\"], \"example_code\": \"from datasets import load_dataset\nfrom transformers import pipeline\ndataset = load_dataset(anton-l/superb_demo, ks, split=test)\nclassifier = pipeline(audio-classification, model=superb/hubert-base-superb-ks)\nlabels = classifier(dataset[0][file], top_k=5)\", \"performance\": {\"dataset\": \"Speech Commands dataset v1.0\", \"accuracy\": 0.9672}, \"description\": \"This is a ported version of S3PRL's Hubert for the SUPERB Keyword Spotting task. The base model is hubert-base-ls960, which is pretrained on 16kHz sampled speech audio. When using the model make sure that your speech input is also sampled at 16Khz. For more information refer to SUPERB: Speech processing Universal PERformance Benchmark.\"}}\nWrite a python program in 1 to 2 lines to call API in huggingface.\n\nThe answer should follow the format: {<<<domain>>> $DOMAIN, <<<api_call>>>: $API_CALL, <<<api_provider>>>: $API_PROVIDER, <<<explanation>>>: $EXPLANATION, <<<code>>>: $CODE}. Here are the requirements:\n1. The $API_CALL should have only 1 line of code that calls api.\n2. The $API_PROVIDER should be the programming framework used.\n3. $EXPLANATION should be a step-by-step explanation.\n4. The $CODE is the python code.\n5. Do not repeat the format in your answer.", "category": "generic"}
{"question_id": 81, "text": " The model needs to have speech recognition capability to identify languages in a given audio file.\n \n Use this API documentation for reference:  {\"domain\": \"Audio Classification\", \"framework\": \"Hugging Face Transformers\", \"functionality\": \"Language Identification\", \"api_name\": \"sanchit-gandhi/whisper-medium-fleurs-lang-id\", \"api_call\": \"AutoModelForSpeechClassification.from_pretrained('sanchit-gandhi/whisper-medium-fleurs-lang-id')\", \"api_arguments\": [\"model = AutoModelForSpeechClassification.from_pretrained('sanchit-gandhi/whisper-medium-fleurs-lang-id')\", \"processor = Wav2Vec2Processor.from_pretrained('sanchit-gandhi/whisper-medium-fleurs-lang-id')\"], \"python_environment_requirements\": [\"transformers==4.27.0.dev0\", \"pytorch==1.13.1\", \"datasets==2.9.0\", \"tokenizers==0.13.2\"], \"example_code\": \"\", \"performance\": {\"dataset\": \"google/xtreme_s\", \"accuracy\": 0.8805}, \"description\": \"This model is a fine-tuned version of openai/whisper-medium on the FLEURS subset of the google/xtreme_s dataset. It is used for language identification in audio classification tasks.\"}}\nWrite a python program in 1 to 2 lines to call API in huggingface.\n\nThe answer should follow the format: {<<<domain>>> $DOMAIN, <<<api_call>>>: $API_CALL, <<<api_provider>>>: $API_PROVIDER, <<<explanation>>>: $EXPLANATION, <<<code>>>: $CODE}. Here are the requirements:\n1. The $API_CALL should have only 1 line of code that calls api.\n2. The $API_PROVIDER should be the programming framework used.\n3. $EXPLANATION should be a step-by-step explanation.\n4. The $CODE is the python code.\n5. Do not repeat the format in your answer.", "category": "generic"}
{"question_id": 82, "text": " I have just recorded a meeting, I want to find the best segments from the audio where people are speaking, and construct a summary.\n \n Use this API documentation for reference:  {\"domain\": \"Audio Voice Activity Detection\", \"framework\": \"Hugging Face\", \"functionality\": \"Voice Activity Detection\", \"api_name\": \"Eklavya/ZFF_VAD\", \"api_call\": \"pipeline('voice-activity-detection', model='Eklavya/ZFF_VAD')\", \"api_arguments\": \"N/A\", \"python_environment_requirements\": \"transformers, torch\", \"example_code\": \"N/A\", \"performance\": {\"dataset\": \"N/A\", \"accuracy\": \"N/A\"}, \"description\": \"A Voice Activity Detection model by Eklavya, using the Hugging Face framework.\"}}\nWrite a python program in 1 to 2 lines to call API in huggingface.\n\nThe answer should follow the format: {<<<domain>>> $DOMAIN, <<<api_call>>>: $API_CALL, <<<api_provider>>>: $API_PROVIDER, <<<explanation>>>: $EXPLANATION, <<<code>>>: $CODE}. Here are the requirements:\n1. The $API_CALL should have only 1 line of code that calls api.\n2. The $API_PROVIDER should be the programming framework used.\n3. $EXPLANATION should be a step-by-step explanation.\n4. The $CODE is the python code.\n5. Do not repeat the format in your answer.", "category": "generic"}
{"question_id": 83, "text": " I am running a wine store, and I am looking for a machine learning model that can help me classify the quality of wine based on some given features.\n \n Use this API documentation for reference:  {\"domain\": \"Tabular Tabular Classification\", \"framework\": \"Scikit-learn\", \"functionality\": \"Wine Quality classification\", \"api_name\": \"osanseviero/wine-quality\", \"api_call\": \"joblib.load(cached_download(hf_hub_url('julien-c/wine-quality', 'sklearn_model.joblib')))\", \"api_arguments\": \"X\", \"python_environment_requirements\": [\"huggingface_hub\", \"joblib\", \"pandas\"], \"example_code\": \"from huggingface_hub import hf_hub_url, cached_download\nimport joblib\nimport pandas as pd\nREPO_ID = julien-c/wine-quality\nFILENAME = sklearn_model.joblib\nmodel = joblib.load(cached_download(\n hf_hub_url(REPO_ID, FILENAME)\n))\ndata_file = cached_download(\n hf_hub_url(REPO_ID, winequality-red.csv)\n)\nwinedf = pd.read_csv(data_file, sep=;)\nX = winedf.drop([quality], axis=1)\nY = winedf[quality]\nprint(X[:3])\nlabels = model.predict(X[:3])\", \"performance\": {\"dataset\": \"winequality-red.csv\", \"accuracy\": 0.6616635397123202}, \"description\": \"A Simple Example of Scikit-learn Pipeline for Wine Quality classification. Inspired by https://towardsdatascience.com/a-simple-example-of-pipeline-in-machine-learning-with-scikit-learn-e726ffbb6976 by Saptashwa Bhattacharyya.\"}}\nWrite a python program in 1 to 2 lines to call API in huggingface.\n\nThe answer should follow the format: {<<<domain>>> $DOMAIN, <<<api_call>>>: $API_CALL, <<<api_provider>>>: $API_PROVIDER, <<<explanation>>>: $EXPLANATION, <<<code>>>: $CODE}. Here are the requirements:\n1. The $API_CALL should have only 1 line of code that calls api.\n2. The $API_PROVIDER should be the programming framework used.\n3. $EXPLANATION should be a step-by-step explanation.\n4. The $CODE is the python code.\n5. Do not repeat the format in your answer.", "category": "generic"}
{"question_id": 84, "text": " Build a simple application to predict the survival status of passengers on the Titanic based on their age, gender, and passenger class.\n \n Use this API documentation for reference:  {\"domain\": \"Tabular Tabular Classification\", \"framework\": \"Hugging Face\", \"functionality\": \"Binary Classification\", \"api_name\": \"harithapliyal/autotrain-tatanic-survival-51030121311\", \"api_call\": \"AutoModel.from_pretrained('harithapliyal/autotrain-tatanic-survival-51030121311')\", \"api_arguments\": [\"data\"], \"python_environment_requirements\": [\"joblib\", \"pandas\"], \"example_code\": \"import json\nimport joblib\nimport pandas as pd\nmodel = joblib.load('model.joblib')\nconfig = json.load(open('config.json'))\nfeatures = config['features']\ndata = pd.read_csv('data.csv')\ndata = data[features]\ndata.columns = ['feat_' + str(col) for col in data.columns]\npredictions = model.predict(data)\", \"performance\": {\"dataset\": \"harithapliyal/autotrain-data-tatanic-survival\", \"accuracy\": 0.872}, \"description\": \"A tabular classification model trained on the Titanic survival dataset using Hugging Face AutoTrain. The model predicts whether a passenger survived or not based on features such as age, gender, and passenger class.\"}}\nWrite a python program in 1 to 2 lines to call API in huggingface.\n\nThe answer should follow the format: {<<<domain>>> $DOMAIN, <<<api_call>>>: $API_CALL, <<<api_provider>>>: $API_PROVIDER, <<<explanation>>>: $EXPLANATION, <<<code>>>: $CODE}. Here are the requirements:\n1. The $API_CALL should have only 1 line of code that calls api.\n2. The $API_PROVIDER should be the programming framework used.\n3. $EXPLANATION should be a step-by-step explanation.\n4. The $CODE is the python code.\n5. Do not repeat the format in your answer.", "category": "generic"}
{"question_id": 85, "text": " I need to estimate CO2 emissions from vehicles based on their characteristics, such as engine size, transmission type, and miles traveled.\n \n Use this API documentation for reference:  {\"domain\": \"Tabular Tabular Classification\", \"framework\": \"Hugging Face Transformers\", \"functionality\": \"Tabular Classification\", \"api_name\": \"datadmg/autotrain-test-news-44534112235\", \"api_call\": \"AutoModel.from_pretrained('datadmg/autotrain-test-news-44534112235')\", \"api_arguments\": [\"data\"], \"python_environment_requirements\": [\"joblib\", \"pandas\"], \"example_code\": \"import json\nimport joblib\nimport pandas as pd\nmodel = joblib.load('model.joblib')\nconfig = json.load(open('config.json'))\nfeatures = config['features']\ndata = pd.read_csv('data.csv')\ndata = data[features]\ndata.columns = ['feat_' + str(col) for col in data.columns]\npredictions = model.predict(data)\", \"performance\": {\"dataset\": \"datadmg/autotrain-data-test-news\", \"accuracy\": 0.333}, \"description\": \"This model is trained for Multi-class Classification on CO2 Emissions dataset. It uses the Hugging Face Transformers framework and is based on the extra_trees algorithm. The model is trained with AutoTrain and has a tabular classification functionality.\"}}\nWrite a python program in 1 to 2 lines to call API in huggingface.\n\nThe answer should follow the format: {<<<domain>>> $DOMAIN, <<<api_call>>>: $API_CALL, <<<api_provider>>>: $API_PROVIDER, <<<explanation>>>: $EXPLANATION, <<<code>>>: $CODE}. Here are the requirements:\n1. The $API_CALL should have only 1 line of code that calls api.\n2. The $API_PROVIDER should be the programming framework used.\n3. $EXPLANATION should be a step-by-step explanation.\n4. The $CODE is the python code.\n5. Do not repeat the format in your answer.", "category": "generic"}
{"question_id": 86, "text": " We have been asked to predict future criminal re-offense from a given dataset. What model should we adopt and how do we proceed?\n \n Use this API documentation for reference:  {\"domain\": \"Tabular Tabular Classification\", \"framework\": \"Scikit-learn\", \"functionality\": \"Classification\", \"api_name\": \"imodels/figs-compas-recidivism\", \"api_call\": \"joblib.load(cached_download(hf_hub_url('imodels/figs-compas-recidivism', 'sklearn_model.joblib')))\", \"api_arguments\": [\"REPO_ID\", \"FILENAME\"], \"python_environment_requirements\": [\"joblib\", \"huggingface_hub\", \"pandas\", \"numpy\", \"datasets\", \"imodels\", \"sklearn.model_selection\"], \"example_code\": \"from huggingface_hub import hf_hub_url, cached_download\nimport joblib\nimport pandas as pd\nREPO_ID = imodels/figs-compas-recidivism\nFILENAME = sklearn_model.joblib\nmodel = joblib.load(cached_download(\n hf_hub_url(REPO_ID, FILENAME)\n))\npreds = model.predict(X_test)\nprint('accuracy', np.mean(preds==y_test))\", \"performance\": {\"dataset\": \"imodels/compas-recidivism\", \"accuracy\": 0.6759165485112416}, \"description\": \"A tabular classification model for predicting recidivism using the COMPAS dataset. The model is an imodels.FIGSClassifier trained with Scikit-learn and can be used with the Hugging Face Inference API.\"}}\nWrite a python program in 1 to 2 lines to call API in huggingface.\n\nThe answer should follow the format: {<<<domain>>> $DOMAIN, <<<api_call>>>: $API_CALL, <<<api_provider>>>: $API_PROVIDER, <<<explanation>>>: $EXPLANATION, <<<code>>>: $CODE}. Here are the requirements:\n1. The $API_CALL should have only 1 line of code that calls api.\n2. The $API_PROVIDER should be the programming framework used.\n3. $EXPLANATION should be a step-by-step explanation.\n4. The $CODE is the python code.\n5. Do not repeat the format in your answer.", "category": "generic"}
{"question_id": 87, "text": " Our company's goal is to predict carbon emissions based on the given features of the compound.\n \n Use this API documentation for reference:  {\"domain\": \"Tabular Tabular Regression\", \"framework\": \"Hugging Face\", \"functionality\": \"Carbon Emissions\", \"api_name\": \"kochetkovIT/autotrain-ironhack-49741119788\", \"api_call\": \"joblib.load('model.joblib')\", \"api_arguments\": [\"data\"], \"python_environment_requirements\": [\"json\", \"joblib\", \"pandas\"], \"example_code\": \"import json\nimport joblib\nimport pandas as pd\nmodel = joblib.load('model.joblib')\nconfig = json.load(open('config.json'))\nfeatures = config['features']\ndata = pd.read_csv('data.csv')\ndata = data[features]\ndata.columns = ['feat_' + str(col) for col in data.columns]\npredictions = model.predict(data)\", \"performance\": {\"dataset\": \"kochetkovIT/autotrain-data-ironhack\", \"accuracy\": {\"Loss\": 2.603, \"R2\": 0.013, \"MSE\": 6.776, \"MAE\": 1.666, \"RMSLE\": 0.502}}, \"description\": \"A tabular regression model trained using AutoTrain to predict carbon emissions (in grams) with an R2 score of 0.013.\"}}\nWrite a python program in 1 to 2 lines to call API in huggingface.\n\nThe answer should follow the format: {<<<domain>>> $DOMAIN, <<<api_call>>>: $API_CALL, <<<api_provider>>>: $API_PROVIDER, <<<explanation>>>: $EXPLANATION, <<<code>>>: $CODE}. Here are the requirements:\n1. The $API_CALL should have only 1 line of code that calls api.\n2. The $API_PROVIDER should be the programming framework used.\n3. $EXPLANATION should be a step-by-step explanation.\n4. The $CODE is the python code.\n5. Do not repeat the format in your answer.", "category": "generic"}
{"question_id": 88, "text": " The factory wants to make its production process more eco-friendly. Calculate the carbon emissions for given data.\n \n Use this API documentation for reference:  {\"domain\": \"Tabular Tabular Regression\", \"framework\": \"Joblib\", \"functionality\": \"Carbon Emissions\", \"api_name\": \"al02783013/autotrain-faseiii_diciembre-2311773112\", \"api_call\": \"joblib.load('model.joblib')\", \"api_arguments\": [\"data\"], \"python_environment_requirements\": [\"joblib\", \"pandas\", \"json\"], \"example_code\": \"import json\nimport joblib\nimport pandas as pd\nmodel = joblib.load('model.joblib')\nconfig = json.load(open('config.json'))\nfeatures = config['features']\ndata = pd.read_csv('data.csv')\ndata = data[features]\ndata.columns = ['feat_' + str(col) for col in data.columns]\npredictions = model.predict(data)\", \"performance\": {\"dataset\": \"al02783013/autotrain-data-faseiii_diciembre\", \"accuracy\": {\"Loss\": 5487.957, \"R2\": 0.96, \"MSE\": 30117668.0, \"MAE\": 2082.499, \"RMSLE\": 1.918}}, \"description\": \"A tabular regression model trained using AutoTrain to predict carbon emissions based on input features.\"}}\nWrite a python program in 1 to 2 lines to call API in huggingface.\n\nThe answer should follow the format: {<<<domain>>> $DOMAIN, <<<api_call>>>: $API_CALL, <<<api_provider>>>: $API_PROVIDER, <<<explanation>>>: $EXPLANATION, <<<code>>>: $CODE}. Here are the requirements:\n1. The $API_CALL should have only 1 line of code that calls api.\n2. The $API_PROVIDER should be the programming framework used.\n3. $EXPLANATION should be a step-by-step explanation.\n4. The $CODE is the python code.\n5. Do not repeat the format in your answer.", "category": "generic"}
{"question_id": 89, "text": " We want to predict the carbon emissions of a new line of electric vehicles for an annual report. Automate the process of loading a regression model, then calculate the forecast of emissions for this year.\n \n Use this API documentation for reference:  {\"domain\": \"Tabular Tabular Regression\", \"framework\": \"Joblib\", \"functionality\": \"Carbon Emissions\", \"api_name\": \"Robertooo/autotrain-hmaet-2037366891\", \"api_call\": \"joblib.load('model.joblib')\", \"api_arguments\": [\"data\"], \"python_environment_requirements\": [\"json\", \"joblib\", \"pandas\"], \"example_code\": \"import json\nimport joblib\nimport pandas as pd\nmodel = joblib.load('model.joblib')\nconfig = json.load(open('config.json'))\nfeatures = config['features']\ndata = pd.read_csv('data.csv')\ndata = data[features]\ndata.columns = ['feat_' + str(col) for col in data.columns]\npredictions = model.predict(data)\", \"performance\": {\"dataset\": \"Robertooo/autotrain-data-hmaet\", \"accuracy\": {\"Loss\": 0.067, \"R2\": 0.486, \"MSE\": 0.005, \"MAE\": 0.055, \"RMSLE\": 0.036}}, \"description\": \"A tabular regression model trained with AutoTrain to predict carbon emissions.\"}}\nWrite a python program in 1 to 2 lines to call API in huggingface.\n\nThe answer should follow the format: {<<<domain>>> $DOMAIN, <<<api_call>>>: $API_CALL, <<<api_provider>>>: $API_PROVIDER, <<<explanation>>>: $EXPLANATION, <<<code>>>: $CODE}. Here are the requirements:\n1. The $API_CALL should have only 1 line of code that calls api.\n2. The $API_PROVIDER should be the programming framework used.\n3. $EXPLANATION should be a step-by-step explanation.\n4. The $CODE is the python code.\n5. Do not repeat the format in your answer.", "category": "generic"}
{"question_id": 90, "text": " We are planning to launch a website which provides tips to people for their daily lives. Can you please build a model to predict the appropriate amount of tips?\n \n Use this API documentation for reference:  {\"domain\": \"Tabular Tabular Regression\", \"framework\": \"Scikit-learn\", \"functionality\": \"baseline-trainer\", \"api_name\": \"merve/tips9y0jvt5q-tip-regression\", \"api_call\": \"pipeline('tabular-regression', model='merve/tips9y0jvt5q-tip-regression')\", \"api_arguments\": \"N/A\", \"python_environment_requirements\": \"dabl\", \"example_code\": \"N/A\", \"performance\": {\"dataset\": \"tips9y0jvt5q\", \"accuracy\": {\"r2\": 0.41524, \"neg_mean_squared_error\": -1.098792}}, \"description\": \"Baseline Model trained on tips9y0jvt5q to apply regression on tip. The model uses Ridge(alpha=10) and is trained with dabl library as a baseline. For better results, use AutoTrain.\"}}\nWrite a python program in 1 to 2 lines to call API in huggingface.\n\nThe answer should follow the format: {<<<domain>>> $DOMAIN, <<<api_call>>>: $API_CALL, <<<api_provider>>>: $API_PROVIDER, <<<explanation>>>: $EXPLANATION, <<<code>>>: $CODE}. Here are the requirements:\n1. The $API_CALL should have only 1 line of code that calls api.\n2. The $API_PROVIDER should be the programming framework used.\n3. $EXPLANATION should be a step-by-step explanation.\n4. The $CODE is the python code.\n5. Do not repeat the format in your answer.", "category": "generic"}
{"question_id": 91, "text": " We have a robotic arm in our warehouse that needs to be trained to optimize loading and unloading tasks. The robotic arm is based on the CartPole environment.\n \n Use this API documentation for reference:  {\"domain\": \"Reinforcement Learning\", \"framework\": \"Stable-Baselines3\", \"functionality\": \"deep-reinforcement-learning\", \"api_name\": \"ppo-seals-CartPole-v0\", \"api_call\": \"load_from_hub(repo_id='HumanCompatibleAI/ppo-seals-CartPole-v0',filename='{MODEL FILENAME}.zip',)\", \"api_arguments\": [\"algo\", \"env\", \"f\"], \"python_environment_requirements\": [\"rl_zoo3\", \"stable-baselines3\", \"stable-baselines3-contrib\"], \"example_code\": \"python -m rl_zoo3.load_from_hub --algo ppo --env seals/CartPole-v0 -orga HumanCompatibleAI -f logs/\", \"performance\": {\"dataset\": \"seals/CartPole-v0\", \"accuracy\": \"500.00 +/- 0.00\"}, \"description\": \"This is a trained model of a PPO agent playing seals/CartPole-v0 using the stable-baselines3 library and the RL Zoo. The RL Zoo is a training framework for Stable Baselines3 reinforcement learning agents, with hyperparameter optimization and pre-trained agents included.\"}}\nWrite a python program in 1 to 2 lines to call API in huggingface.\n\nThe answer should follow the format: {<<<domain>>> $DOMAIN, <<<api_call>>>: $API_CALL, <<<api_provider>>>: $API_PROVIDER, <<<explanation>>>: $EXPLANATION, <<<code>>>: $CODE}. Here are the requirements:\n1. The $API_CALL should have only 1 line of code that calls api.\n2. The $API_PROVIDER should be the programming framework used.\n3. $EXPLANATION should be a step-by-step explanation.\n4. The $CODE is the python code.\n5. Do not repeat the format in your answer.", "category": "generic"}
{"question_id": 92, "text": " There is an upcoming event called \\\"Space Party\\\" and we need a representative image for the event. Can you assist us in creating an image containing a party in space with astronauts and aliens having fun together?\n \n Use this API documentation for reference:  {\"domain\": \"Multimodal Text-to-Image\", \"framework\": \"Hugging Face\", \"functionality\": \"Text-to-Image Generation\", \"api_name\": \"stabilityai/stable-diffusion-2-1\", \"api_call\": \"StableDiffusionPipeline.from_pretrained('stabilityai/stable-diffusion-2-1', torch_dtype=torch.float16)\", \"api_arguments\": {\"prompt\": \"a photo of an astronaut riding a horse on mars\"}, \"python_environment_requirements\": [\"diffusers\", \"transformers\", \"accelerate\", \"scipy\", \"safetensors\"], \"example_code\": \"from diffusers import StableDiffusionPipeline, DPMSolverMultistepScheduler\nmodel_id = stabilityai/stable-diffusion-2-1\npipe = StableDiffusionPipeline.from_pretrained(model_id, torch_dtype=torch.float16)\npipe.scheduler = DPMSolverMultistepScheduler.from_config(pipe.scheduler.config)\npipe = pipe.to(cuda)\nprompt = a photo of an astronaut riding a horse on mars\nimage = pipe(prompt).images[0]\nimage.save(astronaut_rides_horse.png)\", \"performance\": {\"dataset\": \"COCO2017\", \"accuracy\": \"Not optimized for FID scores\"}, \"description\": \"Stable Diffusion v2-1 is a diffusion-based text-to-image generation model developed by Robin Rombach and Patrick Esser. It is capable of generating and modifying images based on text prompts in English. The model is trained on a subset of the LAION-5B dataset and is primarily intended for research purposes.\"}}\nWrite a python program in 1 to 2 lines to call API in huggingface.\n\nThe answer should follow the format: {<<<domain>>> $DOMAIN, <<<api_call>>>: $API_CALL, <<<api_provider>>>: $API_PROVIDER, <<<explanation>>>: $EXPLANATION, <<<code>>>: $CODE}. Here are the requirements:\n1. The $API_CALL should have only 1 line of code that calls api.\n2. The $API_PROVIDER should be the programming framework used.\n3. $EXPLANATION should be a step-by-step explanation.\n4. The $CODE is the python code.\n5. Do not repeat the format in your answer.", "category": "generic"}
{"question_id": 93, "text": " We're creating a promotional image for a wildlife-themed event. We need to display two tigers in a natural setting.\n \n Use this API documentation for reference:  {\"domain\": \"Multimodal Text-to-Image\", \"framework\": \"Hugging Face\", \"functionality\": \"Generate and modify images based on text prompts\", \"api_name\": \"stabilityai/stable-diffusion-2-depth\", \"api_call\": \"StableDiffusionDepth2ImgPipeline.from_pretrained('stabilityai/stable-diffusion-2-depth', torch_dtype=torch.float16)\", \"api_arguments\": {\"prompt\": \"Text prompt to generate image\", \"image\": \"Initial image (optional)\", \"negative_prompt\": \"Negative text prompt to avoid certain features\", \"strength\": \"Strength of the prompt effect on the generated image\"}, \"python_environment_requirements\": [\"pip install -U git+https://github.com/huggingface/transformers.git\", \"pip install diffusers transformers accelerate scipy safetensors\"], \"example_code\": \"import torch\nimport requests\nfrom PIL import Image\nfrom diffusers import StableDiffusionDepth2ImgPipeline\npipe = StableDiffusionDepth2ImgPipeline.from_pretrained(\n stabilityai/stable-diffusion-2-depth,\n torch_dtype=torch.float16,\n).to(cuda)\nurl = http://images.cocodataset.org/val2017/000000039769.jpg\ninit_image = Image.open(requests.get(url, stream=True).raw)\nprompt = two tigers\nn_propmt = bad, deformed, ugly, bad anotomy\nimage = pipe(prompt=prompt, image=init_image, negative_prompt=n_propmt, strength=0.7).images[0]\", \"performance\": {\"dataset\": \"COCO2017 validation set\", \"accuracy\": \"Not optimized for FID scores\"}, \"description\": \"Stable Diffusion v2 is a latent diffusion model that generates and modifies images based on text prompts. It uses a fixed, pretrained text encoder (OpenCLIP-ViT/H) and is developed by Robin Rombach and Patrick Esser. The model works with English language prompts and is intended for research purposes only.\"}}\nWrite a python program in 1 to 2 lines to call API in huggingface.\n\nThe answer should follow the format: {<<<domain>>> $DOMAIN, <<<api_call>>>: $API_CALL, <<<api_provider>>>: $API_PROVIDER, <<<explanation>>>: $EXPLANATION, <<<code>>>: $CODE}. Here are the requirements:\n1. The $API_CALL should have only 1 line of code that calls api.\n2. The $API_PROVIDER should be the programming framework used.\n3. $EXPLANATION should be a step-by-step explanation.\n4. The $CODE is the python code.\n5. Do not repeat the format in your answer.", "category": "generic"}
{"question_id": 94, "text": " We have a collection of low-resolution images of movie characters, and we need to upscale those images to get a more detailed high-resolution image.\n \n Use this API documentation for reference:  {\"domain\": \"Multimodal Text-to-Image\", \"framework\": \"Hugging Face\", \"functionality\": \"Image Upscaling\", \"api_name\": \"stabilityai/sd-x2-latent-upscaler\", \"api_call\": \"StableDiffusionLatentUpscalePipeline.from_pretrained(stabilityai/sd-x2-latent-upscaler, torch_dtype=torch.float16)\", \"api_arguments\": {\"prompt\": \"text prompt\", \"image\": \"low resolution latents\", \"num_inference_steps\": 20, \"guidance_scale\": 0, \"generator\": \"torch generator\"}, \"python_environment_requirements\": [\"git+https://github.com/huggingface/diffusers.git\", \"transformers\", \"accelerate\", \"scipy\", \"safetensors\"], \"example_code\": \"from diffusers import StableDiffusionLatentUpscalePipeline, StableDiffusionPipeline\nimport torch\npipeline = StableDiffusionPipeline.from_pretrained(CompVis/stable-diffusion-v1-4, torch_dtype=torch.float16)\npipeline.to(cuda)\nupscaler = StableDiffusionLatentUpscalePipeline.from_pretrained(stabilityai/sd-x2-latent-upscaler, torch_dtype=torch.float16)\nupscaler.to(cuda)\nprompt = a photo of an astronaut high resolution, unreal engine, ultra realistic\ngenerator = torch.manual_seed(33)\nlow_res_latents = pipeline(prompt, generator=generator, output_type=latent).images\nupscaled_image = upscaler(prompt=prompt, image=low_res_latents, num_inference_steps=20, guidance_scale=0, generator=generator).images[0]\nupscaled_image.save(astronaut_1024.png)\", \"performance\": {\"dataset\": \"LAION-2B\", \"accuracy\": \"Not specified\"}, \"description\": \"Stable Diffusion x2 latent upscaler is a diffusion-based upscaler model developed by Katherine Crowson in collaboration with Stability AI. It is designed to upscale Stable Diffusion's latent denoised image embeddings, allowing for fast text-to-image and upscaling pipelines. The model was trained on a high-resolution subset of the LAION-2B dataset and works with all Stable Diffusion checkpoints.\"}}\nWrite a python program in 1 to 2 lines to call API in huggingface.\n\nThe answer should follow the format: {<<<domain>>> $DOMAIN, <<<api_call>>>: $API_CALL, <<<api_provider>>>: $API_PROVIDER, <<<explanation>>>: $EXPLANATION, <<<code>>>: $CODE}. Here are the requirements:\n1. The $API_CALL should have only 1 line of code that calls api.\n2. The $API_PROVIDER should be the programming framework used.\n3. $EXPLANATION should be a step-by-step explanation.\n4. The $CODE is the python code.\n5. Do not repeat the format in your answer.", "category": "generic"}
{"question_id": 95, "text": " I want you to create a function that generates captions for a list of images.\n \n Use this API documentation for reference:  {\"domain\": \"Multimodal Image-to-Text\", \"framework\": \"Hugging Face Transformers\", \"functionality\": \"Image Captioning\", \"api_name\": \"nlpconnect/vit-gpt2-image-captioning\", \"api_call\": \"VisionEncoderDecoderModel.from_pretrained('nlpconnect/vit-gpt2-image-captioning')\", \"api_arguments\": {\"model\": \"nlpconnect/vit-gpt2-image-captioning\"}, \"python_environment_requirements\": [\"transformers\", \"torch\", \"PIL\"], \"example_code\": \"from transformers import VisionEncoderDecoderModel, ViTImageProcessor, AutoTokenizer\nimport torch\nfrom PIL import Image\nmodel = VisionEncoderDecoderModel.from_pretrained(nlpconnect/vit-gpt2-image-captioning)\nfeature_extractor = ViTImageProcessor.from_pretrained(nlpconnect/vit-gpt2-image-captioning)\ntokenizer = AutoTokenizer.from_pretrained(nlpconnect/vit-gpt2-image-captioning)\ndevice = torch.device(cuda if torch.cuda.is_available() else cpu)\nmodel.to(device)\nmax_length = 16\nnum_beams = 4\ngen_kwargs = {max_length: max_length, num_beams: num_beams}\ndef predict_step(image_paths):\n images = []\n for image_path in image_paths:\n i_image = Image.open(image_path)\n if i_image.mode != RGB:\n i_image = i_image.convert(mode=RGB)\nimages.append(i_image)\npixel_values = feature_extractor(images=images, return_tensors=pt).pixel_values\n pixel_values = pixel_values.to(device)\noutput_ids = model.generate(pixel_values, **gen_kwargs)\npreds = tokenizer.batch_decode(output_ids, skip_special_tokens=True)\n preds = [pred.strip() for pred in preds]\n return preds\npredict_step(['doctor.e16ba4e4.jpg']) # ['a woman in a hospital bed with a woman in a hospital bed']\", \"performance\": {\"dataset\": \"Not provided\", \"accuracy\": \"Not provided\"}, \"description\": \"An image captioning model that uses transformers to generate captions for input images. The model is based on the Illustrated Image Captioning using transformers approach.\"}}\nWrite a python program in 1 to 2 lines to call API in huggingface.\n\nThe answer should follow the format: {<<<domain>>> $DOMAIN, <<<api_call>>>: $API_CALL, <<<api_provider>>>: $API_PROVIDER, <<<explanation>>>: $EXPLANATION, <<<code>>>: $CODE}. Here are the requirements:\n1. The $API_CALL should have only 1 line of code that calls api.\n2. The $API_PROVIDER should be the programming framework used.\n3. $EXPLANATION should be a step-by-step explanation.\n4. The $CODE is the python code.\n5. Do not repeat the format in your answer.", "category": "generic"}
{"question_id": 96, "text": " We need a tool to help us generate textual descriptions for images and videos related to our product.\n \n Use this API documentation for reference:  {\"domain\": \"Multimodal Image-to-Text\", \"framework\": \"Hugging Face Transformers\", \"functionality\": \"Transformers\", \"api_name\": \"git-large-textcaps\", \"api_call\": \"AutoModelForCausalLM.from_pretrained('microsoft/git-large-textcaps')\", \"api_arguments\": \"image, text\", \"python_environment_requirements\": \"transformers\", \"example_code\": \"N/A\", \"performance\": {\"dataset\": \"TextCaps\", \"accuracy\": \"Refer to the paper\"}, \"description\": \"GIT (short for GenerativeImage2Text) model, large-sized version, fine-tuned on TextCaps. It was introduced in the paper GIT: A Generative Image-to-text Transformer for Vision and Language by Wang et al. and first released in this repository. The model is trained using 'teacher forcing' on a lot of (image, text) pairs. The goal for the model is simply to predict the next text token, giving the image tokens and previous text tokens. This allows the model to be used for tasks like image and video captioning, visual question answering (VQA) on images and videos, and even image classification (by simply conditioning the model on the image and asking it to generate a class for it in text).\"}}\nWrite a python program in 1 to 2 lines to call API in huggingface.\n\nThe answer should follow the format: {<<<domain>>> $DOMAIN, <<<api_call>>>: $API_CALL, <<<api_provider>>>: $API_PROVIDER, <<<explanation>>>: $EXPLANATION, <<<code>>>: $CODE}. Here are the requirements:\n1. The $API_CALL should have only 1 line of code that calls api.\n2. The $API_PROVIDER should be the programming framework used.\n3. $EXPLANATION should be a step-by-step explanation.\n4. The $CODE is the python code.\n5. Do not repeat the format in your answer.", "category": "generic"}
{"question_id": 97, "text": " We need to build an AI-powered tool to assist visually impaired users in understanding their surroundings by answering questions about images.\n \n Use this API documentation for reference:  {\"domain\": \"Multimodal Visual Question Answering\", \"framework\": \"Hugging Face Transformers\", \"functionality\": \"Transformers\", \"api_name\": \"git-large-textvqa\", \"api_call\": \"AutoModelForSeq2SeqLM.from_pretrained('microsoft/git-large-textvqa')\", \"api_arguments\": \"image, question\", \"python_environment_requirements\": \"transformers\", \"example_code\": \"For code examples, we refer to the documentation.\", \"performance\": {\"dataset\": \"TextVQA\", \"accuracy\": \"See table 11 in the paper for more details.\"}, \"description\": \"GIT (short for GenerativeImage2Text) model, large-sized version, fine-tuned on TextVQA. It was introduced in the paper GIT: A Generative Image-to-text Transformer for Vision and Language by Wang et al. and first released in this repository. The model is trained using 'teacher forcing' on a lot of (image, text) pairs. The goal for the model is simply to predict the next text token, giving the image tokens and previous text tokens. This allows the model to be used for tasks like: image and video captioning, visual question answering (VQA) on images and videos, and even image classification (by simply conditioning the model on the image and asking it to generate a class for it in text).\"}}\nWrite a python program in 1 to 2 lines to call API in huggingface.\n\nThe answer should follow the format: {<<<domain>>> $DOMAIN, <<<api_call>>>: $API_CALL, <<<api_provider>>>: $API_PROVIDER, <<<explanation>>>: $EXPLANATION, <<<code>>>: $CODE}. Here are the requirements:\n1. The $API_CALL should have only 1 line of code that calls api.\n2. The $API_PROVIDER should be the programming framework used.\n3. $EXPLANATION should be a step-by-step explanation.\n4. The $CODE is the python code.\n5. Do not repeat the format in your answer.", "category": "generic"}
{"question_id": 98, "text": " I'm a nutritionist and want to help my clients by answering questions about their meals. They will send me an image of their food and ask me a question about it, like \\\"Is this vegan?\\\" or \\\"How many calories do you think it contains?\\\"\n \n Use this API documentation for reference:  {\"domain\": \"Multimodal Visual Question Answering\", \"framework\": \"Hugging Face Transformers\", \"functionality\": \"Visual Question Answering\", \"api_name\": \"temp_vilt_vqa\", \"api_call\": \"pipeline('visual-question-answering', model='Bingsu/temp_vilt_vqa', tokenizer='Bingsu/temp_vilt_vqa')\", \"api_arguments\": {\"model\": \"Bingsu/temp_vilt_vqa\", \"tokenizer\": \"Bingsu/temp_vilt_vqa\"}, \"python_environment_requirements\": \"transformers\", \"example_code\": \"\", \"performance\": {\"dataset\": \"\", \"accuracy\": \"\"}, \"description\": \"A visual question answering model for answering questions related to images using the Hugging Face Transformers library.\"}}\nWrite a python program in 1 to 2 lines to call API in huggingface.\n\nThe answer should follow the format: {<<<domain>>> $DOMAIN, <<<api_call>>>: $API_CALL, <<<api_provider>>>: $API_PROVIDER, <<<explanation>>>: $EXPLANATION, <<<code>>>: $CODE}. Here are the requirements:\n1. The $API_CALL should have only 1 line of code that calls api.\n2. The $API_PROVIDER should be the programming framework used.\n3. $EXPLANATION should be a step-by-step explanation.\n4. The $CODE is the python code.\n5. Do not repeat the format in your answer.", "category": "generic"}
{"question_id": 99, "text": " Our client is a legal firm that needs assistance in extracting specific information from a large number of legal documents. Automate the process of answering questions related to these documents.\n \n Use this API documentation for reference:  {\"domain\": \"Multimodal Document Question Answer\", \"framework\": \"Hugging Face Transformers\", \"functionality\": \"Document Question Answering\", \"api_name\": \"layoutlmv2-base-uncased_finetuned_docvqa\", \"api_call\": \"pipeline('question-answering', model='Sayantan1993/layoutlmv2-base-uncased_finetuned_docvqa')\", \"api_arguments\": \"{'question': 'your_question', 'context': 'your_context'}\", \"python_environment_requirements\": \"transformers\", \"example_code\": \"\", \"performance\": {\"dataset\": \"\", \"accuracy\": \"\"}, \"description\": \"A model for document question answering, fine-tuned on the DocVQA dataset using LayoutLMv2-base-uncased.\"}}\nWrite a python program in 1 to 2 lines to call API in huggingface.\n\nThe answer should follow the format: {<<<domain>>> $DOMAIN, <<<api_call>>>: $API_CALL, <<<api_provider>>>: $API_PROVIDER, <<<explanation>>>: $EXPLANATION, <<<code>>>: $CODE}. Here are the requirements:\n1. The $API_CALL should have only 1 line of code that calls api.\n2. The $API_PROVIDER should be the programming framework used.\n3. $EXPLANATION should be a step-by-step explanation.\n4. The $CODE is the python code.\n5. Do not repeat the format in your answer.", "category": "generic"}
{"question_id": 100, "text": " In a healthcare company, we are trying to create an automated system for answering patient-related questions based on their medical documents. We need a solution using NLP.\n \n Use this API documentation for reference:  {\"domain\": \"Multimodal Document Question Answer\", \"framework\": \"Hugging Face Transformers\", \"functionality\": \"Document Question Answering\", \"api_name\": \"layoutlmv2-base-uncased-finetuned-infovqa\", \"api_call\": \"AutoModelForDocumentQuestionAnswering.from_pretrained('tiennvcs/layoutlmv2-base-uncased-finetuned-infovqa')\", \"api_arguments\": {}, \"python_environment_requirements\": {\"transformers\": \"4.12.2\", \"pytorch\": \"1.8.0+cu101\", \"datasets\": \"1.14.0\", \"tokenizers\": \"0.10.3\"}, \"example_code\": \"\", \"performance\": {\"dataset\": \"unknown\", \"accuracy\": {\"Loss\": 2.087}}, \"description\": \"This model is a fine-tuned version of microsoft/layoutlmv2-base-uncased on an unknown dataset.\"}}\nWrite a python program in 1 to 2 lines to call API in huggingface.\n\nThe answer should follow the format: {<<<domain>>> $DOMAIN, <<<api_call>>>: $API_CALL, <<<api_provider>>>: $API_PROVIDER, <<<explanation>>>: $EXPLANATION, <<<code>>>: $CODE}. Here are the requirements:\n1. The $API_CALL should have only 1 line of code that calls api.\n2. The $API_PROVIDER should be the programming framework used.\n3. $EXPLANATION should be a step-by-step explanation.\n4. The $CODE is the python code.\n5. Do not repeat the format in your answer.", "category": "generic"}
{"question_id": 101, "text": " Develop a program which can answer questions related to a scanned document.\n \n Use this API documentation for reference:  {\"domain\": \"Multimodal Document Question Answer\", \"framework\": \"Hugging Face Transformers\", \"functionality\": \"Transformers\", \"api_name\": \"LayoutLMX_pt_question_answer_ocrazure_correct_V15_30_03_2023\", \"api_call\": \"AutoModelForDocumentQuestionAnswering.from_pretrained('L-oenai/LayoutLMX_pt_question_answer_ocrazure_correct_V15_30_03_2023')\", \"api_arguments\": {}, \"python_environment_requirements\": {\"transformers\": \">=4.11.0\"}, \"example_code\": {}, \"performance\": {\"dataset\": {}, \"accuracy\": {}}, \"description\": \"A document question answering model based on LayoutLMv2, which can be used to extract answers from images with text and layout information.\"}}\nWrite a python program in 1 to 2 lines to call API in huggingface.\n\nThe answer should follow the format: {<<<domain>>> $DOMAIN, <<<api_call>>>: $API_CALL, <<<api_provider>>>: $API_PROVIDER, <<<explanation>>>: $EXPLANATION, <<<code>>>: $CODE}. Here are the requirements:\n1. The $API_CALL should have only 1 line of code that calls api.\n2. The $API_PROVIDER should be the programming framework used.\n3. $EXPLANATION should be a step-by-step explanation.\n4. The $CODE is the python code.\n5. Do not repeat the format in your answer.", "category": "generic"}
{"question_id": 102, "text": " I have received a PDF document and a question. My task is to find the answer part in the document.\n \n Use this API documentation for reference:  {\"domain\": \"Multimodal Document Question Answer\", \"framework\": \"Hugging Face\", \"functionality\": \"Question Answering\", \"api_name\": \"impira/layoutlm-document-qa\", \"api_call\": \"pipeline('question-answering', model=LayoutLMForQuestionAnswering.from_pretrained('impira/layoutlm-document-qa', return_dict=True))\", \"api_arguments\": [\"image_url\", \"question\"], \"python_environment_requirements\": [\"PIL\", \"pytesseract\", \"PyTorch\", \"transformers\"], \"example_code\": \"nlp(https://templates.invoicehome.com/invoice-template-us-neat-750px.png, What is the invoice number?)\", \"performance\": {\"dataset\": \"SQuAD2.0 and DocVQA\", \"accuracy\": \"Not provided\"}, \"description\": \"A fine-tuned version of the multi-modal LayoutLM model for the task of question answering on documents.\"}}\nWrite a python program in 1 to 2 lines to call API in huggingface.\n\nThe answer should follow the format: {<<<domain>>> $DOMAIN, <<<api_call>>>: $API_CALL, <<<api_provider>>>: $API_PROVIDER, <<<explanation>>>: $EXPLANATION, <<<code>>>: $CODE}. Here are the requirements:\n1. The $API_CALL should have only 1 line of code that calls api.\n2. The $API_PROVIDER should be the programming framework used.\n3. $EXPLANATION should be a step-by-step explanation.\n4. The $CODE is the python code.\n5. Do not repeat the format in your answer.", "category": "generic"}
{"question_id": 103, "text": " An interior design firm builds a software to understand the depth of rooms captured in photographs for remodeling activities.\n \n Use this API documentation for reference:  {\"domain\": \"Computer Vision Depth Estimation\", \"framework\": \"Hugging Face Transformers\", \"functionality\": \"Transformers\", \"api_name\": \"glpn-nyu-finetuned-diode-221215-093747\", \"api_call\": \"AutoModel.from_pretrained('sayakpaul/glpn-nyu-finetuned-diode-221215-093747')\", \"api_arguments\": [], \"python_environment_requirements\": [\"transformers\", \"torch\"], \"example_code\": \"\", \"performance\": {\"dataset\": \"DIODE\", \"accuracy\": \"\"}, \"description\": \"A depth estimation model fine-tuned on the DIODE dataset.\"}}\nWrite a python program in 1 to 2 lines to call API in huggingface.\n\nThe answer should follow the format: {<<<domain>>> $DOMAIN, <<<api_call>>>: $API_CALL, <<<api_provider>>>: $API_PROVIDER, <<<explanation>>>: $EXPLANATION, <<<code>>>: $CODE}. Here are the requirements:\n1. The $API_CALL should have only 1 line of code that calls api.\n2. The $API_PROVIDER should be the programming framework used.\n3. $EXPLANATION should be a step-by-step explanation.\n4. The $CODE is the python code.\n5. Do not repeat the format in your answer.", "category": "generic"}
{"question_id": 104, "text": " We are running an autonomous vehicle company and want to implement a depth estimation module for the real-time video feed captured by our camera.\n \n Use this API documentation for reference:  {\"domain\": \"Computer Vision Depth Estimation\", \"framework\": \"Hugging Face Transformers\", \"functionality\": \"Depth Estimation\", \"api_name\": \"glpn-nyu-finetuned-diode-221116-104421\", \"api_call\": \"AutoModel.from_pretrained('sayakpaul/glpn-nyu-finetuned-diode-221116-104421')\", \"api_arguments\": \"\", \"python_environment_requirements\": \"transformers==4.24.0, pytorch==1.12.1+cu113, tokenizers==0.13.2\", \"example_code\": \"\", \"performance\": {\"dataset\": \"diode-subset\", \"accuracy\": {\"Loss\": 0.3736, \"Mae\": 0.3079, \"Rmse\": 0.4321, \"Abs Rel\": 0.3666, \"Log Mae\": 0.1288, \"Log Rmse\": 0.1794, \"Delta1\": 0.4929, \"Delta2\": 0.7934, \"Delta3\": 0.9234}}, \"description\": \"This model is a fine-tuned version of vinvino02/glpn-nyu on the diode-subset dataset.\"}}\nWrite a python program in 1 to 2 lines to call API in huggingface.\n\nThe answer should follow the format: {<<<domain>>> $DOMAIN, <<<api_call>>>: $API_CALL, <<<api_provider>>>: $API_PROVIDER, <<<explanation>>>: $EXPLANATION, <<<code>>>: $CODE}. Here are the requirements:\n1. The $API_CALL should have only 1 line of code that calls api.\n2. The $API_PROVIDER should be the programming framework used.\n3. $EXPLANATION should be a step-by-step explanation.\n4. The $CODE is the python code.\n5. Do not repeat the format in your answer.", "category": "generic"}
{"question_id": 105, "text": " Our team wants to create a new app for autonomous vehicles. For that, we need to estimate the depth of the field from images.\n \n Use this API documentation for reference:  {\"domain\": \"Computer Vision Depth Estimation\", \"framework\": \"Hugging Face Transformers\", \"functionality\": \"Transformers\", \"api_name\": \"glpn-nyu-finetuned-diode-221121-063504\", \"api_call\": \"AutoModelForImageClassification.from_pretrained('sayakpaul/glpn-nyu-finetuned-diode-221121-063504')\", \"api_arguments\": [], \"python_environment_requirements\": [\"transformers==4.24.0\", \"torch==1.12.1+cu116\", \"tokenizers==0.13.2\"], \"example_code\": \"\", \"performance\": {\"dataset\": \"diode-subset\", \"accuracy\": {\"Loss\": 0.3533, \"Mae\": 0.2668, \"Rmse\": 0.3716, \"Abs Rel\": 0.3427, \"Log Mae\": 0.1167, \"Log Rmse\": 0.1703, \"Delta1\": 0.5522, \"Delta2\": 0.8362, \"Delta3\": 0.9382}}, \"description\": \"This model is a fine-tuned version of vinvino02/glpn-nyu on the diode-subset dataset for depth estimation.\"}}\nWrite a python program in 1 to 2 lines to call API in huggingface.\n\nThe answer should follow the format: {<<<domain>>> $DOMAIN, <<<api_call>>>: $API_CALL, <<<api_provider>>>: $API_PROVIDER, <<<explanation>>>: $EXPLANATION, <<<code>>>: $CODE}. Here are the requirements:\n1. The $API_CALL should have only 1 line of code that calls api.\n2. The $API_PROVIDER should be the programming framework used.\n3. $EXPLANATION should be a step-by-step explanation.\n4. The $CODE is the python code.\n5. Do not repeat the format in your answer.", "category": "generic"}
{"question_id": 106, "text": " As a city planner, I need to measure the depth of spaces in a series of images taken from streets.\n \n Use this API documentation for reference:  {\"domain\": \"Computer Vision Depth Estimation\", \"framework\": \"Hugging Face Transformers\", \"functionality\": \"Transformers\", \"api_name\": \"glpn-nyu-finetuned-diode-221221-102136\", \"api_call\": \"pipeline('depth-estimation', model='sayakpaul/glpn-nyu-finetuned-diode-221221-102136')\", \"api_arguments\": [], \"python_environment_requirements\": [\"Transformers 4.24.0\", \"Pytorch 1.12.1+cu116\", \"Datasets 2.8.0\", \"Tokenizers 0.13.2\"], \"example_code\": \"\", \"performance\": {\"dataset\": \"diode-subset\", \"accuracy\": {\"Loss\": 0.4222, \"Mae\": 0.411, \"Rmse\": 0.6292, \"Abs Rel\": 0.3778, \"Log Mae\": 0.1636, \"Log Rmse\": 0.224, \"Delta1\": 0.432, \"Delta2\": 0.6806, \"Delta3\": 0.8068}}, \"description\": \"This model is a fine-tuned version of vinvino02/glpn-nyu on the diode-subset dataset.\"}}\nWrite a python program in 1 to 2 lines to call API in huggingface.\n\nThe answer should follow the format: {<<<domain>>> $DOMAIN, <<<api_call>>>: $API_CALL, <<<api_provider>>>: $API_PROVIDER, <<<explanation>>>: $EXPLANATION, <<<code>>>: $CODE}. Here are the requirements:\n1. The $API_CALL should have only 1 line of code that calls api.\n2. The $API_PROVIDER should be the programming framework used.\n3. $EXPLANATION should be a step-by-step explanation.\n4. The $CODE is the python code.\n5. Do not repeat the format in your answer.", "category": "generic"}
{"question_id": 107, "text": " In our online ecommerce platform, we want to build an AI app to automatically recognize the type of products. It should be able to identify common items like clothing, electronics, furniture, and more.\n \n Use this API documentation for reference:  {\"domain\": \"Computer Vision Image Classification\", \"framework\": \"Hugging Face Transformers\", \"functionality\": \"Image Classification\", \"api_name\": \"facebook/convnext-large-224\", \"api_call\": \"ConvNextForImageClassification.from_pretrained('facebook/convnext-large-224')\", \"api_arguments\": {\"pretrained_model_name_or_path\": \"facebook/convnext-large-224\"}, \"python_environment_requirements\": {\"transformers\": \"Hugging Face Transformers\", \"torch\": \"PyTorch\", \"datasets\": \"Hugging Face Datasets\"}, \"example_code\": {\"import\": [\"from transformers import ConvNextFeatureExtractor, ConvNextForImageClassification\", \"import torch\", \"from datasets import load_dataset\"], \"load_dataset\": \"dataset = load_dataset('huggingface/cats-image')\", \"image\": \"image = dataset['test']['image'][0]\", \"feature_extractor\": \"feature_extractor = ConvNextFeatureExtractor.from_pretrained('facebook/convnext-large-224')\", \"model\": \"model = ConvNextForImageClassification.from_pretrained('facebook/convnext-large-224')\", \"inputs\": \"inputs = feature_extractor(image, return_tensors='pt')\", \"logits\": \"with torch.no_grad():\n  logits = model(**inputs).logits\", \"predicted_label\": \"predicted_label = logits.argmax(-1).item()\", \"print\": \"print(model.config.id2label[predicted_label])\"}, \"performance\": {\"dataset\": \"imagenet-1k\", \"accuracy\": \"Not specified\"}, \"description\": \"ConvNeXT is a pure convolutional model (ConvNet), inspired by the design of Vision Transformers, that claims to outperform them. The authors started from a ResNet and 'modernized' its design by taking the Swin Transformer as inspiration.\"}}\nWrite a python program in 1 to 2 lines to call API in huggingface.\n\nThe answer should follow the format: {<<<domain>>> $DOMAIN, <<<api_call>>>: $API_CALL, <<<api_provider>>>: $API_PROVIDER, <<<explanation>>>: $EXPLANATION, <<<code>>>: $CODE}. Here are the requirements:\n1. The $API_CALL should have only 1 line of code that calls api.\n2. The $API_PROVIDER should be the programming framework used.\n3. $EXPLANATION should be a step-by-step explanation.\n4. The $CODE is the python code.\n5. Do not repeat the format in your answer.", "category": "generic"}
{"question_id": 108, "text": " We need to recognize the breed of dog in the given image.\n \n Use this API documentation for reference:  {\"domain\": \"Computer Vision Image Classification\", \"framework\": \"Hugging Face Transformers\", \"functionality\": \"Image Classification\", \"api_name\": \"google/vit-base-patch16-224\", \"api_call\": \"ViTForImageClassification.from_pretrained('google/vit-base-patch16-224')\", \"api_arguments\": {\"pretrained_model_name_or_path\": \"google/vit-base-patch16-224\", \"from_tf\": \"False\", \"config\": \"None\", \"cache_dir\": \"None\", \"revision\": \"None\", \"use_auth_token\": \"False\"}, \"python_environment_requirements\": {\"transformers\": \"4.0.0\", \"torch\": \"1.9.0\", \"PIL\": \"8.3.2\", \"requests\": \"2.26.0\"}, \"example_code\": {\"1\": \"from transformers import ViTImageProcessor, ViTForImageClassification\", \"2\": \"from PIL import Image\", \"3\": \"import requests\", \"4\": \"url = 'http://images.cocodataset.org/val2017/000000039769.jpg'\", \"5\": \"image = Image.open(requests.get(url, stream=True).raw)\", \"6\": \"processor = ViTImageProcessor.from_pretrained('google/vit-base-patch16-224')\", \"7\": \"model = ViTForImageClassification.from_pretrained('google/vit-base-patch16-224')\", \"8\": \"inputs = processor(images=image, return_tensors='pt')\", \"9\": \"outputs = model(**inputs)\", \"10\": \"logits = outputs.logits\", \"11\": \"predicted_class_idx = logits.argmax(-1).item()\", \"12\": \"print('Predicted class:', model.config.id2label[predicted_class_idx])\"}, \"performance\": {\"dataset\": \"imagenet-1k\", \"accuracy\": \"Not provided\"}, \"description\": \"Vision Transformer (ViT) model pre-trained on ImageNet-21k (14 million images, 21,843 classes) at resolution 224x224, and fine-tuned on ImageNet 2012 (1 million images, 1,000 classes) at resolution 224x224. It was introduced in the paper An Image is Worth 16x16 Words: Transformers for Image Recognition at Scale by Dosovitskiy et al.\"}}\nWrite a python program in 1 to 2 lines to call API in huggingface.\n\nThe answer should follow the format: {<<<domain>>> $DOMAIN, <<<api_call>>>: $API_CALL, <<<api_provider>>>: $API_PROVIDER, <<<explanation>>>: $EXPLANATION, <<<code>>>: $CODE}. Here are the requirements:\n1. The $API_CALL should have only 1 line of code that calls api.\n2. The $API_PROVIDER should be the programming framework used.\n3. $EXPLANATION should be a step-by-step explanation.\n4. The $CODE is the python code.\n5. Do not repeat the format in your answer.", "category": "generic"}
{"question_id": 109, "text": " Develop a solution that can categorize an image of a cell phone, laptop, or smartwatch as one of these respective device types.\n \n Use this API documentation for reference:  {\"domain\": \"Computer Vision Image Classification\", \"framework\": \"Hugging Face Transformers\", \"functionality\": \"Transformers\", \"api_name\": \"lysandre/tiny-vit-random\", \"api_call\": \"ViTForImageClassification.from_pretrained('lysandre/tiny-vit-random')\", \"api_arguments\": \"image_path\", \"python_environment_requirements\": \"transformers\", \"example_code\": \"\", \"performance\": {\"dataset\": \"\", \"accuracy\": \"\"}, \"description\": \"A tiny-vit-random model for image classification using Hugging Face Transformers.\"}}\nWrite a python program in 1 to 2 lines to call API in huggingface.\n\nThe answer should follow the format: {<<<domain>>> $DOMAIN, <<<api_call>>>: $API_CALL, <<<api_provider>>>: $API_PROVIDER, <<<explanation>>>: $EXPLANATION, <<<code>>>: $CODE}. Here are the requirements:\n1. The $API_CALL should have only 1 line of code that calls api.\n2. The $API_PROVIDER should be the programming framework used.\n3. $EXPLANATION should be a step-by-step explanation.\n4. The $CODE is the python code.\n5. Do not repeat the format in your answer.", "category": "generic"}
{"question_id": 110, "text": " Build a system to help companies identify logos from a collection of images.\n \n Use this API documentation for reference:  {\"domain\": \"Computer Vision Image Classification\", \"framework\": \"Hugging Face Transformers\", \"functionality\": \"Image Classification\", \"api_name\": \"convnextv2_huge.fcmae_ft_in1k\", \"api_call\": \"timm.create_model('convnextv2_huge.fcmae_ft_in1k', pretrained=True)\", \"api_arguments\": {\"pretrained\": \"True\"}, \"python_environment_requirements\": [\"timm\"], \"example_code\": \"from urllib.request import urlopen\nfrom PIL import Image\nimport timm\nimg = Image.open(urlopen('https://huggingface.co/datasets/huggingface/documentation-images/resolve/main/beignets-task-guide.png'))\nmodel = timm.create_model('convnextv2_huge.fcmae_ft_in1k', pretrained=True)\nmodel = model.eval()\ndata_config = timm.data.resolve_model_data_config(model)\ntransforms = timm.data.create_transform(**data_config, is_training=False)\noutput = model(transforms(img).unsqueeze(0))\ntop5_probabilities, top5_class_indices = torch.topk(output.softmax(dim=1) * 100, k=5)\", \"performance\": {\"dataset\": \"imagenet-1k\", \"accuracy\": 86.256}, \"description\": \"A ConvNeXt-V2 image classification model. Pretrained with a fully convolutional masked autoencoder framework (FCMAE) and fine-tuned on ImageNet-1k.\"}}\nWrite a python program in 1 to 2 lines to call API in huggingface.\n\nThe answer should follow the format: {<<<domain>>> $DOMAIN, <<<api_call>>>: $API_CALL, <<<api_provider>>>: $API_PROVIDER, <<<explanation>>>: $EXPLANATION, <<<code>>>: $CODE}. Here are the requirements:\n1. The $API_CALL should have only 1 line of code that calls api.\n2. The $API_PROVIDER should be the programming framework used.\n3. $EXPLANATION should be a step-by-step explanation.\n4. The $CODE is the python code.\n5. Do not repeat the format in your answer.", "category": "generic"}
{"question_id": 111, "text": " Develop a pipeline that detects objects present in an image using computer vision.\n \n Use this API documentation for reference:  {\"domain\": \"Computer Vision Object Detection\", \"framework\": \"Hugging Face Transformers\", \"functionality\": \"Object Detection\", \"api_name\": \"hustvl/yolos-tiny\", \"api_call\": \"YolosForObjectDetection.from_pretrained('hustvl/yolos-tiny')\", \"api_arguments\": {\"images\": \"image\", \"return_tensors\": \"pt\"}, \"python_environment_requirements\": [\"transformers\", \"PIL\", \"requests\"], \"example_code\": \"from transformers import YolosFeatureExtractor, YolosForObjectDetection\nfrom PIL import Image\nimport requests\nurl = 'http://images.cocodataset.org/val2017/000000039769.jpg'\nimage = Image.open(requests.get(url, stream=True).raw)\nfeature_extractor = YolosFeatureExtractor.from_pretrained('hustvl/yolos-tiny')\nmodel = YolosForObjectDetection.from_pretrained('hustvl/yolos-tiny')\ninputs = feature_extractor(images=image, return_tensors=pt)\noutputs = model(**inputs)\nlogits = outputs.logits\nbboxes = outputs.pred_boxes\", \"performance\": {\"dataset\": \"COCO 2017 validation\", \"accuracy\": \"28.7 AP\"}, \"description\": \"YOLOS is a Vision Transformer (ViT) trained using the DETR loss. Despite its simplicity, a base-sized YOLOS model is able to achieve 42 AP on COCO validation 2017 (similar to DETR and more complex frameworks such as Faster R-CNN). The model is trained using a bipartite matching loss: one compares the predicted classes + bounding boxes of each of the N = 100 object queries to the ground truth annotations, padded up to the same length N (so if an image only contains 4 objects, 96 annotations will just have a no object as class and no bounding box as bounding box). The Hungarian matching algorithm is used to create an optimal one-to-one mapping between each of the N queries and each of the N annotations. Next, standard cross-entropy (for the classes) and a linear combination of the L1 and generalized IoU loss (for the bounding boxes) are used to optimize the parameters of the model.\"}}\nWrite a python program in 1 to 2 lines to call API in huggingface.\n\nThe answer should follow the format: {<<<domain>>> $DOMAIN, <<<api_call>>>: $API_CALL, <<<api_provider>>>: $API_PROVIDER, <<<explanation>>>: $EXPLANATION, <<<code>>>: $CODE}. Here are the requirements:\n1. The $API_CALL should have only 1 line of code that calls api.\n2. The $API_PROVIDER should be the programming framework used.\n3. $EXPLANATION should be a step-by-step explanation.\n4. The $CODE is the python code.\n5. Do not repeat the format in your answer.", "category": "generic"}
{"question_id": 112, "text": " Assit me to process and segment an image for further analysis.\n \n Use this API documentation for reference:  {\"domain\": \"Computer Vision Image Segmentation\", \"framework\": \"Hugging Face Transformers\", \"functionality\": \"Image Segmentation\", \"api_name\": \"nvidia/segformer-b5-finetuned-ade-640-640\", \"api_call\": \"SegformerForSemanticSegmentation.from_pretrained('nvidia/segformer-b5-finetuned-ade-640-640')\", \"api_arguments\": [\"images\", \"return_tensors\"], \"python_environment_requirements\": [\"transformers\", \"PIL\", \"requests\"], \"example_code\": \"from transformers import SegformerFeatureExtractor, SegformerForSemanticSegmentation\nfrom PIL import Image\nimport requests\nfeature_extractor = SegformerFeatureExtractor.from_pretrained(nvidia/segformer-b5-finetuned-ade-512-512)\nmodel = SegformerForSemanticSegmentation.from_pretrained(nvidia/segformer-b5-finetuned-ade-512-512)\nurl = http://images.cocodataset.org/val2017/000000039769.jpg\nimage = Image.open(requests.get(url, stream=True).raw)\ninputs = feature_extractor(images=image, return_tensors=pt)\noutputs = model(**inputs)\nlogits = outputs.logits\", \"performance\": {\"dataset\": \"ADE20K\", \"accuracy\": \"Not provided\"}, \"description\": \"SegFormer model fine-tuned on ADE20k at resolution 640x640. It was introduced in the paper SegFormer: Simple and Efficient Design for Semantic Segmentation with Transformers by Xie et al. and first released in this repository.\"}}\nWrite a python program in 1 to 2 lines to call API in huggingface.\n\nThe answer should follow the format: {<<<domain>>> $DOMAIN, <<<api_call>>>: $API_CALL, <<<api_provider>>>: $API_PROVIDER, <<<explanation>>>: $EXPLANATION, <<<code>>>: $CODE}. Here are the requirements:\n1. The $API_CALL should have only 1 line of code that calls api.\n2. The $API_PROVIDER should be the programming framework used.\n3. $EXPLANATION should be a step-by-step explanation.\n4. The $CODE is the python code.\n5. Do not repeat the format in your answer.", "category": "generic"}
{"question_id": 113, "text": " We need to analyze satellite images to categorize the types of land use. For this purpose, I need to segment the images and identify different objects.\n \n Use this API documentation for reference:  {\"domain\": \"Computer Vision Image Segmentation\", \"framework\": \"Hugging Face Transformers\", \"functionality\": \"Transformers\", \"api_name\": \"shi-labs/oneformer_coco_swin_large\", \"api_call\": \"'OneFormerForUniversalSegmentation.from_pretrained(shi-labs/oneformer_coco_swin_large)'\", \"api_arguments\": {\"images\": \"image\", \"task_inputs\": [\"semantic\", \"instance\", \"panoptic\"], \"return_tensors\": \"pt\"}, \"python_environment_requirements\": [\"transformers\", \"PIL\", \"requests\"], \"example_code\": \"from transformers import OneFormerProcessor, OneFormerForUniversalSegmentation\nfrom PIL import Image\nimport requests\nurl = https://huggingface.co/datasets/shi-labs/oneformer_demo/blob/main/coco.jpeg\nimage = Image.open(requests.get(url, stream=True).raw)\nprocessor = OneFormerProcessor.from_pretrained(shi-labs/oneformer_coco_swin_large)\nmodel = OneFormerForUniversalSegmentation.from_pretrained(shi-labs/oneformer_coco_swin_large)\nsemantic_inputs = processor(images=image, task_inputs=[semantic], return_tensors=pt)\nsemantic_outputs = model(**semantic_inputs)\npredicted_semantic_map = processor.post_process_semantic_segmentation(outputs, target_sizes=[image.size[::-1]])[0]\", \"performance\": {\"dataset\": \"ydshieh/coco_dataset_script\", \"accuracy\": \"Not provided\"}, \"description\": \"OneFormer model trained on the COCO dataset (large-sized version, Swin backbone). It was introduced in the paper OneFormer: One Transformer to Rule Universal Image Segmentation by Jain et al. and first released in this repository. OneFormer is the first multi-task universal image segmentation framework. It needs to be trained only once with a single universal architecture, a single model, and on a single dataset, to outperform existing specialized models across semantic, instance, and panoptic segmentation tasks. OneFormer uses a task token to condition the model on the task in focus, making the architecture task-guided for training, and task-dynamic for inference, all with a single model.\"}}\nWrite a python program in 1 to 2 lines to call API in huggingface.\n\nThe answer should follow the format: {<<<domain>>> $DOMAIN, <<<api_call>>>: $API_CALL, <<<api_provider>>>: $API_PROVIDER, <<<explanation>>>: $EXPLANATION, <<<code>>>: $CODE}. Here are the requirements:\n1. The $API_CALL should have only 1 line of code that calls api.\n2. The $API_PROVIDER should be the programming framework used.\n3. $EXPLANATION should be a step-by-step explanation.\n4. The $CODE is the python code.\n5. Do not repeat the format in your answer.", "category": "generic"}
{"question_id": 114, "text": " We are a city planning department and want to evaluate the city layout. Analyze the image we provide to segment and understand the various urban elements.\n \n Use this API documentation for reference:  {\"domain\": \"Computer Vision Image Segmentation\", \"framework\": \"Hugging Face Transformers\", \"functionality\": \"Semantic Segmentation\", \"api_name\": \"nvidia/segformer-b5-finetuned-cityscapes-1024-1024\", \"api_call\": \"SegformerForSemanticSegmentation.from_pretrained('nvidia/segformer-b5-finetuned-cityscapes-1024-1024')\", \"api_arguments\": {\"images\": \"image\", \"return_tensors\": \"pt\"}, \"python_environment_requirements\": {\"packages\": [\"transformers\", \"PIL\", \"requests\"]}, \"example_code\": \"from transformers import SegformerFeatureExtractor, SegformerForSemanticSegmentation\nfrom PIL import Image\nimport requests\nfeature_extractor = SegformerFeatureExtractor.from_pretrained(nvidia/segformer-b5-finetuned-cityscapes-1024-1024)\nmodel = SegformerForSemanticSegmentation.from_pretrained(nvidia/segformer-b5-finetuned-cityscapes-1024-1024)\nurl = http://images.cocodataset.org/val2017/000000039769.jpg\nimage = Image.open(requests.get(url, stream=True).raw)\ninputs = feature_extractor(images=image, return_tensors=pt)\noutputs = model(**inputs)\nlogits = outputs.logits\", \"performance\": {\"dataset\": \"CityScapes\", \"accuracy\": \"Not provided\"}, \"description\": \"SegFormer model fine-tuned on CityScapes at resolution 1024x1024. It was introduced in the paper SegFormer: Simple and Efficient Design for Semantic Segmentation with Transformers by Xie et al. and first released in this repository.\"}}\nWrite a python program in 1 to 2 lines to call API in huggingface.\n\nThe answer should follow the format: {<<<domain>>> $DOMAIN, <<<api_call>>>: $API_CALL, <<<api_provider>>>: $API_PROVIDER, <<<explanation>>>: $EXPLANATION, <<<code>>>: $CODE}. Here are the requirements:\n1. The $API_CALL should have only 1 line of code that calls api.\n2. The $API_PROVIDER should be the programming framework used.\n3. $EXPLANATION should be a step-by-step explanation.\n4. The $CODE is the python code.\n5. Do not repeat the format in your answer.", "category": "generic"}
{"question_id": 115, "text": " My company develops drones for agriculture purposes, and we need a model to segment aerial images accurately.\n \n Use this API documentation for reference:  {\"domain\": \"Computer Vision Image Segmentation\", \"framework\": \"Hugging Face Transformers\", \"functionality\": \"Transformers\", \"api_name\": \"facebook/maskformer-swin-base-ade\", \"api_call\": \"MaskFormerForInstanceSegmentation.from_pretrained('facebook/maskformer-swin-base-ade')\", \"api_arguments\": {\"from_pretrained\": \"facebook/maskformer-swin-base-ade\"}, \"python_environment_requirements\": {\"transformers\": \"latest\", \"PIL\": \"latest\", \"requests\": \"latest\"}, \"example_code\": \"from transformers import MaskFormerFeatureExtractor, MaskFormerForInstanceSegmentation\nfrom PIL import Image\nimport requests\nurl = https://huggingface.co/datasets/hf-internal-testing/fixtures_ade20k/resolve/main/ADE_val_00000001.jpg\nimage = Image.open(requests.get(url, stream=True).raw)\nfeature_extractor = MaskFormerFeatureExtractor.from_pretrained(facebook/maskformer-swin-base-ade)\ninputs = feature_extractor(images=image, return_tensors=pt)\nmodel = MaskFormerForInstanceSegmentation.from_pretrained(facebook/maskformer-swin-base-ade)\noutputs = model(**inputs)\nclass_queries_logits = outputs.class_queries_logits\nmasks_queries_logits = outputs.masks_queries_logits\npredicted_semantic_map = feature_extractor.post_process_semantic_segmentation(outputs, target_sizes=[image.size[::-1]])[0]\", \"performance\": {\"dataset\": \"ADE20k\", \"accuracy\": \"Not provided\"}, \"description\": \"MaskFormer model trained on ADE20k semantic segmentation (base-sized version, Swin backbone). It was introduced in the paper Per-Pixel Classification is Not All You Need for Semantic Segmentation and first released in this repository. This model addresses instance, semantic and panoptic segmentation with the same paradigm: by predicting a set of masks and corresponding labels. Hence, all 3 tasks are treated as if they were instance segmentation.\"}}\nWrite a python program in 1 to 2 lines to call API in huggingface.\n\nThe answer should follow the format: {<<<domain>>> $DOMAIN, <<<api_call>>>: $API_CALL, <<<api_provider>>>: $API_PROVIDER, <<<explanation>>>: $EXPLANATION, <<<code>>>: $CODE}. Here are the requirements:\n1. The $API_CALL should have only 1 line of code that calls api.\n2. The $API_PROVIDER should be the programming framework used.\n3. $EXPLANATION should be a step-by-step explanation.\n4. The $CODE is the python code.\n5. Do not repeat the format in your answer.", "category": "generic"}
{"question_id": 116, "text": " I want to generate images from text descriptions and use the scribble images as control inputs for my project.\n \n Use this API documentation for reference:  {\"domain\": \"Computer Vision Image-to-Image\", \"framework\": \"Diffusers\", \"functionality\": \"Text-to-Image\", \"api_name\": \"lllyasviel/control_v11p_sd15_scribble\", \"api_call\": \"ControlNetModel.from_pretrained('lllyasviel/control_v11p_sd15_scribble')\", \"api_arguments\": {\"checkpoint\": \"lllyasviel/control_v11p_sd15_scribble\", \"torch_dtype\": \"torch.float16\"}, \"python_environment_requirements\": [\"diffusers\", \"transformers\", \"accelerate\", \"controlnet_aux==0.3.0\"], \"example_code\": \"import torch\nimport os\nfrom huggingface_hub import HfApi\nfrom pathlib import Path\nfrom diffusers.utils import load_image\nfrom PIL import Image\nimport numpy as np\nfrom controlnet_aux import PidiNetDetector, HEDdetector\nfrom diffusers import (\n ControlNetModel,\n StableDiffusionControlNetPipeline,\n UniPCMultistepScheduler,\n)\ncheckpoint = lllyasviel/control_v11p_sd15_scribble\nimage = load_image(\n https://huggingface.co/lllyasviel/control_v11p_sd15_scribble/resolve/main/images/input.png\n)\nprompt = royal chamber with fancy bed\nprocessor = HEDdetector.from_pretrained('lllyasviel/Annotators')\ncontrol_image = processor(image, scribble=True)\ncontrol_image.save(./images/control.png)\ncontrolnet = ControlNetModel.from_pretrained(checkpoint, torch_dtype=torch.float16)\npipe = StableDiffusionControlNetPipeline.from_pretrained(\n runwayml/stable-diffusion-v1-5, controlnet=controlnet, torch_dtype=torch.float16\n)\npipe.scheduler = UniPCMultistepScheduler.from_config(pipe.scheduler.config)\npipe.enable_model_cpu_offload()\ngenerator = torch.manual_seed(0)\nimage = pipe(prompt, num_inference_steps=30, generator=generator, image=control_image).images[0]\nimage.save('images/image_out.png')\", \"performance\": {\"dataset\": \"Stable Diffusion v1-5\", \"accuracy\": \"Not specified\"}, \"description\": \"Controlnet v1.1 is a neural network structure to control diffusion models by adding extra conditions. This checkpoint corresponds to the ControlNet conditioned on Scribble images. It can be used in combination with Stable Diffusion, such as runwayml/stable-diffusion-v1-5.\"}}\nWrite a python program in 1 to 2 lines to call API in huggingface.\n\nThe answer should follow the format: {<<<domain>>> $DOMAIN, <<<api_call>>>: $API_CALL, <<<api_provider>>>: $API_PROVIDER, <<<explanation>>>: $EXPLANATION, <<<code>>>: $CODE}. Here are the requirements:\n1. The $API_CALL should have only 1 line of code that calls api.\n2. The $API_PROVIDER should be the programming framework used.\n3. $EXPLANATION should be a step-by-step explanation.\n4. The $CODE is the python code.\n5. Do not repeat the format in your answer.", "category": "generic"}
{"question_id": 118, "text": " We want to recommend workouts to our users, based on the type of sports they enjoy. Help us classify sports videos.\n \n Use this API documentation for reference:  {\"domain\": \"Computer Vision Video Classification\", \"framework\": \"Hugging Face Transformers\", \"functionality\": \"Video Classification\", \"api_name\": \"MCG-NJU/videomae-large-finetuned-kinetics\", \"api_call\": \"VideoMAEForVideoClassification.from_pretrained('MCG-NJU/videomae-large-finetuned-kinetics')\", \"api_arguments\": [\"video\"], \"python_environment_requirements\": [\"transformers\"], \"example_code\": \"from transformers import VideoMAEImageProcessor, VideoMAEForVideoClassification\nimport numpy as np\nimport torch\nvideo = list(np.random.randn(16, 3, 224, 224))\nprocessor = VideoMAEImageProcessor.from_pretrained(MCG-NJU/videomae-large-finetuned-kinetics)\nmodel = VideoMAEForVideoClassification.from_pretrained(MCG-NJU/videomae-large-finetuned-kinetics)\ninputs = processor(video, return_tensors=pt)\nwith torch.no_grad():\n outputs = model(**inputs)\n logits = outputs.logits\npredicted_class_idx = logits.argmax(-1).item()\nprint(Predicted class:, model.config.id2label[predicted_class_idx])\", \"performance\": {\"dataset\": \"Kinetics-400\", \"accuracy\": {\"top-1\": 84.7, \"top-5\": 96.5}}, \"description\": \"VideoMAE model pre-trained for 1600 epochs in a self-supervised way and fine-tuned in a supervised way on Kinetics-400. It was introduced in the paper VideoMAE: Masked Autoencoders are Data-Efficient Learners for Self-Supervised Video Pre-Training by Tong et al. and first released in this repository.\"}}\nWrite a python program in 1 to 2 lines to call API in huggingface.\n\nThe answer should follow the format: {<<<domain>>> $DOMAIN, <<<api_call>>>: $API_CALL, <<<api_provider>>>: $API_PROVIDER, <<<explanation>>>: $EXPLANATION, <<<code>>>: $CODE}. Here are the requirements:\n1. The $API_CALL should have only 1 line of code that calls api.\n2. The $API_PROVIDER should be the programming framework used.\n3. $EXPLANATION should be a step-by-step explanation.\n4. The $CODE is the python code.\n5. Do not repeat the format in your answer.", "category": "generic"}
{"question_id": 119, "text": " We need to classify videos showing different actions for our new video moderation system.\n \n Use this API documentation for reference:  {\"domain\": \"Computer Vision Video Classification\", \"framework\": \"Hugging Face Transformers\", \"functionality\": \"Video Classification\", \"api_name\": \"videomae-base-short-ssv2\", \"api_call\": \"VideoMAEForPreTraining.from_pretrained('MCG-NJU/videomae-base-short-ssv2')\", \"api_arguments\": [\"video\", \"return_tensors\"], \"python_environment_requirements\": [\"transformers\", \"numpy\", \"torch\"], \"example_code\": \"from transformers import VideoMAEImageProcessor, VideoMAEForPreTraining\nimport numpy as np\nimport torch\nnum_frames = 16\nvideo = list(np.random.randn(16, 3, 224, 224))\nprocessor = VideoMAEImageProcessor.from_pretrained(MCG-NJU/videomae-base-short-ssv2)\nmodel = VideoMAEForPreTraining.from_pretrained(MCG-NJU/videomae-base-short-ssv2)\npixel_values = processor(video, return_tensors=pt).pixel_values\nnum_patches_per_frame = (model.config.image_size // model.config.patch_size) ** 2\nseq_length = (num_frames // model.config.tubelet_size) * num_patches_per_frame\nbool_masked_pos = torch.randint(0, 2, (1, seq_length)).bool()\noutputs = model(pixel_values, bool_masked_pos=bool_masked_pos)\nloss = outputs.loss\", \"performance\": {\"dataset\": \"Something-Something-v2\", \"accuracy\": \"N/A\"}, \"description\": \"VideoMAE is an extension of Masked Autoencoders (MAE) to video. The architecture of the model is very similar to that of a standard Vision Transformer (ViT), with a decoder on top for predicting pixel values for masked patches. Videos are presented to the model as a sequence of fixed-size patches (resolution 16x16), which are linearly embedded. One also adds a [CLS] token to the beginning of a sequence to use it for classification tasks. One also adds fixed sinus/cosinus position embeddings before feeding the sequence to the layers of the Transformer encoder. By pre-training the model, it learns an inner representation of videos that can then be used to extract features useful for downstream tasks: if you have a dataset of labeled videos for instance, you can train a standard classifier by placing a linear layer on top of the pre-trained encoder. One typically places a linear layer on top of the [CLS] token, as the last hidden state of this token can be seen as a representation of an entire video.\"}}\nWrite a python program in 1 to 2 lines to call API in huggingface.\n\nThe answer should follow the format: {<<<domain>>> $DOMAIN, <<<api_call>>>: $API_CALL, <<<api_provider>>>: $API_PROVIDER, <<<explanation>>>: $EXPLANATION, <<<code>>>: $CODE}. Here are the requirements:\n1. The $API_CALL should have only 1 line of code that calls api.\n2. The $API_PROVIDER should be the programming framework used.\n3. $EXPLANATION should be a step-by-step explanation.\n4. The $CODE is the python code.\n5. Do not repeat the format in your answer.", "category": "generic"}
{"question_id": 120, "text": " I need the AI to tell if an image is of a cat or a dog, regardless of its breed or species.\n \n Use this API documentation for reference:  {\"domain\": \"Computer Vision Zero-Shot Image Classification\", \"framework\": \"Hugging Face Transformers\", \"functionality\": \"Zero-Shot Image Classification\", \"api_name\": \"openai/clip-vit-base-patch32\", \"api_call\": \"CLIPModel.from_pretrained('openai/clip-vit-base-patch32')\", \"api_arguments\": [\"text\", \"images\", \"return_tensors\", \"padding\"], \"python_environment_requirements\": [\"PIL\", \"requests\", \"transformers\"], \"example_code\": \"from PIL import Image\nimport requests\nfrom transformers import CLIPProcessor, CLIPModel\nmodel = CLIPModel.from_pretrained(openai/clip-vit-base-patch32)\nprocessor = CLIPProcessor.from_pretrained(openai/clip-vit-base-patch32)\nurl = http://images.cocodataset.org/val2017/000000039769.jpg\nimage = Image.open(requests.get(url, stream=True).raw)\ninputs = processor(text=[a photo of a cat, a photo of a dog], images=image, return_tensors=pt, padding=True)\noutputs = model(**inputs)\nlogits_per_image = outputs.logits_per_image\nprobs = logits_per_image.softmax(dim=1)\", \"performance\": {\"dataset\": [\"Food101\", \"CIFAR10\", \"CIFAR100\", \"Birdsnap\", \"SUN397\", \"Stanford Cars\", \"FGVC Aircraft\", \"VOC2007\", \"DTD\", \"Oxford-IIIT Pet dataset\", \"Caltech101\", \"Flowers102\", \"MNIST\", \"SVHN\", \"IIIT5K\", \"Hateful Memes\", \"SST-2\", \"UCF101\", \"Kinetics700\", \"Country211\", \"CLEVR Counting\", \"KITTI Distance\", \"STL-10\", \"RareAct\", \"Flickr30\", \"MSCOCO\", \"ImageNet\", \"ImageNet-A\", \"ImageNet-R\", \"ImageNet Sketch\", \"ObjectNet (ImageNet Overlap)\", \"Youtube-BB\", \"ImageNet-Vid\"], \"accuracy\": \"varies\"}, \"description\": \"The CLIP model was developed by researchers at OpenAI to learn about what contributes to robustness in computer vision tasks. The model was also developed to test the ability of models to generalize to arbitrary image classification tasks in a zero-shot manner.\"}}\nWrite a python program in 1 to 2 lines to call API in huggingface.\n\nThe answer should follow the format: {<<<domain>>> $DOMAIN, <<<api_call>>>: $API_CALL, <<<api_provider>>>: $API_PROVIDER, <<<explanation>>>: $EXPLANATION, <<<code>>>: $CODE}. Here are the requirements:\n1. The $API_CALL should have only 1 line of code that calls api.\n2. The $API_PROVIDER should be the programming framework used.\n3. $EXPLANATION should be a step-by-step explanation.\n4. The $CODE is the python code.\n5. Do not repeat the format in your answer.", "category": "generic"}
{"question_id": 121, "text": " We have a set of pictures for pets (dogs and cats). We need to offer an AI-based solution to classify the pictures given the pet name.\n \n Use this API documentation for reference:  {\"domain\": \"Computer Vision Zero-Shot Image Classification\", \"framework\": \"Hugging Face\", \"functionality\": \"Zero-Shot Image Classification\", \"api_name\": \"CLIPModel.from_pretrained('laion/CLIP-convnext_base_w-laion2B-s13B-b82K')\", \"api_call\": \"CLIPModel.from_pretrained('laion/CLIP-convnext_base_w-laion2B-s13B-b82K')\", \"api_arguments\": {\"image_path\": \"path to the image file\", \"labels\": \"list of possible class names\"}, \"python_environment_requirements\": [\"transformers\"], \"example_code\": \"from transformers import pipeline; clip = pipeline('image-classification', model='laion/CLIP-convnext_base_w-laion2B-s13B-b82K'); clip('path/to/image.jpg', ['cat', 'dog'])\", \"performance\": {\"dataset\": \"ImageNet-1k\", \"accuracy\": \"70.8 - 71.7%\"}, \"description\": \"A series of CLIP ConvNeXt-Base (w/ wide embed dim) models trained on subsets LAION-5B using OpenCLIP. The models achieve between 70.8 and 71.7 zero-shot top-1 accuracy on ImageNet-1k. The models can be used for zero-shot image classification, image and text retrieval, and other related tasks.\"}}\nWrite a python program in 1 to 2 lines to call API in huggingface.\n\nThe answer should follow the format: {<<<domain>>> $DOMAIN, <<<api_call>>>: $API_CALL, <<<api_provider>>>: $API_PROVIDER, <<<explanation>>>: $EXPLANATION, <<<code>>>: $CODE}. Here are the requirements:\n1. The $API_CALL should have only 1 line of code that calls api.\n2. The $API_PROVIDER should be the programming framework used.\n3. $EXPLANATION should be a step-by-step explanation.\n4. The $CODE is the python code.\n5. Do not repeat the format in your answer.", "category": "generic"}
{"question_id": 122, "text": " Our startup team is now building an app for diagnosing plant diseases based on images. We need to get the diagnosis for different types of plant issues.\n \n Use this API documentation for reference:  {\"domain\": \"Computer Vision Zero-Shot Image Classification\", \"framework\": \"Hugging Face\", \"functionality\": \"Zero-Shot Image Classification\", \"api_name\": \"timm/eva02_enormous_patch14_plus_clip_224.laion2b_s9b_b144k\", \"api_call\": \"clip.load('timm/eva02_enormous_patch14_plus_clip_224.laion2b_s9b_b144k')\", \"api_arguments\": \"image, class_names\", \"python_environment_requirements\": \"huggingface_hub, openai, transformers\", \"example_code\": \"N/A\", \"performance\": {\"dataset\": \"N/A\", \"accuracy\": \"N/A\"}, \"description\": \"This model is a zero-shot image classification model based on OpenCLIP. It can be used for classifying images into various categories without any additional training.\"}}\nWrite a python program in 1 to 2 lines to call API in huggingface.\n\nThe answer should follow the format: {<<<domain>>> $DOMAIN, <<<api_call>>>: $API_CALL, <<<api_provider>>>: $API_PROVIDER, <<<explanation>>>: $EXPLANATION, <<<code>>>: $CODE}. Here are the requirements:\n1. The $API_CALL should have only 1 line of code that calls api.\n2. The $API_PROVIDER should be the programming framework used.\n3. $EXPLANATION should be a step-by-step explanation.\n4. The $CODE is the python code.\n5. Do not repeat the format in your answer.", "category": "generic"}
{"question_id": 123, "text": " We need to analyze customer reviews and find out how well our new product is doing in the market.\n \n Use this API documentation for reference:  {\"domain\": \"Natural Language Processing Text Classification\", \"framework\": \"Hugging Face Transformers\", \"functionality\": \"Sentiment Analysis\", \"api_name\": \"bert-base-multilingual-uncased-sentiment\", \"api_call\": \"pipeline('sentiment-analysis', model='nlptown/bert-base-multilingual-uncased-sentiment')\", \"api_arguments\": [\"text\"], \"python_environment_requirements\": [\"transformers\"], \"example_code\": \"result = sentiment_pipeline('I love this product!')\", \"performance\": {\"dataset\": [{\"language\": \"English\", \"accuracy\": {\"exact\": \"67%\", \"off-by-1\": \"95%\"}}, {\"language\": \"Dutch\", \"accuracy\": {\"exact\": \"57%\", \"off-by-1\": \"93%\"}}, {\"language\": \"German\", \"accuracy\": {\"exact\": \"61%\", \"off-by-1\": \"94%\"}}, {\"language\": \"French\", \"accuracy\": {\"exact\": \"59%\", \"off-by-1\": \"94%\"}}, {\"language\": \"Italian\", \"accuracy\": {\"exact\": \"59%\", \"off-by-1\": \"95%\"}}, {\"language\": \"Spanish\", \"accuracy\": {\"exact\": \"58%\", \"off-by-1\": \"95%\"}}]}, \"description\": \"This a bert-base-multilingual-uncased model finetuned for sentiment analysis on product reviews in six languages: English, Dutch, German, French, Spanish and Italian. It predicts the sentiment of the review as a number of stars (between 1 and 5).\"}}\nWrite a python program in 1 to 2 lines to call API in huggingface.\n\nThe answer should follow the format: {<<<domain>>> $DOMAIN, <<<api_call>>>: $API_CALL, <<<api_provider>>>: $API_PROVIDER, <<<explanation>>>: $EXPLANATION, <<<code>>>: $CODE}. Here are the requirements:\n1. The $API_CALL should have only 1 line of code that calls api.\n2. The $API_PROVIDER should be the programming framework used.\n3. $EXPLANATION should be a step-by-step explanation.\n4. The $CODE is the python code.\n5. Do not repeat the format in your answer.", "category": "generic"}
{"question_id": 124, "text": " A new tutoring company is founded, and they want a tutoring AI. To do so, they need help in creating better explanations for a chemistry concept.\n \n Use this API documentation for reference:  {\"domain\": \"Natural Language Processing Text Classification\", \"framework\": \"Hugging Face Transformers\", \"functionality\": \"Paraphrase-based utterance augmentation\", \"api_name\": \"prithivida/parrot_fluency_model\", \"api_call\": \"pipeline('text-classification', model='prithivida/parrot_fluency_model')\", \"api_arguments\": \"text\", \"python_environment_requirements\": [\"transformers\"], \"example_code\": \"parrot('your input text')\", \"performance\": {\"dataset\": \"N/A\", \"accuracy\": \"N/A\"}, \"description\": \"Parrot is a paraphrase-based utterance augmentation framework purpose-built to accelerate training NLU models. A paraphrase framework is more than just a paraphrasing model.\"}}\nWrite a python program in 1 to 2 lines to call API in huggingface.\n\nThe answer should follow the format: {<<<domain>>> $DOMAIN, <<<api_call>>>: $API_CALL, <<<api_provider>>>: $API_PROVIDER, <<<explanation>>>: $EXPLANATION, <<<code>>>: $CODE}. Here are the requirements:\n1. The $API_CALL should have only 1 line of code that calls api.\n2. The $API_PROVIDER should be the programming framework used.\n3. $EXPLANATION should be a step-by-step explanation.\n4. The $CODE is the python code.\n5. Do not repeat the format in your answer.", "category": "generic"}
{"question_id": 125, "text": " Create a function that can determine if a given text is a question or a statement.\n \n Use this API documentation for reference:  {\"domain\": \"Natural Language Processing Text Classification\", \"framework\": \"Hugging Face Transformers\", \"functionality\": \"Text Classification\", \"api_name\": \"shahrukhx01/question-vs-statement-classifier\", \"api_call\": \"AutoModelForSequenceClassification.from_pretrained('shahrukhx01/question-vs-statement-classifier')\", \"api_arguments\": {\"tokenizer\": \"AutoTokenizer.from_pretrained(shahrukhx01/question-vs-statement-classifier)\"}, \"python_environment_requirements\": {\"transformers\": \"from transformers import AutoTokenizer, AutoModelForSequenceClassification\"}, \"example_code\": \"tokenizer = AutoTokenizer.from_pretrained(shahrukhx01/question-vs-statement-classifier)\nmodel = AutoModelForSequenceClassification.from_pretrained(shahrukhx01/question-vs-statement-classifier)\", \"performance\": {\"dataset\": \"Haystack\", \"accuracy\": \"Not provided\"}, \"description\": \"Trained to add the feature for classifying queries between Question Query vs Statement Query using classification in Haystack\"}}\nWrite a python program in 1 to 2 lines to call API in huggingface.\n\nThe answer should follow the format: {<<<domain>>> $DOMAIN, <<<api_call>>>: $API_CALL, <<<api_provider>>>: $API_PROVIDER, <<<explanation>>>: $EXPLANATION, <<<code>>>: $CODE}. Here are the requirements:\n1. The $API_CALL should have only 1 line of code that calls api.\n2. The $API_PROVIDER should be the programming framework used.\n3. $EXPLANATION should be a step-by-step explanation.\n4. The $CODE is the python code.\n5. Do not repeat the format in your answer.", "category": "generic"}
{"question_id": 126, "text": " I want to create a system that can answer questions by sorting out possible answers to a question.\n \n Use this API documentation for reference:  {\"domain\": \"Natural Language Processing Text Classification\", \"framework\": \"Hugging Face Transformers\", \"functionality\": \"Information Retrieval\", \"api_name\": \"cross-encoder/ms-marco-TinyBERT-L-2-v2\", \"api_call\": \"AutoModelForSequenceClassification.from_pretrained('cross-encoder/ms-marco-TinyBERT-L-2-v2')\", \"api_arguments\": {\"tokenizer\": \"tokenizer = AutoTokenizer.from_pretrained('model_name')\", \"features\": \"features = tokenizer(['How many people live in Berlin?', 'How many people live in Berlin?'], ['Berlin has a population of 3,520,031 registered inhabitants in an area of 891.82 square kilometers.', 'New York City is famous for the Metropolitan Museum of Art.'], padding=True, truncation=True, return_tensors='pt')\"}, \"python_environment_requirements\": [\"transformers\", \"torch\"], \"example_code\": {\"import\": \"from transformers import AutoTokenizer, AutoModelForSequenceClassification\", \"model\": \"model = AutoModelForSequenceClassification.from_pretrained('model_name')\", \"tokenizer\": \"tokenizer = AutoTokenizer.from_pretrained('model_name')\", \"features\": \"features = tokenizer(['How many people live in Berlin?', 'How many people live in Berlin?'], ['Berlin has a population of 3,520,031 registered inhabitants in an area of 891.82 square kilometers.', 'New York City is famous for the Metropolitan Museum of Art.'], padding=True, truncation=True, return_tensors='pt')\", \"scores\": \"with torch.no_grad():\n    scores = model(**features).logits\n    print(scores)\"}, \"performance\": {\"dataset\": \"TREC Deep Learning 2019\", \"accuracy\": \"69.84 (NDCG@10)\"}, \"description\": \"This model was trained on the MS Marco Passage Ranking task. It can be used for Information Retrieval: Given a query, encode the query with all possible passages (e.g. retrieved with ElasticSearch). Then sort the passages in a decreasing order. The training code is available here: SBERT.net Training MS Marco.\"}}\nWrite a python program in 1 to 2 lines to call API in huggingface.\n\nThe answer should follow the format: {<<<domain>>> $DOMAIN, <<<api_call>>>: $API_CALL, <<<api_provider>>>: $API_PROVIDER, <<<explanation>>>: $EXPLANATION, <<<code>>>: $CODE}. Here are the requirements:\n1. The $API_CALL should have only 1 line of code that calls api.\n2. The $API_PROVIDER should be the programming framework used.\n3. $EXPLANATION should be a step-by-step explanation.\n4. The $CODE is the python code.\n5. Do not repeat the format in your answer.", "category": "generic"}
{"question_id": 127, "text": " We have a news article and we need to extract all the entities like the names of people, organizations, and locations.\n \n Use this API documentation for reference:  {\"domain\": \"Natural Language Processing Token Classification\", \"framework\": \"Transformers\", \"functionality\": \"Named Entity Recognition\", \"api_name\": \"distilbert-base-multilingual-cased-ner-hrl\", \"api_call\": \"AutoModelForTokenClassification.from_pretrained('Davlan/distilbert-base-multilingual-cased-ner-hrl')\", \"api_arguments\": {\"model\": \"Davlan/distilbert-base-multilingual-cased-ner-hrl\", \"tokenizer\": \"Davlan/distilbert-base-multilingual-cased-ner-hrl\"}, \"python_environment_requirements\": [\"transformers\"], \"example_code\": \"from transformers import AutoTokenizer, AutoModelForTokenClassification\nfrom transformers import pipeline\ntokenizer = AutoTokenizer.from_pretrained(Davlan/distilbert-base-multilingual-cased-ner-hrl)\nmodel = AutoModelForTokenClassification.from_pretrained(Davlan/distilbert-base-multilingual-cased-ner-hrl)\nnlp = pipeline(ner, model=model, tokenizer=tokenizer)\nexample = Nader Jokhadar had given Syria the lead with a well-struck header in the seventh minute.\nner_results = nlp(example)\nprint(ner_results)\", \"performance\": {\"dataset\": [{\"name\": \"ANERcorp\", \"language\": \"Arabic\"}, {\"name\": \"conll 2003\", \"language\": \"German\"}, {\"name\": \"conll 2003\", \"language\": \"English\"}, {\"name\": \"conll 2002\", \"language\": \"Spanish\"}, {\"name\": \"Europeana Newspapers\", \"language\": \"French\"}, {\"name\": \"Italian I-CAB\", \"language\": \"Italian\"}, {\"name\": \"Latvian NER\", \"language\": \"Latvian\"}, {\"name\": \"conll 2002\", \"language\": \"Dutch\"}, {\"name\": \"Paramopama + Second Harem\", \"language\": \"Portuguese\"}, {\"name\": \"MSRA\", \"language\": \"Chinese\"}], \"accuracy\": \"Not specified\"}, \"description\": \"distilbert-base-multilingual-cased-ner-hrl is a Named Entity Recognition model for 10 high resourced languages (Arabic, German, English, Spanish, French, Italian, Latvian, Dutch, Portuguese and Chinese) based on a fine-tuned Distiled BERT base model. It has been trained to recognize three types of entities: location (LOC), organizations (ORG), and person (PER).\"}}\nWrite a python program in 1 to 2 lines to call API in huggingface.\n\nThe answer should follow the format: {<<<domain>>> $DOMAIN, <<<api_call>>>: $API_CALL, <<<api_provider>>>: $API_PROVIDER, <<<explanation>>>: $EXPLANATION, <<<code>>>: $CODE}. Here are the requirements:\n1. The $API_CALL should have only 1 line of code that calls api.\n2. The $API_PROVIDER should be the programming framework used.\n3. $EXPLANATION should be a step-by-step explanation.\n4. The $CODE is the python code.\n5. Do not repeat the format in your answer.", "category": "generic"}
{"question_id": 128, "text": " We are purchasing a CRM system to keep track of our customers and their organizations. We want to extract useful entities from customer emails automatically.\n \n Use this API documentation for reference:  {\"domain\": \"Natural Language Processing Token Classification\", \"framework\": \"Hugging Face Transformers\", \"functionality\": \"Named Entity Recognition\", \"api_name\": \"flair/ner-english-ontonotes\", \"api_call\": \"SequenceTagger.load('flair/ner-english-ontonotes')\", \"api_arguments\": [\"sentence\"], \"python_environment_requirements\": [\"flair\"], \"example_code\": \"from flair.data import Sentence\nfrom flair.models import SequenceTagger\ntagger = SequenceTagger.load(flair/ner-english-ontonotes)\nsentence = Sentence(On September 1st George Washington won 1 dollar.)\ntagger.predict(sentence)\nprint(sentence)\nfor entity in sentence.get_spans('ner'):\n    print(entity)\", \"performance\": {\"dataset\": \"Ontonotes\", \"accuracy\": \"89.27\"}, \"description\": \"This is the 18-class NER model for English that ships with Flair. It predicts 18 tags such as cardinal value, date value, event name, building name, geo-political entity, language name, law name, location name, money name, affiliation, ordinal value, organization name, percent value, person name, product name, quantity value, time value, and name of work of art. Based on Flair embeddings and LSTM-CRF.\"}}\nWrite a python program in 1 to 2 lines to call API in huggingface.\n\nThe answer should follow the format: {<<<domain>>> $DOMAIN, <<<api_call>>>: $API_CALL, <<<api_provider>>>: $API_PROVIDER, <<<explanation>>>: $EXPLANATION, <<<code>>>: $CODE}. Here are the requirements:\n1. The $API_CALL should have only 1 line of code that calls api.\n2. The $API_PROVIDER should be the programming framework used.\n3. $EXPLANATION should be a step-by-step explanation.\n4. The $CODE is the python code.\n5. Do not repeat the format in your answer.", "category": "generic"}
{"question_id": 129, "text": " As a researcher, I am trying to find an answer to my question in a table containing information about animals and their characteristics.\n \n Use this API documentation for reference:  {\"domain\": \"Natural Language Processing Table Question Answering\", \"framework\": \"Hugging Face Transformers\", \"functionality\": \"Transformers\", \"api_name\": \"neulab/omnitab-large-finetuned-wtq\", \"api_call\": \"AutoModelForSeq2SeqLM.from_pretrained('neulab/omnitab-large-finetuned-wtq')\", \"api_arguments\": {\"table\": \"pd.DataFrame\", \"query\": \"str\"}, \"python_environment_requirements\": {\"transformers\": \"AutoTokenizer, AutoModelForSeq2SeqLM\", \"pandas\": \"pd\"}, \"example_code\": \"from transformers import AutoTokenizer, AutoModelForSeq2SeqLM\nimport pandas as pd\ntokenizer = AutoTokenizer.from_pretrained(neulab/omnitab-large-finetuned-wtq)\nmodel = AutoModelForSeq2SeqLM.from_pretrained(neulab/omnitab-large-finetuned-wtq)\ndata = {\n year: [1896, 1900, 1904, 2004, 2008, 2012],\n city: [athens, paris, st. louis, athens, beijing, london]\n}\ntable = pd.DataFrame.from_dict(data)\nquery = In which year did beijing host the Olympic Games?\nencoding = tokenizer(table=table, query=query, return_tensors=pt)\noutputs = model.generate(**encoding)\nprint(tokenizer.batch_decode(outputs, skip_special_tokens=True))\", \"performance\": {\"dataset\": \"wikitablequestions\", \"accuracy\": null}, \"description\": \"OmniTab is a table-based QA model proposed in OmniTab: Pretraining with Natural and Synthetic Data for Few-shot Table-based Question Answering. The original Github repository is https://github.com/jzbjyb/OmniTab.\"}}\nWrite a python program in 1 to 2 lines to call API in huggingface.\n\nThe answer should follow the format: {<<<domain>>> $DOMAIN, <<<api_call>>>: $API_CALL, <<<api_provider>>>: $API_PROVIDER, <<<explanation>>>: $EXPLANATION, <<<code>>>: $CODE}. Here are the requirements:\n1. The $API_CALL should have only 1 line of code that calls api.\n2. The $API_PROVIDER should be the programming framework used.\n3. $EXPLANATION should be a step-by-step explanation.\n4. The $CODE is the python code.\n5. Do not repeat the format in your answer.", "category": "generic"}
{"question_id": 130, "text": " A teacher wants to create a quiz for her students. We are now working on the questions and answers for the quiz that be arranged in a table format.\n \n Use this API documentation for reference:  {\"domain\": \"Natural Language Processing Table Question Answering\", \"framework\": \"PyTorch Transformers\", \"functionality\": \"Table Question Answering\", \"api_name\": \"table-question-answering-tapas\", \"api_call\": \"pipeline('table-question-answering', model='Meena/table-question-answering-tapas')\", \"api_arguments\": [], \"python_environment_requirements\": [\"transformers\"], \"example_code\": \"This model can be loaded on the Inference API on-demand.\", \"performance\": {\"dataset\": [{\"name\": \"SQA (Sequential Question Answering by Microsoft)\", \"accuracy\": null}, {\"name\": \"WTQ (Wiki Table Questions by Stanford University)\", \"accuracy\": null}, {\"name\": \"WikiSQL (by Salesforce)\", \"accuracy\": null}]}, \"description\": \"TAPAS, the model learns an inner representation of the English language used in tables and associated texts, which can then be used to extract features useful for downstream tasks such as answering questions about a table, or determining whether a sentence is entailed or refuted by the contents of a table. It is a BERT-based model specifically designed (and pre-trained) for answering questions about tabular data. TAPAS uses relative position embeddings and has 7 token types that encode tabular structure. It is pre-trained on the masked language modeling (MLM) objective on a large dataset comprising millions of tables from English Wikipedia and corresponding texts.\"}}\nWrite a python program in 1 to 2 lines to call API in huggingface.\n\nThe answer should follow the format: {<<<domain>>> $DOMAIN, <<<api_call>>>: $API_CALL, <<<api_provider>>>: $API_PROVIDER, <<<explanation>>>: $EXPLANATION, <<<code>>>: $CODE}. Here are the requirements:\n1. The $API_CALL should have only 1 line of code that calls api.\n2. The $API_PROVIDER should be the programming framework used.\n3. $EXPLANATION should be a step-by-step explanation.\n4. The $CODE is the python code.\n5. Do not repeat the format in your answer.", "category": "generic"}
{"question_id": 131, "text": " I work for a financial company that stores all of its data in tables. We need a way to extract key information efficiently by asking natural language questions.\n \n Use this API documentation for reference:  {\"domain\": \"Natural Language Processing Table Question Answering\", \"framework\": \"PyTorch Transformers\", \"functionality\": \"Table-based QA\", \"api_name\": \"neulab/omnitab-large-1024shot\", \"api_call\": \"AutoModelForSeq2SeqLM.from_pretrained('neulab/omnitab-large-1024shot')\", \"api_arguments\": {\"table\": \"pd.DataFrame.from_dict(data)\", \"query\": \"str\"}, \"python_environment_requirements\": [\"transformers\", \"pandas\"], \"example_code\": \"from transformers import AutoTokenizer, AutoModelForSeq2SeqLM\nimport pandas as pd\ntokenizer = AutoTokenizer.from_pretrained(neulab/omnitab-large-1024shot)\nmodel = AutoModelForSeq2SeqLM.from_pretrained(neulab/omnitab-large-1024shot)\ndata = {\n year: [1896, 1900, 1904, 2004, 2008, 2012],\n city: [athens, paris, st. louis, athens, beijing, london]\n}\ntable = pd.DataFrame.from_dict(data)\nquery = In which year did beijing host the Olympic Games?\nencoding = tokenizer(table=table, query=query, return_tensors=pt)\noutputs = model.generate(**encoding)\nprint(tokenizer.batch_decode(outputs, skip_special_tokens=True))\", \"performance\": {\"dataset\": \"wikitablequestions\", \"accuracy\": \"Not provided\"}, \"description\": \"OmniTab is a table-based QA model proposed in OmniTab: Pretraining with Natural and Synthetic Data for Few-shot Table-based Question Answering. neulab/omnitab-large-1024shot (based on BART architecture) is initialized with microsoft/tapex-large and continuously pretrained on natural and synthetic data (SQL2NL model trained in the 1024-shot setting).\"}}\nWrite a python program in 1 to 2 lines to call API in huggingface.\n\nThe answer should follow the format: {<<<domain>>> $DOMAIN, <<<api_call>>>: $API_CALL, <<<api_provider>>>: $API_PROVIDER, <<<explanation>>>: $EXPLANATION, <<<code>>>: $CODE}. Here are the requirements:\n1. The $API_CALL should have only 1 line of code that calls api.\n2. The $API_PROVIDER should be the programming framework used.\n3. $EXPLANATION should be a step-by-step explanation.\n4. The $CODE is the python code.\n5. Do not repeat the format in your answer.", "category": "generic"}
{"question_id": 132, "text": " We have a dataset related to coffee and tea prices. We need to answer a question on who sells hot chocolate and their prices.\n###Input: {\\\"table\\\": [[\\\"Shop\\\", \\\"Drink\\\", \\\"Price\\\"], [\\\"Cafe A\\\", \\\"Coffee\\\", \\\"3.00\\\"], [\\\"Cafe B\\\", \\\"Tea\\\", \\\"2.50\\\"], [\\\"Cafe C\\\", \\\"Hot Chocolate\\\", \\\"4.50\\\"], [\\\"Cafe D\\\", \\\"Hot Chocolate\\\", \\\"3.75\\\"]], \\\"queries\\\": [\\\"Which shops sell hot chocolate and what are their prices?\\\"]}\n \n Use this API documentation for reference:  {\"domain\": \"Natural Language Processing Table Question Answering\", \"framework\": \"Transformers\", \"functionality\": \"Table Question Answering\", \"api_name\": \"google/tapas-mini-finetuned-sqa\", \"api_call\": \"TapasForQuestionAnswering.from_pretrained('google/tapas-mini-finetuned-sqa')\", \"api_arguments\": [\"model_name\", \"table\", \"queries\"], \"python_environment_requirements\": [\"transformers\"], \"example_code\": \"N/A\", \"performance\": {\"dataset\": \"msr_sqa\", \"accuracy\": 0.5148}, \"description\": \"TAPAS mini model fine-tuned on Sequential Question Answering (SQA)\"}}\nWrite a python program in 1 to 2 lines to call API in huggingface.\n\nThe answer should follow the format: {<<<domain>>> $DOMAIN, <<<api_call>>>: $API_CALL, <<<api_provider>>>: $API_PROVIDER, <<<explanation>>>: $EXPLANATION, <<<code>>>: $CODE}. Here are the requirements:\n1. The $API_CALL should have only 1 line of code that calls api.\n2. The $API_PROVIDER should be the programming framework used.\n3. $EXPLANATION should be a step-by-step explanation.\n4. The $CODE is the python code.\n5. Do not repeat the format in your answer.", "category": "generic"}
{"question_id": 133, "text": " A company is running a survey and they want to know how many respondents have given a specific answer for each question of the survey.\n \n Use this API documentation for reference:  {\"domain\": \"Natural Language Processing Table Question Answering\", \"framework\": \"Hugging Face Transformers\", \"functionality\": \"Table Question Answering\", \"api_name\": \"navteca/tapas-large-finetuned-wtq\", \"api_call\": \"AutoModelForTableQuestionAnswering.from_pretrained('navteca/tapas-large-finetuned-wtq')\", \"api_arguments\": {\"table\": \"table_data\", \"query\": \"query\"}, \"python_environment_requirements\": {\"transformers\": \"AutoModelForTableQuestionAnswering, AutoTokenizer, pipeline\"}, \"example_code\": \"from transformers import AutoModelForTableQuestionAnswering, AutoTokenizer, pipeline\n# Load model & tokenizer\ntapas_model = AutoModelForTableQuestionAnswering.from_pretrained('navteca/tapas-large-finetuned-wtq')\ntapas_tokenizer = AutoTokenizer.from_pretrained('navteca/tapas-large-finetuned-wtq')\n# Get predictions\nnlp = pipeline('table-question-answering', model=tapas_model, tokenizer=tapas_tokenizer)\nresult = nlp({'table': {'Repository': ['Transformers', 'Datasets', 'Tokenizers'], 'Stars': ['36542', '4512', '3934'], 'Contributors': ['651', '77', '34'], 'Programming language': ['Python', 'Python', 'Rust, Python and NodeJS']}, 'query': 'How many stars does the transformers repository have?'})\nprint(result)\", \"performance\": {\"dataset\": \"wikisql\", \"accuracy\": \"Not provided\"}, \"description\": \"TAPAS large model fine-tuned on WikiTable Questions (WTQ). It is a BERT-like transformers model pretrained on a large corpus of English data from Wikipedia in a self-supervised fashion. It can be used for answering questions related to a table.\"}}\nWrite a python program in 1 to 2 lines to call API in huggingface.\n\nThe answer should follow the format: {<<<domain>>> $DOMAIN, <<<api_call>>>: $API_CALL, <<<api_provider>>>: $API_PROVIDER, <<<explanation>>>: $EXPLANATION, <<<code>>>: $CODE}. Here are the requirements:\n1. The $API_CALL should have only 1 line of code that calls api.\n2. The $API_PROVIDER should be the programming framework used.\n3. $EXPLANATION should be a step-by-step explanation.\n4. The $CODE is the python code.\n5. Do not repeat the format in your answer.", "category": "generic"}
{"question_id": 134, "text": " Extract information about a non-compete clause from a legal document with a context related to data protection.\n###Input: \\\"The data protection provisions set forth in this agreement shall be in effect for a period of 2 years after the termination of services. The non-compete clause states that the service provider is prohibited from providing similar services to any competitor within a 50-mile radius and during the 1-year period following termination of services.\\\"\n \n Use this API documentation for reference:  {\"domain\": \"Natural Language Processing Question Answering\", \"framework\": \"Hugging Face Transformers\", \"functionality\": \"Question Answering\", \"api_name\": \"Rakib/roberta-base-on-cuad\", \"api_call\": \"AutoModelForQuestionAnswering.from_pretrained('Rakib/roberta-base-on-cuad')\", \"api_arguments\": {\"tokenizer\": \"AutoTokenizer.from_pretrained(Rakib/roberta-base-on-cuad)\"}, \"python_environment_requirements\": {\"transformers\": \"latest\"}, \"example_code\": {\"import\": \"from transformers import AutoTokenizer, AutoModelForQuestionAnswering\", \"tokenizer\": \"tokenizer = AutoTokenizer.from_pretrained(Rakib/roberta-base-on-cuad)\", \"model\": \"model = AutoModelForQuestionAnswering.from_pretrained(Rakib/roberta-base-on-cuad)\"}, \"performance\": {\"dataset\": \"cuad\", \"accuracy\": \"46.6%\"}, \"description\": \"This model is trained for the task of Question Answering on Legal Documents using the CUAD dataset. It is based on the RoBERTa architecture and can be used to extract answers from legal contracts and documents.\"}}\nWrite a python program in 1 to 2 lines to call API in huggingface.\n\nThe answer should follow the format: {<<<domain>>> $DOMAIN, <<<api_call>>>: $API_CALL, <<<api_provider>>>: $API_PROVIDER, <<<explanation>>>: $EXPLANATION, <<<code>>>: $CODE}. Here are the requirements:\n1. The $API_CALL should have only 1 line of code that calls api.\n2. The $API_PROVIDER should be the programming framework used.\n3. $EXPLANATION should be a step-by-step explanation.\n4. The $CODE is the python code.\n5. Do not repeat the format in your answer.", "category": "generic"}
{"question_id": 135, "text": " Tell me the day of the game when it was played given the following context: \\\"The game was played on February 7, 2016 at Levi's Stadium in the San Francisco Bay Area at Santa Clara, California.\\\"\n###Input: {'context': \\\"The game was played on February 7, 2016 at Levi's Stadium in the San Francisco Bay Area at Santa Clara, California.\\\", 'question': \\\"What day was the game played on?\\\"}\n \n Use this API documentation for reference:  {\"domain\": \"Natural Language Processing Question Answering\", \"framework\": \"Hugging Face Transformers\", \"functionality\": \"Question Answering\", \"api_name\": \"csarron/bert-base-uncased-squad-v1\", \"api_call\": \"pipeline('question-answering', model='csarron/bert-base-uncased-squad-v1', tokenizer='csarron/bert-base-uncased-squad-v1')\", \"api_arguments\": {\"model\": \"csarron/bert-base-uncased-squad-v1\", \"tokenizer\": \"csarron/bert-base-uncased-squad-v1\"}, \"python_environment_requirements\": \"Python 3.7.5\", \"example_code\": \"from transformers import pipeline\nqa_pipeline = pipeline(\n question-answering,\n model=csarron/bert-base-uncased-squad-v1,\n tokenizer=csarron/bert-base-uncased-squad-v1\n)\npredictions = qa_pipeline({\n 'context': The game was played on February 7, 2016 at Levi's Stadium in the San Francisco Bay Area at Santa Clara, California.,\n 'question': What day was the game played on?\n})\nprint(predictions)\", \"performance\": {\"dataset\": \"SQuAD1.1\", \"accuracy\": {\"EM\": 80.9, \"F1\": 88.2}}, \"description\": \"BERT-base uncased model fine-tuned on SQuAD v1. This model is case-insensitive and does not make a difference between english and English.\"}}\nWrite a python program in 1 to 2 lines to call API in huggingface.\n\nThe answer should follow the format: {<<<domain>>> $DOMAIN, <<<api_call>>>: $API_CALL, <<<api_provider>>>: $API_PROVIDER, <<<explanation>>>: $EXPLANATION, <<<code>>>: $CODE}. Here are the requirements:\n1. The $API_CALL should have only 1 line of code that calls api.\n2. The $API_PROVIDER should be the programming framework used.\n3. $EXPLANATION should be a step-by-step explanation.\n4. The $CODE is the python code.\n5. Do not repeat the format in your answer.", "category": "generic"}
{"question_id": 136, "text": " We need to identify the relationship between two sentences whether they are contradictory, entail each other, or neutral.\n \n Use this API documentation for reference:  {\"domain\": \"Natural Language Processing Zero-Shot Classification\", \"framework\": \"Hugging Face Transformers\", \"functionality\": \"Zero-Shot Classification\", \"api_name\": \"cross-encoder/nli-deberta-v3-small\", \"api_call\": \"CrossEncoder('cross-encoder/nli-deberta-v3-small')\", \"api_arguments\": [\"sentence1\", \"sentence2\"], \"python_environment_requirements\": [\"sentence_transformers\", \"transformers\"], \"example_code\": \"from sentence_transformers import CrossEncoder\nmodel = CrossEncoder('cross-encoder/nli-deberta-v3-small')\nscores = model.predict([('A man is eating pizza', 'A man eats something'), ('A black race car starts up in front of a crowd of people.', 'A man is driving down a lonely road.')])\", \"performance\": {\"dataset\": {\"SNLI-test\": \"91.65\", \"MNLI-mismatched\": \"87.55\"}, \"accuracy\": {\"SNLI-test\": \"91.65\", \"MNLI-mismatched\": \"87.55\"}}, \"description\": \"Cross-Encoder for Natural Language Inference based on microsoft/deberta-v3-small, trained on the SNLI and MultiNLI datasets. For a given sentence pair, it will output three scores corresponding to the labels: contradiction, entailment, neutral.\"}}\nWrite a python program in 1 to 2 lines to call API in huggingface.\n\nThe answer should follow the format: {<<<domain>>> $DOMAIN, <<<api_call>>>: $API_CALL, <<<api_provider>>>: $API_PROVIDER, <<<explanation>>>: $EXPLANATION, <<<code>>>: $CODE}. Here are the requirements:\n1. The $API_CALL should have only 1 line of code that calls api.\n2. The $API_PROVIDER should be the programming framework used.\n3. $EXPLANATION should be a step-by-step explanation.\n4. The $CODE is the python code.\n5. Do not repeat the format in your answer.", "category": "generic"}
{"question_id": 137, "text": " I am a climate change agency, looking to have my research summaries translated into Chinese for international audiences.\n \n Use this API documentation for reference:  {\"domain\": \"Natural Language Processing Text2Text Generation\", \"framework\": \"Hugging Face Transformers\", \"functionality\": [\"Translation\", \"Summarization\", \"Question Answering\", \"Text Classification\", \"Text Regression\"], \"api_name\": \"t5-small\", \"api_call\": \"T5Model.from_pretrained('t5-small')\", \"api_arguments\": {\"input_ids\": \"input tokenized text\", \"decoder_input_ids\": \"input tokenized text for decoder\"}, \"python_environment_requirements\": [\"transformers\"], \"example_code\": \"from transformers import T5Tokenizer, T5Model\ntokenizer = T5Tokenizer.from_pretrained('t5-small')\nmodel = T5Model.from_pretrained('t5-small')\ninput_ids = tokenizer('Studies have been shown that owning a dog is good for you', return_tensors='pt').input_ids\ndecoder_input_ids = tokenizer('Studies show that', return_tensors='pt').input_ids\noutputs = model(input_ids=input_ids, decoder_input_ids=decoder_input_ids)\nlast_hidden_states = outputs.last_hidden_state\", \"performance\": {\"dataset\": \"c4\", \"accuracy\": \"See research paper, Table 14 for full results\"}, \"description\": \"T5-Small is a Text-To-Text Transfer Transformer (T5) model with 60 million parameters. It is designed to perform a variety of NLP tasks, including machine translation, document summarization, question answering, and classification tasks. The model is pre-trained on the Colossal Clean Crawled Corpus (C4) and can be fine-tuned for specific tasks.\"}}\nWrite a python program in 1 to 2 lines to call API in huggingface.\n\nThe answer should follow the format: {<<<domain>>> $DOMAIN, <<<api_call>>>: $API_CALL, <<<api_provider>>>: $API_PROVIDER, <<<explanation>>>: $EXPLANATION, <<<code>>>: $CODE}. Here are the requirements:\n1. The $API_CALL should have only 1 line of code that calls api.\n2. The $API_PROVIDER should be the programming framework used.\n3. $EXPLANATION should be a step-by-step explanation.\n4. The $CODE is the python code.\n5. Do not repeat the format in your answer.", "category": "generic"}
{"question_id": 138, "text": " We are building an app to summarize long articles for users. We need a solution to create a condensed summary of the given text.\n \n Use this API documentation for reference:  {\"domain\": \"Natural Language Processing Summarization\", \"framework\": \"Transformers\", \"functionality\": \"text2text-generation\", \"api_name\": \"sshleifer/distilbart-cnn-12-6\", \"api_call\": \"BartForConditionalGeneration.from_pretrained('sshleifer/distilbart-cnn-12-6')\", \"api_arguments\": \"\", \"python_environment_requirements\": \"huggingface/transformers\", \"example_code\": \"\", \"performance\": {\"dataset\": [{\"name\": \"cnn_dailymail\", \"accuracy\": {\"Rouge 2\": \"22.12\", \"Rouge-L\": \"36.99\"}}]}, \"description\": \"DistilBART is a distilled version of BART, a model for text summarization. This specific checkpoint, 'sshleifer/distilbart-cnn-12-6', is trained on the cnn_dailymail dataset and provides a fast and effective way to generate summaries of text. The model can be loaded using the Hugging Face Transformers library.\"}}\nWrite a python program in 1 to 2 lines to call API in huggingface.\n\nThe answer should follow the format: {<<<domain>>> $DOMAIN, <<<api_call>>>: $API_CALL, <<<api_provider>>>: $API_PROVIDER, <<<explanation>>>: $EXPLANATION, <<<code>>>: $CODE}. Here are the requirements:\n1. The $API_CALL should have only 1 line of code that calls api.\n2. The $API_PROVIDER should be the programming framework used.\n3. $EXPLANATION should be a step-by-step explanation.\n4. The $CODE is the python code.\n5. Do not repeat the format in your answer.", "category": "generic"}
{"question_id": 139, "text": " A news agency wants to summaries their international news articles daily.\n \n Use this API documentation for reference:  {\"domain\": \"Natural Language Processing Summarization\", \"framework\": \"Transformers\", \"functionality\": \"text2text-generation\", \"api_name\": \"csebuetnlp/mT5_multilingual_XLSum\", \"api_call\": \"AutoModelForSeq2SeqLM.from_pretrained('csebuetnlp/mT5_multilingual_XLSum')\", \"api_arguments\": [\"model_name\"], \"python_environment_requirements\": [\"transformers==4.11.0.dev0\"], \"example_code\": \"import re\nfrom transformers import AutoTokenizer, AutoModelForSeq2SeqLM\nWHITESPACE_HANDLER = lambda k: re.sub('\\\\s+', ' ', re.sub('\\\n+', ' ', k.strip()))\narticle_text = Videos that say approved vaccines are dangerous and cause autism, cancer or infertility are among those that will be taken down, the company said. The policy includes the termination of accounts of anti-vaccine influencers. Tech giants have been criticised for not doing more to counter false health information on their sites. In July, US President Joe Biden said social media platforms were largely responsible for people's scepticism in getting vaccinated by spreading misinformation, and appealed for them to address the issue. YouTube, which is owned by Google, said 130,000 videos were removed from its platform since last year, when it implemented a ban on content spreading misinformation about Covid vaccines. In a blog post, the company said it had seen false claims about Covid jabs spill over into misinformation about vaccines in general. The new policy covers long-approved vaccines, such as those against measles or hepatitis B. We're expanding our medical misinformation policies on YouTube with new guidelines on currently administered vaccines that are approved and confirmed to be safe and effective by local health authorities and the WHO, the post said, referring to the World Health Organization.\nmodel_name = csebuetnlp/mT5_multilingual_XLSum\ntokenizer = AutoTokenizer.from_pretrained(model_name)\nmodel = AutoModelForSeq2SeqLM.from_pretrained(model_name)\ninput_ids = tokenizer(\n [WHITESPACE_HANDLER(article_text)],\n return_tensors=pt,\n padding=max_length,\n truncation=True,\n max_length=512\n)[input_ids]\noutput_ids = model.generate(\n input_ids=input_ids,\n max_length=84,\n no_repeat_ngram_size=2,\n num_beams=4\n)[0]\nsummary = tokenizer.decode(\n output_ids,\n skip_special_tokens=True,\n clean_up_tokenization_spaces=False\n)\nprint(summary)\", \"performance\": {\"dataset\": \"xsum\", \"accuracy\": {\"ROUGE-1\": 36.5, \"ROUGE-2\": 13.934, \"ROUGE-L\": 28.988, \"ROUGE-LSUM\": 28.996, \"loss\": 2.067, \"gen_len\": 26.973}}, \"description\": \"This repository contains the mT5 checkpoint finetuned on the 45 languages of XL-Sum dataset. It is a multilingual abstractive summarization model that supports text-to-text generation for 43 languages.\"}}\nWrite a python program in 1 to 2 lines to call API in huggingface.\n\nThe answer should follow the format: {<<<domain>>> $DOMAIN, <<<api_call>>>: $API_CALL, <<<api_provider>>>: $API_PROVIDER, <<<explanation>>>: $EXPLANATION, <<<code>>>: $CODE}. Here are the requirements:\n1. The $API_CALL should have only 1 line of code that calls api.\n2. The $API_PROVIDER should be the programming framework used.\n3. $EXPLANATION should be a step-by-step explanation.\n4. The $CODE is the python code.\n5. Do not repeat the format in your answer.", "category": "generic"}
{"question_id": 140, "text": " Design a smart home system that can have conversations with the user for controlling the home appliances.\n \n Use this API documentation for reference:  {\"domain\": \"Natural Language Processing Conversational\", \"framework\": \"Hugging Face Transformers\", \"functionality\": \"Transformers\", \"api_name\": \"facebook/blenderbot-90M\", \"api_call\": \"AutoModelForCausalLM.from_pretrained('facebook/blenderbot-90M')\", \"api_arguments\": {\"input_message\": \"str\"}, \"python_environment_requirements\": \"transformers\", \"example_code\": \"from transformers import AutoModelForCausalLM, AutoTokenizer\ntokenizer = AutoTokenizer.from_pretrained('facebook/blenderbot-90M')\nmodel = AutoModelForCausalLM.from_pretrained('facebook/blenderbot-90M')\n# Chat with the model\ninput_message = 'What is your favorite color?'\ntokenized_input = tokenizer.encode(input_message + tokenizer.eos_token, return_tensors='pt')\noutput = model.generate(tokenized_input, max_length=1000, pad_token_id=tokenizer.eos_token_id)\nresponse = tokenizer.decode(output[:, tokenized_input.shape[-1]:][0], skip_special_tokens=True)\nprint(response)\", \"performance\": {\"dataset\": \"blended_skill_talk\", \"accuracy\": \"Not provided\"}, \"description\": \"BlenderBot-90M is a conversational AI model developed by Facebook AI. It is trained on the Blended Skill Talk dataset and aims to provide engaging and human-like responses in a multi-turn dialogue setting. The model is deprecated, and it is recommended to use the identical model https://huggingface.co/facebook/blenderbot_small-90M instead.\"}}\nWrite a python program in 1 to 2 lines to call API in huggingface.\n\nThe answer should follow the format: {<<<domain>>> $DOMAIN, <<<api_call>>>: $API_CALL, <<<api_provider>>>: $API_PROVIDER, <<<explanation>>>: $EXPLANATION, <<<code>>>: $CODE}. Here are the requirements:\n1. The $API_CALL should have only 1 line of code that calls api.\n2. The $API_PROVIDER should be the programming framework used.\n3. $EXPLANATION should be a step-by-step explanation.\n4. The $CODE is the python code.\n5. Do not repeat the format in your answer.", "category": "generic"}
{"question_id": 141, "text": " An educational publishing company is developing a language comprehension program for elementary school students. They want a system that can generate a short story based on a given prompt.\n \n Use this API documentation for reference:  {\"domain\": \"Natural Language Processing Text Generation\", \"framework\": \"PyTorch Transformers\", \"functionality\": \"Text Generation\", \"api_name\": \"decapoda-research/llama-7b-hf\", \"api_call\": \"AutoModel.from_pretrained('decapoda-research/llama-7b-hf')\", \"api_arguments\": \"\", \"python_environment_requirements\": \"transformers\", \"example_code\": \"from transformers import pipeline\ngen = pipeline('text-generation', model='decapoda-research/llama-7b-hf')\nresult = gen('Once upon a time')\nprint(result[0]['generated_text'])\", \"performance\": {\"dataset\": [{\"name\": \"BoolQ\", \"accuracy\": 76.5}, {\"name\": \"PIQA\", \"accuracy\": 79.8}, {\"name\": \"SIQA\", \"accuracy\": 48.9}, {\"name\": \"HellaSwag\", \"accuracy\": 76.1}, {\"name\": \"WinoGrande\", \"accuracy\": 70.1}, {\"name\": \"ARC-e\", \"accuracy\": 76.7}, {\"name\": \"ARC-c\", \"accuracy\": 47.6}, {\"name\": \"OBQAC\", \"accuracy\": 57.2}, {\"name\": \"COPA\", \"accuracy\": 93}]}, \"description\": \"LLaMA-7B is an auto-regressive language model based on the transformer architecture. It is designed for research on large language models, including question answering, natural language understanding, and reading comprehension. The model is trained on various sources, including CCNet, C4, GitHub, Wikipedia, Books, ArXiv, and Stack Exchange, with the majority of the dataset being in English.\"}}\nWrite a python program in 1 to 2 lines to call API in huggingface.\n\nThe answer should follow the format: {<<<domain>>> $DOMAIN, <<<api_call>>>: $API_CALL, <<<api_provider>>>: $API_PROVIDER, <<<explanation>>>: $EXPLANATION, <<<code>>>: $CODE}. Here are the requirements:\n1. The $API_CALL should have only 1 line of code that calls api.\n2. The $API_PROVIDER should be the programming framework used.\n3. $EXPLANATION should be a step-by-step explanation.\n4. The $CODE is the python code.\n5. Do not repeat the format in your answer.", "category": "generic"}
{"question_id": 142, "text": " Can you initiate a conversation with an AI model that plays the role of a friend who just got back from a conference?\n \n Use this API documentation for reference:  {\"domain\": \"Natural Language Processing Text2Text Generation\", \"framework\": \"Hugging Face Transformers\", \"functionality\": \"Conversational\", \"api_name\": \"allenai/cosmo-xl\", \"api_call\": \"AutoModelForSeq2SeqLM.from_pretrained('allenai/cosmo-xl')\", \"api_arguments\": {\"pretrained_model_name_or_path\": \"allenai/cosmo-xl\"}, \"python_environment_requirements\": {\"torch\": \"latest\", \"transformers\": \"latest\"}, \"example_code\": {\"import\": [\"import torch\", \"from transformers import AutoTokenizer, AutoModelForSeq2SeqLM\"], \"initialize\": [\"device = torch.device(cuda if torch.cuda.is_available() else cpu)\", \"tokenizer = AutoTokenizer.from_pretrained(allenai/cosmo-xl)\", \"model = AutoModelForSeq2SeqLM.from_pretrained(allenai/cosmo-xl).to(device)\"], \"example\": [\"def set_input(situation_narrative, role_instruction, conversation_history):\", \" input_text =  <turn> .join(conversation_history)\", \"if role_instruction != :\", \" input_text = {} &lt;sep&gt; {}'.format(role_instruction, input_text)\", \"if situation_narrative != :\", \" input_text = {} &lt;sep&gt; {}'.format(situation_narrative, input_text)\", \"return input_text\", \"def generate(situation_narrative, role_instruction, conversation_history):\", \" input_text = set_input(situation_narrative, role_instruction, conversation_history)\", \" inputs = tokenizer([input_text], return_tensors=pt).to(device)\", \" outputs = model.generate(inputs[input_ids], max_new_tokens=128, temperature=1.0, top_p=.95, do_sample=True)\", \" response = tokenizer.decode(outputs[0], skip_special_tokens=True, clean_up_tokenization_spaces=False)\", \" return response\", \"situation = Cosmo had a really fun time participating in the EMNLP conference at Abu Dhabi.\", \"instruction = You are Cosmo and you are talking to a friend.\", \"conversation = [\", \" Hey, how was your trip to Abu Dhabi?\", \"]\", \"response = generate(situation, instruction, conversation)\", \"print(response)\"]}, \"performance\": {\"dataset\": {\"allenai/soda\": \"\", \"allenai/prosocial-dialog\": \"\"}, \"accuracy\": \"\"}, \"description\": \"COSMO is a conversation agent with greater generalizability on both in- and out-of-domain chitchat datasets (e.g., DailyDialog, BlendedSkillTalk). It is trained on two datasets: SODA and ProsocialDialog. COSMO is especially aiming to model natural human conversations. It can accept situation descriptions as well as instructions on what role it should play in the situation.\"}}\nWrite a python program in 1 to 2 lines to call API in huggingface.\n\nThe answer should follow the format: {<<<domain>>> $DOMAIN, <<<api_call>>>: $API_CALL, <<<api_provider>>>: $API_PROVIDER, <<<explanation>>>: $EXPLANATION, <<<code>>>: $CODE}. Here are the requirements:\n1. The $API_CALL should have only 1 line of code that calls api.\n2. The $API_PROVIDER should be the programming framework used.\n3. $EXPLANATION should be a step-by-step explanation.\n4. The $CODE is the python code.\n5. Do not repeat the format in your answer.", "category": "generic"}
{"question_id": 143, "text": " As a software developer, I want a program that can automatically generate code snippets for me based on natural language descriptions.\n \n Use this API documentation for reference:  {\"domain\": \"Natural Language Processing Text Generation\", \"framework\": \"Hugging Face Transformers\", \"functionality\": \"Program Synthesis\", \"api_name\": \"Salesforce/codegen-2B-multi\", \"api_call\": \"AutoModelForCausalLM.from_pretrained('Salesforce/codegen-2B-multi')\", \"api_arguments\": {\"input_ids\": \"input_ids\", \"max_length\": 128}, \"python_environment_requirements\": {\"transformers\": \"AutoTokenizer, AutoModelForCausalLM\"}, \"example_code\": \"from transformers import AutoTokenizer, AutoModelForCausalLM\ntokenizer = AutoTokenizer.from_pretrained(Salesforce/codegen-2B-multi)\nmodel = AutoModelForCausalLM.from_pretrained(Salesforce/codegen-2B-multi)\ntext = def hello_world():\ninput_ids = tokenizer(text, return_tensors=pt).input_ids\ngenerated_ids = model.generate(input_ids, max_length=128)\nprint(tokenizer.decode(generated_ids[0], skip_special_tokens=True))\", \"performance\": {\"dataset\": \"HumanEval, MTPB\"}, \"description\": \"CodeGen is a family of autoregressive language models for program synthesis. The models are originally released in this repository, under 3 pre-training data variants (NL, Multi, Mono) and 4 model size variants (350M, 2B, 6B, 16B). The checkpoint included in this repository is denoted as CodeGen-Multi 2B, where Multi means the model is initialized with CodeGen-NL 2B and further pre-trained on a dataset of multiple programming languages, and 2B refers to the number of trainable parameters.\"}}\nWrite a python program in 1 to 2 lines to call API in huggingface.\n\nThe answer should follow the format: {<<<domain>>> $DOMAIN, <<<api_call>>>: $API_CALL, <<<api_provider>>>: $API_PROVIDER, <<<explanation>>>: $EXPLANATION, <<<code>>>: $CODE}. Here are the requirements:\n1. The $API_CALL should have only 1 line of code that calls api.\n2. The $API_PROVIDER should be the programming framework used.\n3. $EXPLANATION should be a step-by-step explanation.\n4. The $CODE is the python code.\n5. Do not repeat the format in your answer.", "category": "generic"}
{"question_id": 144, "text": " Provide a short summary of an article about cryptocurrency investment risks.\n###Input: Cryptocurrencies have become exceedingly popular among investors seeking higher returns and diversification in their portfolios. However, investing in these digital currencies carries several inherent risks. Market volatility is a major factor \\u2013 cryptocurrencies can experience wild price swings, sometimes even within hours or minutes. This high volatility makes it difficult to predict the future value of the investments and can result in significant losses. Furthermore, the lack of regulatory oversight and security concerns may also lead to potential frauds and hacks, exposing investors to additional risk. Lastly, the environmental impact of mining digital currencies like Bitcoin has come under scrutiny, questioning the long-term sustainability of the cryptocurrency market.\n \n Use this API documentation for reference:  {\"domain\": \"Natural Language Processing Text Generation\", \"framework\": \"Hugging Face Transformers\", \"functionality\": \"Text Generation\", \"api_name\": \"decapoda-research/llama-13b-hf\", \"api_call\": \"pipeline('text-generation', model='decapoda-research/llama-13b-hf')\", \"api_arguments\": \"text\", \"python_environment_requirements\": \"transformers\", \"example_code\": \"generator('Once upon a time')\", \"performance\": {\"dataset\": [{\"name\": \"BoolQ\", \"accuracy\": \"85.3\"}, {\"name\": \"PIQA\", \"accuracy\": \"82.8\"}, {\"name\": \"SIQA\", \"accuracy\": \"52.3\"}, {\"name\": \"HellaSwag\", \"accuracy\": \"84.2\"}, {\"name\": \"WinoGrande\", \"accuracy\": \"77\"}, {\"name\": \"ARC-e\", \"accuracy\": \"81.5\"}, {\"name\": \"ARC-c\", \"accuracy\": \"56\"}, {\"name\": \"OBQACOPA\", \"accuracy\": \"60.2\"}]}, \"description\": \"LLaMA-13B is an auto-regressive language model based on the transformer architecture developed by the FAIR team of Meta AI. It is designed for research purposes, such as question answering, natural language understanding, and reading comprehension. The model has been trained on a variety of sources, including web data, GitHub, Wikipedia, and books in 20 languages. It has been evaluated on several benchmarks, including BoolQ, PIQA, SIQA, HellaSwag, WinoGrande, ARC, and OpenBookQA.\"}}\nWrite a python program in 1 to 2 lines to call API in huggingface.\n\nThe answer should follow the format: {<<<domain>>> $DOMAIN, <<<api_call>>>: $API_CALL, <<<api_provider>>>: $API_PROVIDER, <<<explanation>>>: $EXPLANATION, <<<code>>>: $CODE}. Here are the requirements:\n1. The $API_CALL should have only 1 line of code that calls api.\n2. The $API_PROVIDER should be the programming framework used.\n3. $EXPLANATION should be a step-by-step explanation.\n4. The $CODE is the python code.\n5. Do not repeat the format in your answer.", "category": "generic"}
{"question_id": 145, "text": " You have just met a person that speaks French. As a hotel manager, you need to tell them, \\\"Welcome to our hotel, we hope you enjoy your stay.\\\" in French.\n \n Use this API documentation for reference:  {\"domain\": \"Natural Language Processing Text2Text Generation\", \"framework\": \"Hugging Face Transformers\", \"functionality\": \"Multilingual Translation\", \"api_name\": \"facebook/m2m100_418M\", \"api_call\": \"M2M100ForConditionalGeneration.from_pretrained('facebook/m2m100_418M')\", \"api_arguments\": {\"encoded_input\": \"Encoded input text\", \"target_lang\": \"Target language code\"}, \"python_environment_requirements\": [\"transformers\", \"sentencepiece\"], \"example_code\": [\"from transformers import M2M100ForConditionalGeneration, M2M100Tokenizer\", \"hi_text = \", \"chinese_text = \", \"model = M2M100ForConditionalGeneration.from_pretrained(facebook/m2m100_418M)\", \"tokenizer = M2M100Tokenizer.from_pretrained(facebook/m2m100_418M)\", \"tokenizer.src_lang = hi\", \"encoded_hi = tokenizer(hi_text, return_tensors=pt)\", \"generated_tokens = model.generate(**encoded_hi, forced_bos_token_id=tokenizer.get_lang_id(fr))\", \"tokenizer.batch_decode(generated_tokens, skip_special_tokens=True)\"], \"performance\": {\"dataset\": \"WMT\", \"accuracy\": \"Not provided\"}, \"description\": \"M2M100 is a multilingual encoder-decoder (seq-to-seq) model trained for Many-to-Many multilingual translation. It can directly translate between the 9,900 directions of 100 languages. To translate into a target language, the target language id is forced as the first generated token.\"}}\nWrite a python program in 1 to 2 lines to call API in huggingface.\n\nThe answer should follow the format: {<<<domain>>> $DOMAIN, <<<api_call>>>: $API_CALL, <<<api_provider>>>: $API_PROVIDER, <<<explanation>>>: $EXPLANATION, <<<code>>>: $CODE}. Here are the requirements:\n1. The $API_CALL should have only 1 line of code that calls api.\n2. The $API_PROVIDER should be the programming framework used.\n3. $EXPLANATION should be a step-by-step explanation.\n4. The $CODE is the python code.\n5. Do not repeat the format in your answer.", "category": "generic"}
{"question_id": 146, "text": " They are planning a trip to Germany and want to spend some leisure time in the parks of Munich, find out how to ask a question about the location of parks in Munich in German.\n \n Use this API documentation for reference:  {\"domain\": \"Natural Language Processing Text2Text Generation\", \"framework\": \"Hugging Face Transformers\", \"functionality\": \"Language model\", \"api_name\": \"google/flan-t5-large\", \"api_call\": \"T5ForConditionalGeneration.from_pretrained('google/flan-t5-large')\", \"api_arguments\": [\"input_text\", \"input_ids\", \"outputs\"], \"python_environment_requirements\": [\"transformers\", \"torch\"], \"example_code\": \"from transformers import T5Tokenizer, T5ForConditionalGeneration\ntokenizer = T5Tokenizer.from_pretrained(google/flan-t5-large)\nmodel = T5ForConditionalGeneration.from_pretrained(google/flan-t5-large)\ninput_text = translate English to German: How old are you?\ninput_ids = tokenizer(input_text, return_tensors=pt).input_ids\noutputs = model.generate(input_ids)\nprint(tokenizer.decode(outputs[0]))\", \"performance\": {\"dataset\": [{\"name\": \"MMLU\", \"accuracy\": \"75.2%\"}]}, \"description\": \"FLAN-T5 large is a language model fine-tuned on over 1000 tasks and multiple languages. It achieves state-of-the-art performance on several benchmarks, including 75.2% on five-shot MMLU. The model is based on pretrained T5 and fine-tuned with instructions for better zero-shot and few-shot performance. It can be used for research on language models, zero-shot NLP tasks, in-context few-shot learning NLP tasks, reasoning, question answering, and advancing fairness and safety research.\"}}\nWrite a python program in 1 to 2 lines to call API in huggingface.\n\nThe answer should follow the format: {<<<domain>>> $DOMAIN, <<<api_call>>>: $API_CALL, <<<api_provider>>>: $API_PROVIDER, <<<explanation>>>: $EXPLANATION, <<<code>>>: $CODE}. Here are the requirements:\n1. The $API_CALL should have only 1 line of code that calls api.\n2. The $API_PROVIDER should be the programming framework used.\n3. $EXPLANATION should be a step-by-step explanation.\n4. The $CODE is the python code.\n5. Do not repeat the format in your answer.", "category": "generic"}
{"question_id": 147, "text": " We are a company offering speech to text services. We need to summarize the conversion and make it open-ended question.\n \n Use this API documentation for reference:  {\"domain\": \"Natural Language Processing Text2Text Generation\", \"framework\": \"Hugging Face Transformers\", \"functionality\": \"Transformers\", \"api_name\": \"bart-large-cnn-samsum-ChatGPT_v3\", \"api_call\": \"AutoModelForSeq2SeqLM.from_pretrained('Qiliang/bart-large-cnn-samsum-ChatGPT_v3')\", \"api_arguments\": [\"input_text\"], \"python_environment_requirements\": [\"transformers==4.24.0\", \"torch==1.12.1\", \"datasets==2.6.1\", \"tokenizers==0.13.2\"], \"example_code\": \"\", \"performance\": {\"dataset\": \"unknown\", \"accuracy\": \"unknown\"}, \"description\": \"This model is a fine-tuned version of philschmid/bart-large-cnn-samsum on an unknown dataset.\"}}\nWrite a python program in 1 to 2 lines to call API in huggingface.\n\nThe answer should follow the format: {<<<domain>>> $DOMAIN, <<<api_call>>>: $API_CALL, <<<api_provider>>>: $API_PROVIDER, <<<explanation>>>: $EXPLANATION, <<<code>>>: $CODE}. Here are the requirements:\n1. The $API_CALL should have only 1 line of code that calls api.\n2. The $API_PROVIDER should be the programming framework used.\n3. $EXPLANATION should be a step-by-step explanation.\n4. The $CODE is the python code.\n5. Do not repeat the format in your answer.", "category": "generic"}
{"question_id": 148, "text": " To help me with my writing, I need an AI tool that can fill in the gaps for me when I get stuck. It should be able to complete phrases or sentences.\n \n Use this API documentation for reference:  {\"domain\": \"Natural Language Processing Fill-Mask\", \"framework\": \"Hugging Face Transformers\", \"functionality\": \"Fill-Mask\", \"api_name\": \"microsoft/deberta-v3-base\", \"api_call\": \"DebertaModel.from_pretrained('microsoft/deberta-v3-base')\", \"api_arguments\": [\"model_name_or_path\"], \"python_environment_requirements\": [\"transformers\", \"torch\"], \"example_code\": \"from transformers import pipeline\nfill_mask = pipeline('fill-mask', model='microsoft/deberta-v3-base')\nresult = fill_mask('Hugging Face is a [MASK] company.')\", \"performance\": {\"dataset\": {\"SQuAD 2.0\": {\"F1\": 88.4, \"EM\": 85.4}, \"MNLI-m/mm\": {\"ACC\": \"90.6/90.7\"}}}, \"description\": \"DeBERTa V3 improves the BERT and RoBERTa models using disentangled attention and enhanced mask decoder. It further improves the efficiency of DeBERTa using ELECTRA-Style pre-training with Gradient Disentangled Embedding Sharing. The DeBERTa V3 base model comes with 12 layers and a hidden size of 768. It has only 86M backbone parameters with a vocabulary containing 128K tokens which introduces 98M parameters in the Embedding layer. This model was trained using the 160GB data as DeBERTa V2.\"}}\nWrite a python program in 1 to 2 lines to call API in huggingface.\n\nThe answer should follow the format: {<<<domain>>> $DOMAIN, <<<api_call>>>: $API_CALL, <<<api_provider>>>: $API_PROVIDER, <<<explanation>>>: $EXPLANATION, <<<code>>>: $CODE}. Here are the requirements:\n1. The $API_CALL should have only 1 line of code that calls api.\n2. The $API_PROVIDER should be the programming framework used.\n3. $EXPLANATION should be a step-by-step explanation.\n4. The $CODE is the python code.\n5. Do not repeat the format in your answer.", "category": "generic"}
{"question_id": 149, "text": " A writer needs help with generating the next word in the phrase \\\"The dog jumped over the\\\" __.\n \n Use this API documentation for reference:  {\"domain\": \"Natural Language Processing Fill-Mask\", \"framework\": \"Transformers\", \"functionality\": \"Fill-Mask\", \"api_name\": \"microsoft/deberta-v2-xxlarge\", \"api_call\": \"DebertaV2ForMaskedLM.from_pretrained('microsoft/deberta-v2-xxlarge')\", \"api_arguments\": {\"model_name_or_path\": \"microsoft/deberta-v2-xxlarge\"}, \"python_environment_requirements\": {\"pip_install\": [\"datasets\", \"deepspeed\"]}, \"example_code\": \"python -m torch.distributed.launch --nproc_per_node=${num_gpus} run_glue.py --model_name_or_path microsoft/deberta-v2-xxlarge --task_name $TASK_NAME --do_train --do_eval --max_seq_length 256 --per_device_train_batch_size ${batch_size} --learning_rate 3e-6 --num_train_epochs 3 --output_dir $output_dir --overwrite_output_dir --logging_steps 10 --logging_dir $output_dir --deepspeed ds_config.json\", \"performance\": {\"dataset\": [{\"name\": \"SQuAD 1.1\", \"accuracy\": \"F1/EM: 96.1/91.4\"}, {\"name\": \"SQuAD 2.0\", \"accuracy\": \"F1/EM: 92.2/89.7\"}, {\"name\": \"MNLI-m/mm\", \"accuracy\": \"Acc: 91.7/91.9\"}, {\"name\": \"SST-2\", \"accuracy\": \"Acc: 97.2\"}, {\"name\": \"QNLI\", \"accuracy\": \"Acc: 96.0\"}, {\"name\": \"CoLA\", \"accuracy\": \"MCC: 72.0\"}, {\"name\": \"RTE\", \"accuracy\": \"Acc: 93.5\"}, {\"name\": \"MRPC\", \"accuracy\": \"Acc/F1: 93.1/94.9\"}, {\"name\": \"QQP\", \"accuracy\": \"Acc/F1: 92.7/90.3\"}, {\"name\": \"STS-B\", \"accuracy\": \"P/S: 93.2/93.1\"}]}, \"description\": \"DeBERTa improves the BERT and RoBERTa models using disentangled attention and enhanced mask decoder. It outperforms BERT and RoBERTa on majority of NLU tasks with 80GB training data. This is the DeBERTa V2 xxlarge model with 48 layers, 1536 hidden size. The total parameters are 1.5B and it is trained with 160GB raw data.\"}}\nWrite a python program in 1 to 2 lines to call API in huggingface.\n\nThe answer should follow the format: {<<<domain>>> $DOMAIN, <<<api_call>>>: $API_CALL, <<<api_provider>>>: $API_PROVIDER, <<<explanation>>>: $EXPLANATION, <<<code>>>: $CODE}. Here are the requirements:\n1. The $API_CALL should have only 1 line of code that calls api.\n2. The $API_PROVIDER should be the programming framework used.\n3. $EXPLANATION should be a step-by-step explanation.\n4. The $CODE is the python code.\n5. Do not repeat the format in your answer.", "category": "generic"}
{"question_id": 151, "text": " Help me find similarity scores for different restaurant reviews.\n \n Use this API documentation for reference:  {\"domain\": \"Natural Language Processing Sentence Similarity\", \"framework\": \"Hugging Face Transformers\", \"functionality\": \"Sentence Transformers\", \"api_name\": \"flax-sentence-embeddings/all_datasets_v4_MiniLM-L6\", \"api_call\": \"SentenceTransformer('flax-sentence-embeddings/all_datasets_v4_MiniLM-L6')\", \"api_arguments\": \"text\", \"python_environment_requirements\": \"sentence-transformers library\", \"example_code\": \"from sentence_transformers import SentenceTransformer\nmodel = SentenceTransformer('flax-sentence-embeddings/all_datasets_v4_MiniLM-L6')\ntext = Replace me by any text you'd like.\ntext_embbedding = model.encode(text)\", \"performance\": {\"dataset\": \"1,097,953,922\", \"accuracy\": \"N/A\"}, \"description\": \"The model is trained on very large sentence level datasets using a self-supervised contrastive learning objective. It is fine-tuned on a 1B sentence pairs dataset, and it aims to capture the semantic information of input sentences. The sentence vector can be used for information retrieval, clustering, or sentence similarity tasks.\"}}\nWrite a python program in 1 to 2 lines to call API in huggingface.\n\nThe answer should follow the format: {<<<domain>>> $DOMAIN, <<<api_call>>>: $API_CALL, <<<api_provider>>>: $API_PROVIDER, <<<explanation>>>: $EXPLANATION, <<<code>>>: $CODE}. Here are the requirements:\n1. The $API_CALL should have only 1 line of code that calls api.\n2. The $API_PROVIDER should be the programming framework used.\n3. $EXPLANATION should be a step-by-step explanation.\n4. The $CODE is the python code.\n5. Do not repeat the format in your answer.", "category": "generic"}
{"question_id": 152, "text": " Our company is building an automated assistance system for the visually impaired. We need a tool to convert text into spoken instructions.\n \n Use this API documentation for reference:  {\"domain\": \"Audio Text-to-Speech\", \"framework\": \"ESPnet\", \"functionality\": \"Text-to-Speech\", \"api_name\": \"kan-bayashi_ljspeech_vits\", \"api_call\": \"pipeline('text-to-speech', model='espnet/kan-bayashi_ljspeech_vits')\", \"api_arguments\": \"text\", \"python_environment_requirements\": \"transformers\", \"example_code\": \"from transformers import pipeline; tts = pipeline('text-to-speech', model='espnet/kan-bayashi_ljspeech_vits'); tts('Hello World')\", \"performance\": {\"dataset\": \"ljspeech\", \"accuracy\": \"Not mentioned\"}, \"description\": \"A Text-to-Speech model trained on the ljspeech dataset using the ESPnet toolkit. This model can be used to convert text input into synthesized speech.\"}}\nWrite a python program in 1 to 2 lines to call API in huggingface.\n\nThe answer should follow the format: {<<<domain>>> $DOMAIN, <<<api_call>>>: $API_CALL, <<<api_provider>>>: $API_PROVIDER, <<<explanation>>>: $EXPLANATION, <<<code>>>: $CODE}. Here are the requirements:\n1. The $API_CALL should have only 1 line of code that calls api.\n2. The $API_PROVIDER should be the programming framework used.\n3. $EXPLANATION should be a step-by-step explanation.\n4. The $CODE is the python code.\n5. Do not repeat the format in your answer.", "category": "generic"}
{"question_id": 153, "text": " I would like to create an application where users may listen to translations of English sentences. I need a Text-to-Speech model to support this functionality.\n \n Use this API documentation for reference:  {\"domain\": \"Audio Text-to-Speech\", \"framework\": \"ESPnet\", \"functionality\": \"Text-to-Speech\", \"api_name\": \"mio/Artoria\", \"api_call\": \"pipeline('text-to-speech', model='mio/Artoria')\", \"api_arguments\": [\"text\"], \"python_environment_requirements\": [\"transformers\"], \"example_code\": \"from transformers import pipeline; tts = pipeline('text-to-speech', model='mio/Artoria'); tts('s')\", \"performance\": {\"dataset\": \"fate\", \"accuracy\": \"Not provided\"}, \"description\": \"This model was trained by mio using fate recipe in espnet. It is a text-to-speech model that can convert text input into speech output.\"}}\nWrite a python program in 1 to 2 lines to call API in huggingface.\n\nThe answer should follow the format: {<<<domain>>> $DOMAIN, <<<api_call>>>: $API_CALL, <<<api_provider>>>: $API_PROVIDER, <<<explanation>>>: $EXPLANATION, <<<code>>>: $CODE}. Here are the requirements:\n1. The $API_CALL should have only 1 line of code that calls api.\n2. The $API_PROVIDER should be the programming framework used.\n3. $EXPLANATION should be a step-by-step explanation.\n4. The $CODE is the python code.\n5. Do not repeat the format in your answer.", "category": "generic"}
{"question_id": 154, "text": " We are a language learning app platform. We need a text-to-speech tool to read a sentence in multiple languages for our users.\n \n Use this API documentation for reference:  {\"domain\": \"Audio Text-to-Speech\", \"framework\": \"Fairseq\", \"functionality\": \"Speech-to-speech translation\", \"api_name\": \"facebook/unit_hifigan_mhubert_vp_en_es_fr_it3_400k_layer11_km1000_lj_dur\", \"api_call\": \"load_model_ensemble_and_task_from_hf_hub('facebook/unit_hifigan_mhubert_vp_en_es_fr_it3_400k_layer11_km1000_lj_dur')\", \"api_arguments\": {\"audio\": \"16000Hz mono channel audio\"}, \"python_environment_requirements\": [\"fairseq\", \"hub_utils\", \"huggingface_hub\", \"IPython.display\", \"torchaudio\"], \"example_code\": [\"import json\", \"import os\", \"from pathlib import Path\", \"import IPython.display as ipd\", \"from fairseq import hub_utils\", \"from fairseq.checkpoint_utils import load_model_ensemble_and_task_from_hf_hub\", \"from fairseq.models.speech_to_text.hub_interface import S2THubInterface\", \"from fairseq.models.text_to_speech import CodeHiFiGANVocoder\", \"from fairseq.models.text_to_speech.hub_interface import VocoderHubInterface\", \"from huggingface_hub import snapshot_download\", \"import torchaudio\", \"cache_dir = os.getenv(HUGGINGFACE_HUB_CACHE)\", \"models, cfg, task = load_model_ensemble_and_task_from_hf_hub(\", \"facebook/xm_transformer_s2ut_800m-es-en-st-asr-bt_h1_2022,\", \"arg_overrides={config_yaml: config.yaml, task: speech_to_text},\", \"cache_dir=cache_dir,\", \")\", \"model = models[0].cpu()\", \"cfg[task].cpu = True\", \"generator = task.build_generator([model], cfg)\", \"# requires 16000Hz mono channel audio\", \"audio, _ = torchaudio.load(/Users/lpw/git/api-inference-community/docker_images/fairseq/tests/samples/sample2.flac)\", \"sample = S2THubInterface.get_model_input(task, audio)\", \"unit = S2THubInterface.get_prediction(task, model, generator, sample)\", \"library_name = fairseq\", \"cache_dir = (\", \" cache_dir or (Path.home() / .cache / library_name).as_posix()\", \")\", \"cache_dir = snapshot_download(\", \" ffacebook/unit_hifigan_mhubert_vp_en_es_fr_it3_400k_layer11_km1000_lj_dur, cache_dir=cache_dir, library_name=library_name\", \")\", \"x = hub_utils.from_pretrained(\", \" cache_dir,\", \" model.pt,\", \" .,\", \" archive_map=CodeHiFiGANVocoder.hub_models(),\", \" config_yaml=config.json,\", \" fp16=False,\", \" is_vocoder=True,\", \")\", \"with open(f{x['args']['data']}/config.json) as f:\", \" vocoder_cfg = json.load(f)\", \"assert (\", \" len(x[args][model_path]) == 1\", \"), Too many vocoder models in the input\", \"vocoder = CodeHiFiGANVocoder(x[args][model_path][0], vocoder_cfg)\", \"tts_model = VocoderHubInterface(vocoder_cfg, vocoder)\", \"tts_sample = tts_model.get_model_input(unit)\", \"wav, sr = tts_model.get_prediction(tts_sample)\", \"ipd.Audio(wav, rate=sr)\"], \"performance\": {\"dataset\": \"covost2\", \"accuracy\": null}, \"description\": \"Speech-to-speech translation model from fairseq S2UT (paper/code) for Spanish-English. Trained on mTEDx, CoVoST 2, Europarl-ST, and VoxPopuli.\"}}\nWrite a python program in 1 to 2 lines to call API in huggingface.\n\nThe answer should follow the format: {<<<domain>>> $DOMAIN, <<<api_call>>>: $API_CALL, <<<api_provider>>>: $API_PROVIDER, <<<explanation>>>: $EXPLANATION, <<<code>>>: $CODE}. Here are the requirements:\n1. The $API_CALL should have only 1 line of code that calls api.\n2. The $API_PROVIDER should be the programming framework used.\n3. $EXPLANATION should be a step-by-step explanation.\n4. The $CODE is the python code.\n5. Do not repeat the format in your answer.", "category": "generic"}
{"question_id": 155, "text": " I need to create an audio output that translates the given text to speech for a French audiobook assistant.\n \n Use this API documentation for reference:  {\"domain\": \"Audio Text-to-Speech\", \"framework\": \"Fairseq\", \"functionality\": \"Text-to-Speech\", \"api_name\": \"facebook/tts_transformer-fr-cv7_css10\", \"api_call\": \"load_model_ensemble_and_task_from_hf_hub('facebook/tts_transformer-fr-cv7_css10')\", \"api_arguments\": {\"text\": \"string\"}, \"python_environment_requirements\": [\"fairseq\"], \"example_code\": \"from fairseq.checkpoint_utils import load_model_ensemble_and_task_from_hf_hub\nfrom fairseq.models.text_to_speech.hub_interface import TTSHubInterface\nimport IPython.display as ipd\nmodels, cfg, task = load_model_ensemble_and_task_from_hf_hub(\n facebook/tts_transformer-fr-cv7_css10,\n arg_overrides={vocoder: hifigan, fp16: False}\n)\nmodel = models[0]\nTTSHubInterface.update_cfg_with_data_cfg(cfg, task.data_cfg)\ngenerator = task.build_generator(model, cfg)\ntext = Bonjour, ceci est un test.\nsample = TTSHubInterface.get_model_input(task, text)\nwav, rate = TTSHubInterface.get_prediction(task, model, generator, sample)\nipd.Audio(wav, rate=rate)\", \"performance\": {\"dataset\": \"common_voice\", \"accuracy\": \"N/A\"}, \"description\": \"Transformer text-to-speech model from fairseq S^2. French, single-speaker male voice. Pre-trained on Common Voice v7, fine-tuned on CSS10.\"}}\nWrite a python program in 1 to 2 lines to call API in huggingface.\n\nThe answer should follow the format: {<<<domain>>> $DOMAIN, <<<api_call>>>: $API_CALL, <<<api_provider>>>: $API_PROVIDER, <<<explanation>>>: $EXPLANATION, <<<code>>>: $CODE}. Here are the requirements:\n1. The $API_CALL should have only 1 line of code that calls api.\n2. The $API_PROVIDER should be the programming framework used.\n3. $EXPLANATION should be a step-by-step explanation.\n4. The $CODE is the python code.\n5. Do not repeat the format in your answer.", "category": "generic"}
{"question_id": 156, "text": " We are opening a platform where users can record their own podcast and host it on our platform, can you help us to convert the audio into text automatically?\n \n Use this API documentation for reference:  {\"domain\": \"Audio Automatic Speech Recognition\", \"framework\": \"Transformers\", \"functionality\": \"Transcription\", \"api_name\": \"facebook/wav2vec2-base-960h\", \"api_call\": \"Wav2Vec2ForCTC.from_pretrained('facebook/wav2vec2-base-960h')\", \"api_arguments\": [\"input_values\"], \"python_environment_requirements\": [\"transformers\", \"datasets\", \"torch\", \"jiwer\"], \"example_code\": \"from transformers import Wav2Vec2Processor, Wav2Vec2ForCTC\nfrom datasets import load_dataset\nimport torch\nprocessor = Wav2Vec2Processor.from_pretrained('facebook/wav2vec2-base-960h')\nmodel = Wav2Vec2ForCTC.from_pretrained('facebook/wav2vec2-base-960h')\nds = load_dataset('patrickvonplaten/librispeech_asr_dummy', 'clean', split='validation')\ninput_values = processor(ds[0]['audio']['array'], return_tensors='pt', padding='longest').input_values\nlogits = model(input_values).logits\npredicted_ids = torch.argmax(logits, dim=-1)\ntranscription = processor.batch_decode(predicted_ids)\", \"performance\": {\"dataset\": \"LibriSpeech\", \"accuracy\": {\"clean\": 3.4, \"other\": 8.6}}, \"description\": \"Facebook's Wav2Vec2 base model pretrained and fine-tuned on 960 hours of Librispeech on 16kHz sampled speech audio. It is designed for automatic speech recognition and can transcribe audio files.\"}}\nWrite a python program in 1 to 2 lines to call API in huggingface.\n\nThe answer should follow the format: {<<<domain>>> $DOMAIN, <<<api_call>>>: $API_CALL, <<<api_provider>>>: $API_PROVIDER, <<<explanation>>>: $EXPLANATION, <<<code>>>: $CODE}. Here are the requirements:\n1. The $API_CALL should have only 1 line of code that calls api.\n2. The $API_PROVIDER should be the programming framework used.\n3. $EXPLANATION should be a step-by-step explanation.\n4. The $CODE is the python code.\n5. Do not repeat the format in your answer.", "category": "generic"}
{"question_id": 157, "text": " We are currently working on the development of a speech-to-text application for transcription purposes. Can you help generate the required transcription code?\n \n Use this API documentation for reference:  {\"domain\": \"Audio Automatic Speech Recognition\", \"framework\": \"Hugging Face Transformers\", \"functionality\": \"Transcription\", \"api_name\": \"openai/whisper-tiny.en\", \"api_call\": \"WhisperForConditionalGeneration.from_pretrained('openai/whisper-tiny.en')\", \"api_arguments\": {\"model_name\": \"openai/whisper-tiny.en\"}, \"python_environment_requirements\": [\"transformers\", \"datasets\", \"torch\"], \"example_code\": [\"from transformers import WhisperProcessor, WhisperForConditionalGeneration\", \"from datasets import load_dataset\", \"processor = WhisperProcessor.from_pretrained(openai/whisper-tiny.en)\", \"model = WhisperForConditionalGeneration.from_pretrained(openai/whisper-tiny.en)\", \"ds = load_dataset(hf-internal-testing/librispeech_asr_dummy, clean, split=validation)\", \"sample = ds[0][audio]\", \"input_features = processor(sample[array], sampling_rate=sample[sampling_rate], return_tensors=pt).input_features\", \"predicted_ids = model.generate(input_features)\", \"transcription = processor.batch_decode(predicted_ids, skip_special_tokens=True)\"], \"performance\": {\"dataset\": \"LibriSpeech (clean)\", \"accuracy\": 8.437}, \"description\": \"Whisper is a pre-trained model for automatic speech recognition (ASR) and speech translation. Trained on 680k hours of labelled data, Whisper models demonstrate a strong ability to generalise to many datasets and domains without the need for fine-tuning.\"}}\nWrite a python program in 1 to 2 lines to call API in huggingface.\n\nThe answer should follow the format: {<<<domain>>> $DOMAIN, <<<api_call>>>: $API_CALL, <<<api_provider>>>: $API_PROVIDER, <<<explanation>>>: $EXPLANATION, <<<code>>>: $CODE}. Here are the requirements:\n1. The $API_CALL should have only 1 line of code that calls api.\n2. The $API_PROVIDER should be the programming framework used.\n3. $EXPLANATION should be a step-by-step explanation.\n4. The $CODE is the python code.\n5. Do not repeat the format in your answer.", "category": "generic"}
{"question_id": 158, "text": " We are building an app for tour guides to transcribe what they say during the tour. Then the transcript will be translated into sign language.\n \n Use this API documentation for reference:  {\"domain\": \"Audio Automatic Speech Recognition\", \"framework\": \"Hugging Face Transformers\", \"functionality\": \"Automatic Speech Recognition and Speech Translation\", \"api_name\": \"openai/whisper-base\", \"api_call\": \"WhisperForConditionalGeneration.from_pretrained('openai/whisper-base')\", \"api_arguments\": {\"model_name\": \"openai/whisper-base\", \"input_features\": \"input_features\", \"forced_decoder_ids\": \"forced_decoder_ids\"}, \"python_environment_requirements\": [\"transformers\", \"datasets\"], \"example_code\": [\"from transformers import WhisperProcessor, WhisperForConditionalGeneration\", \"from datasets import load_dataset\", \"processor = WhisperProcessor.from_pretrained(openai/whisper-base)\", \"model = WhisperForConditionalGeneration.from_pretrained(openai/whisper-base)\", \"ds = load_dataset(hf-internal-testing/librispeech_asr_dummy, clean, split=validation)\", \"sample = ds[0][audio]\", \"input_features = processor(sample[array], sampling_rate=sample[sampling_rate], return_tensors=pt).input_features\", \"predicted_ids = model.generate(input_features)\", \"transcription = processor.batch_decode(predicted_ids, skip_special_tokens=True)\"], \"performance\": {\"dataset\": \"LibriSpeech (clean) test set\", \"accuracy\": \"5.009 WER\"}, \"description\": \"Whisper is a pre-trained model for automatic speech recognition (ASR) and speech translation. Trained on 680k hours of labelled data, Whisper models demonstrate a strong ability to generalize to many datasets and domains without the need for fine-tuning.\"}}\nWrite a python program in 1 to 2 lines to call API in huggingface.\n\nThe answer should follow the format: {<<<domain>>> $DOMAIN, <<<api_call>>>: $API_CALL, <<<api_provider>>>: $API_PROVIDER, <<<explanation>>>: $EXPLANATION, <<<code>>>: $CODE}. Here are the requirements:\n1. The $API_CALL should have only 1 line of code that calls api.\n2. The $API_PROVIDER should be the programming framework used.\n3. $EXPLANATION should be a step-by-step explanation.\n4. The $CODE is the python code.\n5. Do not repeat the format in your answer.", "category": "generic"}
{"question_id": 159, "text": " I am an English teacher. I have recently met a Hokkien speaker. I am looking to translate a conversational sentence from English to Hokkien to facilitate communication.\n \n Use this API documentation for reference:  {\"domain\": \"Audio Audio-to-Audio\", \"framework\": \"Fairseq\", \"functionality\": \"speech-to-speech-translation\", \"api_name\": \"xm_transformer_s2ut_en-hk\", \"api_call\": \"load_model_ensemble_and_task_from_hf_hub('facebook/xm_transformer_s2ut_en-hk')\", \"api_arguments\": {\"arg_overrides\": {\"config_yaml\": \"config.yaml\", \"task\": \"speech_to_text\"}, \"cache_dir\": \"cache_dir\"}, \"python_environment_requirements\": [\"fairseq\", \"huggingface_hub\", \"torchaudio\"], \"example_code\": {\"import_modules\": [\"import json\", \"import os\", \"from pathlib import Path\", \"import IPython.display as ipd\", \"from fairseq import hub_utils\", \"from fairseq.checkpoint_utils import load_model_ensemble_and_task_from_hf_hub\", \"from fairseq.models.speech_to_text.hub_interface import S2THubInterface\", \"from fairseq.models.text_to_speech import CodeHiFiGANVocoder\", \"from fairseq.models.text_to_speech.hub_interface import VocoderHubInterface\", \"from huggingface_hub import snapshot_download\", \"import torchaudio\"], \"load_model\": [\"cache_dir = os.getenv('HUGGINGFACE_HUB_CACHE')\", \"models, cfg, task = load_model_ensemble_and_task_from_hf_hub('facebook/xm_transformer_s2ut_en-hk', arg_overrides={'config_yaml': 'config.yaml', 'task': 'speech_to_text'}, cache_dir=cache_dir)\", \"model = models[0].cpu()\", \"cfg['task'].cpu = True\"], \"generate_prediction\": [\"generator = task.build_generator([model], cfg)\", \"audio, _ = torchaudio.load('/path/to/an/audio/file')\", \"sample = S2THubInterface.get_model_input(task, audio)\", \"unit = S2THubInterface.get_prediction(task, model, generator, sample)\"], \"speech_synthesis\": [\"library_name = 'fairseq'\", \"cache_dir = (cache_dir or (Path.home() / '.cache' / library_name).as_posix())\", \"cache_dir = snapshot_download('facebook/unit_hifigan_HK_layer12.km2500_frame_TAT-TTS', cache_dir=cache_dir, library_name=library_name)\", \"x = hub_utils.from_pretrained(cache_dir, 'model.pt', '.', archive_map=CodeHiFiGANVocoder.hub_models(), config_yaml='config.json', fp16=False, is_vocoder=True)\", \"with open(f'{x['args']['data']}/config.json') as f:\", \"  vocoder_cfg = json.load(f)\", \"assert (len(x['args']['model_path']) == 1), 'Too many vocoder models in the input'\", \"vocoder = CodeHiFiGANVocoder(x['args']['model_path'][0], vocoder_cfg)\", \"tts_model = VocoderHubInterface(vocoder_cfg, vocoder)\", \"tts_sample = tts_model.get_model_input(unit)\", \"wav, sr = tts_model.get_prediction(tts_sample)\", \"ipd.Audio(wav, rate=sr)\"]}, \"performance\": {\"dataset\": \"MuST-C\", \"accuracy\": \"Not specified\"}, \"description\": \"Speech-to-speech translation model with single-pass decoder (S2UT) from fairseq: English-Hokkien. Trained with supervised data in TED domain, and weakly supervised data in TED and Audiobook domain.\"}}\nWrite a python program in 1 to 2 lines to call API in huggingface.\n\nThe answer should follow the format: {<<<domain>>> $DOMAIN, <<<api_call>>>: $API_CALL, <<<api_provider>>>: $API_PROVIDER, <<<explanation>>>: $EXPLANATION, <<<code>>>: $CODE}. Here are the requirements:\n1. The $API_CALL should have only 1 line of code that calls api.\n2. The $API_PROVIDER should be the programming framework used.\n3. $EXPLANATION should be a step-by-step explanation.\n4. The $CODE is the python code.\n5. Do not repeat the format in your answer.", "category": "generic"}
{"question_id": 160, "text": " Determine the keyword spoken in a recorded audio file.\n \n Use this API documentation for reference:  {\"domain\": \"Audio Audio Classification\", \"framework\": \"Hugging Face Transformers\", \"functionality\": \"Transformers\", \"api_name\": \"superb/hubert-base-superb-ks\", \"api_call\": \"pipeline('audio-classification', model='superb/hubert-base-superb-ks')\", \"api_arguments\": [\"file\", \"top_k\"], \"python_environment_requirements\": [\"datasets\", \"transformers\", \"torchaudio\"], \"example_code\": \"from datasets import load_dataset\nfrom transformers import pipeline\ndataset = load_dataset(anton-l/superb_demo, ks, split=test)\nclassifier = pipeline(audio-classification, model=superb/hubert-base-superb-ks)\nlabels = classifier(dataset[0][file], top_k=5)\", \"performance\": {\"dataset\": \"Speech Commands dataset v1.0\", \"accuracy\": 0.9672}, \"description\": \"This is a ported version of S3PRL's Hubert for the SUPERB Keyword Spotting task. The base model is hubert-base-ls960, which is pretrained on 16kHz sampled speech audio. When using the model make sure that your speech input is also sampled at 16Khz. For more information refer to SUPERB: Speech processing Universal PERformance Benchmark.\"}}\nWrite a python program in 1 to 2 lines to call API in huggingface.\n\nThe answer should follow the format: {<<<domain>>> $DOMAIN, <<<api_call>>>: $API_CALL, <<<api_provider>>>: $API_PROVIDER, <<<explanation>>>: $EXPLANATION, <<<code>>>: $CODE}. Here are the requirements:\n1. The $API_CALL should have only 1 line of code that calls api.\n2. The $API_PROVIDER should be the programming framework used.\n3. $EXPLANATION should be a step-by-step explanation.\n4. The $CODE is the python code.\n5. Do not repeat the format in your answer.", "category": "generic"}
{"question_id": 161, "text": " Determine which speaker an audio segment belongs to using the provided audio file.\n \n Use this API documentation for reference:  {\"domain\": \"Audio Audio Classification\", \"framework\": \"Hugging Face Transformers\", \"functionality\": \"Speaker Verification\", \"api_name\": \"speechbrain/spkrec-xvect-voxceleb\", \"api_call\": \"EncoderClassifier.from_hparams(source='speechbrain/spkrec-xvect-voxceleb', savedir='pretrained_models/spkrec-xvect-voxceleb')\", \"api_arguments\": [\"source\", \"savedir\"], \"python_environment_requirements\": [\"pip install speechbrain\"], \"example_code\": \"import torchaudio\nfrom speechbrain.pretrained import EncoderClassifier\nclassifier = EncoderClassifier.from_hparams(source=speechbrain/spkrec-xvect-voxceleb, savedir=pretrained_models/spkrec-xvect-voxceleb)\nsignal, fs =torchaudio.load('tests/samples/ASR/spk1_snt1.wav')\nembeddings = classifier.encode_batch(signal)\", \"performance\": {\"dataset\": \"Voxceleb1-test set (Cleaned)\", \"accuracy\": \"EER(%) 3.2\"}, \"description\": \"This repository provides all the necessary tools to extract speaker embeddings with a pretrained TDNN model using SpeechBrain. The system is trained on Voxceleb 1+ Voxceleb2 training data.\"}}\nWrite a python program in 1 to 2 lines to call API in huggingface.\n\nThe answer should follow the format: {<<<domain>>> $DOMAIN, <<<api_call>>>: $API_CALL, <<<api_provider>>>: $API_PROVIDER, <<<explanation>>>: $EXPLANATION, <<<code>>>: $CODE}. Here are the requirements:\n1. The $API_CALL should have only 1 line of code that calls api.\n2. The $API_PROVIDER should be the programming framework used.\n3. $EXPLANATION should be a step-by-step explanation.\n4. The $CODE is the python code.\n5. Do not repeat the format in your answer.", "category": "generic"}
{"question_id": 162, "text": " We have a database of customer voices and are trying to build a voice recognition product so we can recognize customer voices when they call. How should we process and classify?\n \n Use this API documentation for reference:  {\"domain\": \"Audio Audio Classification\", \"framework\": \"Hugging Face Transformers\", \"functionality\": \"Transformers\", \"api_name\": \"superb/hubert-large-superb-sid\", \"api_call\": \"pipeline('audio-classification', model='superb/hubert-large-superb-sid')\", \"api_arguments\": \"file, top_k\", \"python_environment_requirements\": \"datasets, transformers, librosa\", \"example_code\": \"from datasets import load_dataset\nfrom transformers import pipeline\ndataset = load_dataset(anton-l/superb_demo, si, split=test)\nclassifier = pipeline(audio-classification, model=superb/hubert-large-superb-sid)\nlabels = classifier(dataset[0][file], top_k=5)\", \"performance\": {\"dataset\": \"VoxCeleb1\", \"accuracy\": 0.9035}, \"description\": \"Hubert-Large for Speaker Identification. This model is pretrained on 16kHz sampled speech audio and should be used with speech input also sampled at 16Khz. It is used for the SUPERB Speaker Identification task and can classify each utterance for its speaker identity as a multi-class classification.\"}}\nWrite a python program in 1 to 2 lines to call API in huggingface.\n\nThe answer should follow the format: {<<<domain>>> $DOMAIN, <<<api_call>>>: $API_CALL, <<<api_provider>>>: $API_PROVIDER, <<<explanation>>>: $EXPLANATION, <<<code>>>: $CODE}. Here are the requirements:\n1. The $API_CALL should have only 1 line of code that calls api.\n2. The $API_PROVIDER should be the programming framework used.\n3. $EXPLANATION should be a step-by-step explanation.\n4. The $CODE is the python code.\n5. Do not repeat the format in your answer.", "category": "generic"}
{"question_id": 163, "text": " We are developing a voice-controlled drone. Please identify the spoken command in the audio clip provided.\n###Input: \\\"audio_clip.wav\\\"\n \n Use this API documentation for reference:  {\"domain\": \"Audio Classification\", \"framework\": \"Hugging Face Transformers\", \"functionality\": \"Transformers\", \"api_name\": \"mazkooleg/0-9up-unispeech-sat-base-ft\", \"api_call\": \"pipeline('audio-classification', model='mazkooleg/0-9up-unispeech-sat-base-ft')\", \"api_arguments\": \"\", \"python_environment_requirements\": \"transformers==4.27.1, pytorch==1.11.0, datasets==2.10.1, tokenizers==0.12.1\", \"example_code\": \"\", \"performance\": {\"dataset\": \"mazkooleg/0-9up_google_speech_commands_augmented_raw\", \"accuracy\": 0.9979}, \"description\": \"This model is a fine-tuned version of microsoft/unispeech-sat-base on the None dataset. It achieves the following results on the evaluation set: Loss: 0.0123, Accuracy: 0.9979.\"}}\nWrite a python program in 1 to 2 lines to call API in huggingface.\n\nThe answer should follow the format: {<<<domain>>> $DOMAIN, <<<api_call>>>: $API_CALL, <<<api_provider>>>: $API_PROVIDER, <<<explanation>>>: $EXPLANATION, <<<code>>>: $CODE}. Here are the requirements:\n1. The $API_CALL should have only 1 line of code that calls api.\n2. The $API_PROVIDER should be the programming framework used.\n3. $EXPLANATION should be a step-by-step explanation.\n4. The $CODE is the python code.\n5. Do not repeat the format in your answer.", "category": "generic"}
{"question_id": 164, "text": " Make a summary video for our last team meeting. The audio from the video must identify who is speaking and when.\n \n Use this API documentation for reference:  {\"domain\": \"Audio Voice Activity Detection\", \"framework\": \"pyannote.audio\", \"functionality\": \"Speaker diarization\", \"api_name\": \"johnislarry/cloned-pyannote-speaker-diarization-endpoint\", \"api_call\": \"Pipeline.from_pretrained('pyannote/speaker-diarization@2.1',use_auth_token='ACCESS_TOKEN_GOES_HERE')\", \"api_arguments\": [\"num_speakers\", \"min_speakers\", \"max_speakers\", \"segmentation_onset\"], \"python_environment_requirements\": \"pyannote.audio 2.0\", \"example_code\": {\"load_pipeline\": \"from pyannote.audio import Pipeline\npipeline = Pipeline.from_pretrained(pyannote/speaker-diarization@2022.07)\", \"apply_pipeline\": \"diarization = pipeline(audio.wav)\", \"save_output\": \"with open(audio.rttm, w) as rttm:\n  diarization.write_rttm(rttm)\"}, \"performance\": {\"dataset\": [{\"name\": \"AISHELL-4\", \"accuracy\": {\"DER%\": 14.61, \"FA%\": 3.31, \"Miss%\": 4.35, \"Conf%\": 6.95}}, {\"name\": \"AMI Mix-Headset only_words\", \"accuracy\": {\"DER%\": 18.21, \"FA%\": 3.28, \"Miss%\": 11.07, \"Conf%\": 3.87}}, {\"name\": \"AMI Array1-01 only_words\", \"accuracy\": {\"DER%\": 29.0, \"FA%\": 2.71, \"Miss%\": 21.61, \"Conf%\": 4.68}}, {\"name\": \"CALLHOME Part2\", \"accuracy\": {\"DER%\": 30.24, \"FA%\": 3.71, \"Miss%\": 16.86, \"Conf%\": 9.66}}, {\"name\": \"DIHARD 3 Full\", \"accuracy\": {\"DER%\": 20.99, \"FA%\": 4.25, \"Miss%\": 10.74, \"Conf%\": 6.0}}, {\"name\": \"REPERE Phase 2\", \"accuracy\": {\"DER%\": 12.62, \"FA%\": 1.55, \"Miss%\": 3.3, \"Conf%\": 7.76}}, {\"name\": \"VoxConverse v0.0.2\", \"accuracy\": {\"DER%\": 12.76, \"FA%\": 3.45, \"Miss%\": 3.85, \"Conf%\": 5.46}}]}, \"description\": \"This API provides speaker diarization functionality using the pyannote.audio framework. It is capable of processing audio files and outputting speaker diarization results in RTTM format. The API supports providing the number of speakers, minimum and maximum number of speakers, and adjusting the segmentation onset threshold.\"}}\nWrite a python program in 1 to 2 lines to call API in huggingface.\n\nThe answer should follow the format: {<<<domain>>> $DOMAIN, <<<api_call>>>: $API_CALL, <<<api_provider>>>: $API_PROVIDER, <<<explanation>>>: $EXPLANATION, <<<code>>>: $CODE}. Here are the requirements:\n1. The $API_CALL should have only 1 line of code that calls api.\n2. The $API_PROVIDER should be the programming framework used.\n3. $EXPLANATION should be a step-by-step explanation.\n4. The $CODE is the python code.\n5. Do not repeat the format in your answer.", "category": "generic"}
{"question_id": 165, "text": " I want to estimate the price of a house based on its features using this API. Please provide the code.\n \n Use this API documentation for reference:  {\"domain\": \"Tabular Tabular Regression\", \"framework\": \"Joblib\", \"functionality\": \"Single Column Regression\", \"api_name\": \"jwan2021/autotrain-us-housing-prices-1771761513\", \"api_call\": \"joblib.load('model.joblib')\", \"api_arguments\": [\"data\"], \"python_environment_requirements\": [\"joblib\", \"pandas\"], \"example_code\": \"import json\nimport joblib\nimport pandas as pd\nmodel = joblib.load('model.joblib')\nconfig = json.load(open('config.json'))\nfeatures = config['features']\ndata = pd.read_csv('data.csv')\ndata = data[features]\ndata.columns = ['feat_' + str(col) for col in data.columns]\npredictions = model.predict(data)\", \"performance\": {\"dataset\": \"jwan2021/autotrain-data-us-housing-prices\", \"accuracy\": {\"Loss\": 100581.032, \"R2\": 0.922, \"MSE\": 10116543945.03, \"MAE\": 81586.656, \"RMSLE\": 0.101}}, \"description\": \"A single column regression model for predicting US housing prices, trained with AutoTrain and using the Joblib framework.\"}}\nWrite a python program in 1 to 2 lines to call API in huggingface.\n\nThe answer should follow the format: {<<<domain>>> $DOMAIN, <<<api_call>>>: $API_CALL, <<<api_provider>>>: $API_PROVIDER, <<<explanation>>>: $EXPLANATION, <<<code>>>: $CODE}. Here are the requirements:\n1. The $API_CALL should have only 1 line of code that calls api.\n2. The $API_PROVIDER should be the programming framework used.\n3. $EXPLANATION should be a step-by-step explanation.\n4. The $CODE is the python code.\n5. Do not repeat the format in your answer.", "category": "generic"}
{"question_id": 166, "text": " Our company wants to predict housing prices in the US based on given features. Help us use the trained model to predict the prices.\n \n Use this API documentation for reference:  {\"domain\": \"Tabular Tabular Regression\", \"framework\": \"Joblib\", \"functionality\": \"Single Column Regression\", \"api_name\": \"jwan2021/autotrain-us-housing-prices-1771761511\", \"api_call\": \"joblib.load('model.joblib')\", \"api_arguments\": {\"data\": \"pandas.DataFrame\"}, \"python_environment_requirements\": {\"joblib\": \"latest\", \"pandas\": \"latest\"}, \"example_code\": \"import json\nimport joblib\nimport pandas as pd\nmodel = joblib.load('model.joblib')\nconfig = json.load(open('config.json'))\nfeatures = config['features']\ndata = pd.read_csv('data.csv')\ndata = data[features]\ndata.columns = ['feat_' + str(col) for col in data.columns]\npredictions = model.predict(data)\", \"performance\": {\"dataset\": \"jwan2021/autotrain-data-us-housing-prices\", \"accuracy\": {\"Loss\": 134406.507, \"R2\": 0.861, \"MSE\": 18065109105.27, \"MAE\": 103271.843, \"RMSLE\": 0.139}}, \"description\": \"A model trained using AutoTrain for predicting US housing prices. The model is trained on the jwan2021/autotrain-data-us-housing-prices dataset and is a single column regression model with an ID of 1771761511.\"}}\nWrite a python program in 1 to 2 lines to call API in huggingface.\n\nThe answer should follow the format: {<<<domain>>> $DOMAIN, <<<api_call>>>: $API_CALL, <<<api_provider>>>: $API_PROVIDER, <<<explanation>>>: $EXPLANATION, <<<code>>>: $CODE}. Here are the requirements:\n1. The $API_CALL should have only 1 line of code that calls api.\n2. The $API_PROVIDER should be the programming framework used.\n3. $EXPLANATION should be a step-by-step explanation.\n4. The $CODE is the python code.\n5. Do not repeat the format in your answer.", "category": "generic"}
{"question_id": 167, "text": " An environmental organization would like to use our Carbon Emissions prediction model to estimate CO2 emissions of different configurations of vehicles.\n \n Use this API documentation for reference:  {\"domain\": \"Tabular Tabular Regression\", \"framework\": \"Joblib\", \"functionality\": \"Carbon Emissions\", \"api_name\": \"pcoloc/autotrain-600-dragino-1839063122\", \"api_call\": \"joblib.load('model.joblib')\", \"api_arguments\": [\"data\"], \"python_environment_requirements\": [\"joblib\", \"pandas\"], \"example_code\": \"import json\nimport joblib\nimport pandas as pd\nmodel = joblib.load('model.joblib')\nconfig = json.load(open('config.json'))\nfeatures = config['features']\ndata = pd.read_csv('data.csv')\ndata = data[features]\ndata.columns = ['feat_' + str(col) for col in data.columns]\npredictions = model.predict(data)\", \"performance\": {\"dataset\": \"pcoloc/autotrain-data-600-dragino\", \"accuracy\": {\"Loss\": 93.595, \"R2\": 0.502, \"MSE\": 8760.052, \"MAE\": 77.527, \"RMSLE\": 0.445}}, \"description\": \"This model is trained to perform single column regression on carbon emissions data using the AutoTrain framework. It predicts CO2 emissions in grams given the input data.\"}}\nWrite a python program in 1 to 2 lines to call API in huggingface.\n\nThe answer should follow the format: {<<<domain>>> $DOMAIN, <<<api_call>>>: $API_CALL, <<<api_provider>>>: $API_PROVIDER, <<<explanation>>>: $EXPLANATION, <<<code>>>: $CODE}. Here are the requirements:\n1. The $API_CALL should have only 1 line of code that calls api.\n2. The $API_PROVIDER should be the programming framework used.\n3. $EXPLANATION should be a step-by-step explanation.\n4. The $CODE is the python code.\n5. Do not repeat the format in your answer.", "category": "generic"}
{"question_id": 168, "text": " I am a data analyst working in pollution detection, find a model and develop a piece of code for me for environment monitoring.\n \n Use this API documentation for reference:  {\"domain\": \"Tabular Tabular Regression\", \"framework\": \"Joblib\", \"functionality\": \"Carbon Emissions\", \"api_name\": \"pcoloc/autotrain-only-rssi-1813762559\", \"api_call\": \"joblib.load('model.joblib')\", \"api_arguments\": {\"data\": \"data.csv\"}, \"python_environment_requirements\": {\"joblib\": \"latest\", \"pandas\": \"latest\"}, \"example_code\": \"import json\nimport joblib\nimport pandas as pd\nmodel = joblib.load('model.joblib')\nconfig = json.load(open('config.json'))\nfeatures = config['features']\ndata = pd.read_csv('data.csv')\ndata = data[features]\ndata.columns = ['feat_' + str(col) for col in data.columns]\npredictions = model.predict(data)\", \"performance\": {\"dataset\": {\"Loss\": 83.432, \"R2\": 0.312, \"MSE\": 6960.888, \"MAE\": 60.449, \"RMSLE\": 0.532}}, \"description\": \"A tabular regression model trained using AutoTrain for estimating carbon emissions from given features.\"}}\nWrite a python program in 1 to 2 lines to call API in huggingface.\n\nThe answer should follow the format: {<<<domain>>> $DOMAIN, <<<api_call>>>: $API_CALL, <<<api_provider>>>: $API_PROVIDER, <<<explanation>>>: $EXPLANATION, <<<code>>>: $CODE}. Here are the requirements:\n1. The $API_CALL should have only 1 line of code that calls api.\n2. The $API_PROVIDER should be the programming framework used.\n3. $EXPLANATION should be a step-by-step explanation.\n4. The $CODE is the python code.\n5. Do not repeat the format in your answer.", "category": "generic"}
{"question_id": 169, "text": " We want to develop an intelligent prosthetic leg that can improve walking. Use a decision transformer to predict actions to be taken.\n \n Use this API documentation for reference:  {\"domain\": \"Reinforcement Learning\", \"framework\": \"Hugging Face Transformers\", \"functionality\": \"Transformers\", \"api_name\": \"edbeeching/decision-transformer-gym-walker2d-expert\", \"api_call\": \"AutoModel.from_pretrained('edbeeching/decision-transformer-gym-walker2d-expert')\", \"api_arguments\": {\"mean\": [1.2384834, 0.19578537, -0.10475016, -0.18579608, 0.23003316, 0.022800924, -0.37383768, 0.337791, 3.925096, -0.0047428459, 0.025267061, -0.0039287535, -0.01736751, -0.48212224, 0.00035432147, -0.0037124525, 0.0026285544], \"std\": [0.06664903, 0.16980624, 0.17309439, 0.21843709, 0.74599105, 0.02410989, 0.3729872, 0.6226182, 0.9708009, 0.72936815, 1.504065, 2.495893, 3.511518, 5.3656907, 0.79503316, 4.317483, 6.1784487]}, \"python_environment_requirements\": [\"transformers\"], \"example_code\": \"See our Blog Post, Colab notebook or Example Script for usage.\", \"performance\": {\"dataset\": \"Gym Walker2d environment\", \"accuracy\": \"Not provided\"}, \"description\": \"Decision Transformer model trained on expert trajectories sampled from the Gym Walker2d environment.\"}}\nWrite a python program in 1 to 2 lines to call API in huggingface.\n\nThe answer should follow the format: {<<<domain>>> $DOMAIN, <<<api_call>>>: $API_CALL, <<<api_provider>>>: $API_PROVIDER, <<<explanation>>>: $EXPLANATION, <<<code>>>: $CODE}. Here are the requirements:\n1. The $API_CALL should have only 1 line of code that calls api.\n2. The $API_PROVIDER should be the programming framework used.\n3. $EXPLANATION should be a step-by-step explanation.\n4. The $CODE is the python code.\n5. Do not repeat the format in your answer.", "category": "generic"}
{"question_id": 170, "text": " You want to create a bot that can play the Pong No Frameskip-v4 game with exceptional skill.\n \n Use this API documentation for reference:  {\"domain\": \"Reinforcement Learning\", \"framework\": \"Stable-Baselines3\", \"functionality\": \"deep-reinforcement-learning\", \"api_name\": \"ppo-PongNoFrameskip-v4\", \"api_call\": \"load_from_hub(repo_id='sb3/ppo-PongNoFrameskip-v4',filename='{MODEL FILENAME}.zip',)\", \"api_arguments\": [\"algo\", \"env\", \"f\"], \"python_environment_requirements\": [\"RL Zoo\", \"SB3\", \"SB3 Contrib\"], \"example_code\": \"python -m rl_zoo3.load_from_hub --algo ppo --env PongNoFrameskip-v4 -orga sb3 -f logs/\", \"performance\": {\"dataset\": \"PongNoFrameskip-v4\", \"accuracy\": \"21.00 +/- 0.00\"}, \"description\": \"This is a trained model of a PPO agent playing PongNoFrameskip-v4 using the stable-baselines3 library and the RL Zoo. The RL Zoo is a training framework for Stable Baselines3 reinforcement learning agents, with hyperparameter optimization and pre-trained agents included.\"}}\nWrite a python program in 1 to 2 lines to call API in huggingface.\n\nThe answer should follow the format: {<<<domain>>> $DOMAIN, <<<api_call>>>: $API_CALL, <<<api_provider>>>: $API_PROVIDER, <<<explanation>>>: $EXPLANATION, <<<code>>>: $CODE}. Here are the requirements:\n1. The $API_CALL should have only 1 line of code that calls api.\n2. The $API_PROVIDER should be the programming framework used.\n3. $EXPLANATION should be a step-by-step explanation.\n4. The $CODE is the python code.\n5. Do not repeat the format in your answer.", "category": "generic"}
{"question_id": 171, "text": " I am a game developer working on a game project involving moving carts. I need to use reinforcement learning to improve the game experience.\n \n Use this API documentation for reference:  {\"domain\": \"Reinforcement Learning\", \"framework\": \"Stable-Baselines3\", \"functionality\": \"CartPole-v1\", \"api_name\": \"sb3/ppo-CartPole-v1\", \"api_call\": \"load_from_hub(repo_id='sb3/ppo-CartPole-v1',filename='{MODEL FILENAME}.zip',)\", \"api_arguments\": [\"algo\", \"env\", \"f\"], \"python_environment_requirements\": [\"rl_zoo3\", \"stable-baselines3\", \"stable-baselines3-contrib\"], \"example_code\": \"python -m rl_zoo3.load_from_hub --algo ppo --env CartPole-v1 -orga sb3 -f logs/\", \"performance\": {\"dataset\": \"CartPole-v1\", \"accuracy\": \"500.00 +/- 0.00\"}, \"description\": \"This is a trained model of a PPO agent playing CartPole-v1 using the stable-baselines3 library and the RL Zoo. The RL Zoo is a training framework for Stable Baselines3 reinforcement learning agents, with hyperparameter optimization and pre-trained agents included.\"}}\nWrite a python program in 1 to 2 lines to call API in huggingface.\n\nThe answer should follow the format: {<<<domain>>> $DOMAIN, <<<api_call>>>: $API_CALL, <<<api_provider>>>: $API_PROVIDER, <<<explanation>>>: $EXPLANATION, <<<code>>>: $CODE}. Here are the requirements:\n1. The $API_CALL should have only 1 line of code that calls api.\n2. The $API_PROVIDER should be the programming framework used.\n3. $EXPLANATION should be a step-by-step explanation.\n4. The $CODE is the python code.\n5. Do not repeat the format in your answer.", "category": "generic"}
{"question_id": 172, "text": " Develop a soccer playing agent that can outperform its opponents in a 2v2 environment.\n \n Use this API documentation for reference:  {\"domain\": \"Reinforcement Learning\", \"framework\": \"ML-Agents\", \"functionality\": \"SoccerTwos\", \"api_name\": \"Raiden-1001/poca-Soccerv7\", \"api_call\": \"mlagents-load-from-hf --repo-id='Raiden-1001/poca-Soccerv7.1' --local-dir='./downloads'\", \"api_arguments\": [\"your_configuration_file_path.yaml\", \"run_id\"], \"python_environment_requirements\": [\"unity-ml-agents\", \"deep-reinforcement-learning\", \"ML-Agents-SoccerTwos\"], \"example_code\": \"Step 1: Write your model_id: Raiden-1001/poca-Soccerv7\nStep 2: Select your .nn /.onnx file\nClick on Watch the agent play \\ud83d\\udc40\", \"performance\": {\"dataset\": \"SoccerTwos\", \"accuracy\": \"Not provided\"}, \"description\": \"This is a trained model of a poca agent playing SoccerTwos using the Unity ML-Agents Library.\"}}\nWrite a python program in 1 to 2 lines to call API in huggingface.\n\nThe answer should follow the format: {<<<domain>>> $DOMAIN, <<<api_call>>>: $API_CALL, <<<api_provider>>>: $API_PROVIDER, <<<explanation>>>: $EXPLANATION, <<<code>>>: $CODE}. Here are the requirements:\n1. The $API_CALL should have only 1 line of code that calls api.\n2. The $API_PROVIDER should be the programming framework used.\n3. $EXPLANATION should be a step-by-step explanation.\n4. The $CODE is the python code.\n5. Do not repeat the format in your answer.", "category": "generic"}
{"question_id": 173, "text": " We are tasked to analyze text for a Russian newspaper to help understand general sentiment and trends in the text.\n \n Use this API documentation for reference:  {\"domain\": \"Natural Language Processing Text Generation\", \"framework\": \"Hugging Face Transformers\", \"functionality\": \"Feature Extraction\", \"api_name\": \"sberbank-ai/sbert_large_mt_nlu_ru\", \"api_call\": \"AutoModel.from_pretrained('sberbank-ai/sbert_large_mt_nlu_ru')\", \"api_arguments\": [\"sentences\", \"padding\", \"truncation\", \"max_length\", \"return_tensors\"], \"python_environment_requirements\": [\"transformers\", \"torch\"], \"example_code\": \"from transformers import AutoTokenizer, AutoModel\nimport torch\n\n# Mean Pooling - Take attention mask into account for correct averaging\ndef mean_pooling(model_output, attention_mask):\n    token_embeddings = model_output[0]  # First element of model_output contains all token embeddings\n    input_mask_expanded = attention_mask.unsqueeze(-1).expand(token_embeddings.size()).float()\n    sum_embeddings = torch.sum(token_embeddings * input_mask_expanded, 1)\n    sum_mask = torch.clamp(input_mask_expanded.sum(1), min=1e-9)\n    return sum_embeddings / sum_mask\n\n# Sentences we want sentence embeddings for sentences = ['?']\n# Load AutoModel from huggingface model repository\ntokenizer = AutoTokenizer.from_pretrained(sberbank-ai/sbert_large_mt_nlu_ru)\nmodel = AutoModel.from_pretrained(sberbank-ai/sbert_large_mt_nlu_ru)\n# Tokenize sentences\nencoded_input = tokenizer(sentences, padding=True, truncation=True, max_length=24, return_tensors='pt')\n# Compute token embeddings\nwith torch.no_grad():\n    model_output = model(**encoded_input)\n# Perform pooling. In this case, mean pooling\nsentence_embeddings = mean_pooling(model_output, encoded_input['attention_mask'])\", \"performance\": {\"dataset\": \"Russian SuperGLUE\", \"accuracy\": \"Not provided\"}, \"description\": \"BERT large model multitask (cased) for Sentence Embeddings in Russian language.\"}}\nWrite a python program in 1 to 2 lines to call API in huggingface.\n\nThe answer should follow the format: {<<<domain>>> $DOMAIN, <<<api_call>>>: $API_CALL, <<<api_provider>>>: $API_PROVIDER, <<<explanation>>>: $EXPLANATION, <<<code>>>: $CODE}. Here are the requirements:\n1. The $API_CALL should have only 1 line of code that calls api.\n2. The $API_PROVIDER should be the programming framework used.\n3. $EXPLANATION should be a step-by-step explanation.\n4. The $CODE is the python code.\n5. Do not repeat the format in your answer.", "category": "generic"}
{"question_id": 174, "text": " We want to generate an image from a textual description for our PowerPoint presentation.\n \n Use this API documentation for reference:  {\"domain\": \"Multimodal Text-to-Image\", \"framework\": \"Hugging Face\", \"functionality\": \"Text-to-Image\", \"api_name\": \"stabilityai/sd-vae-ft-ema\", \"api_call\": \"StableDiffusionPipeline.from_pretrained('CompVis/stable-diffusion-v1-4', vae=AutoencoderKL.from_pretrained('stabilityai/sd-vae-ft-ema'))\", \"api_arguments\": {\"model\": \"CompVis/stable-diffusion-v1-4\", \"vae\": \"AutoencoderKL.from_pretrained(stabilityai/sd-vae-ft-ema)\"}, \"python_environment_requirements\": {\"diffusers\": \"diffusers library\"}, \"example_code\": \"from diffusers.models import AutoencoderKL\nfrom diffusers import StableDiffusionPipeline\nmodel = CompVis/stable-diffusion-v1-4\nvae = AutoencoderKL.from_pretrained(stabilityai/sd-vae-ft-ema)\npipe = StableDiffusionPipeline.from_pretrained(model, vae=vae)\", \"performance\": {\"dataset\": {\"COCO 2017 (256x256, val, 5000 images)\": {\"accuracy\": {\"rFID\": 4.42, \"PSNR\": \"23.8 +/- 3.9\", \"SSIM\": \"0.69 +/- 0.13\", \"PSIM\": \"0.96 +/- 0.27\"}}, \"LAION-Aesthetics 5+ (256x256, subset, 10000 images)\": {\"accuracy\": {\"rFID\": 1.77, \"PSNR\": \"26.7 +/- 4.8\", \"SSIM\": \"0.82 +/- 0.12\", \"PSIM\": \"0.67 +/- 0.34\"}}}}, \"description\": \"This is a fine-tuned VAE decoder for the Stable Diffusion Pipeline. It has been fine-tuned on a 1:1 ratio of LAION-Aesthetics and LAION-Humans datasets. The decoder can be used as a drop-in replacement for the existing autoencoder.\"}}\nWrite a python program in 1 to 2 lines to call API in huggingface.\n\nThe answer should follow the format: {<<<domain>>> $DOMAIN, <<<api_call>>>: $API_CALL, <<<api_provider>>>: $API_PROVIDER, <<<explanation>>>: $EXPLANATION, <<<code>>>: $CODE}. Here are the requirements:\n1. The $API_CALL should have only 1 line of code that calls api.\n2. The $API_PROVIDER should be the programming framework used.\n3. $EXPLANATION should be a step-by-step explanation.\n4. The $CODE is the python code.\n5. Do not repeat the format in your answer.", "category": "generic"}
{"question_id": 175, "text": " A new manga has been released and we would like to provide a manga reader app with translations. Get the text from the manga image.\n \n Use this API documentation for reference:  {\"domain\": \"Multimodal Image-to-Text\", \"framework\": \"Hugging Face Transformers\", \"functionality\": \"Transformers\", \"api_name\": \"kha-white/manga-ocr-base\", \"api_call\": \"pipeline('ocr', model='kha-white/manga-ocr-base')\", \"api_arguments\": \"image\", \"python_environment_requirements\": \"transformers\", \"example_code\": \"\", \"performance\": {\"dataset\": \"manga109s\", \"accuracy\": \"\"}, \"description\": \"Optical character recognition for Japanese text, with the main focus being Japanese manga. It uses Vision Encoder Decoder framework. Manga OCR can be used as a general purpose printed Japanese OCR, but its main goal was to provide a high quality text recognition, robust against various scenarios specific to manga: both vertical and horizontal text, text with furigana, text overlaid on images, wide variety of fonts and font styles, and low quality images.\"}}\nWrite a python program in 1 to 2 lines to call API in huggingface.\n\nThe answer should follow the format: {<<<domain>>> $DOMAIN, <<<api_call>>>: $API_CALL, <<<api_provider>>>: $API_PROVIDER, <<<explanation>>>: $EXPLANATION, <<<code>>>: $CODE}. Here are the requirements:\n1. The $API_CALL should have only 1 line of code that calls api.\n2. The $API_PROVIDER should be the programming framework used.\n3. $EXPLANATION should be a step-by-step explanation.\n4. The $CODE is the python code.\n5. Do not repeat the format in your answer.", "category": "generic"}
{"question_id": 176, "text": " We are building a social media site which creates automatic captions for users when they post a picture\n \n Use this API documentation for reference:  {\"domain\": \"Multimodal Image-to-Text\", \"framework\": \"Hugging Face Transformers\", \"functionality\": \"Image Captioning\", \"api_name\": \"nlpconnect/vit-gpt2-image-captioning\", \"api_call\": \"VisionEncoderDecoderModel.from_pretrained('nlpconnect/vit-gpt2-image-captioning')\", \"api_arguments\": {\"model\": \"nlpconnect/vit-gpt2-image-captioning\"}, \"python_environment_requirements\": [\"transformers\", \"torch\", \"PIL\"], \"example_code\": \"from transformers import VisionEncoderDecoderModel, ViTImageProcessor, AutoTokenizer\nimport torch\nfrom PIL import Image\nmodel = VisionEncoderDecoderModel.from_pretrained(nlpconnect/vit-gpt2-image-captioning)\nfeature_extractor = ViTImageProcessor.from_pretrained(nlpconnect/vit-gpt2-image-captioning)\ntokenizer = AutoTokenizer.from_pretrained(nlpconnect/vit-gpt2-image-captioning)\ndevice = torch.device(cuda if torch.cuda.is_available() else cpu)\nmodel.to(device)\nmax_length = 16\nnum_beams = 4\ngen_kwargs = {max_length: max_length, num_beams: num_beams}\ndef predict_step(image_paths):\n images = []\n for image_path in image_paths:\n i_image = Image.open(image_path)\n if i_image.mode != RGB:\n i_image = i_image.convert(mode=RGB)\nimages.append(i_image)\npixel_values = feature_extractor(images=images, return_tensors=pt).pixel_values\n pixel_values = pixel_values.to(device)\noutput_ids = model.generate(pixel_values, **gen_kwargs)\npreds = tokenizer.batch_decode(output_ids, skip_special_tokens=True)\n preds = [pred.strip() for pred in preds]\n return preds\npredict_step(['doctor.e16ba4e4.jpg']) # ['a woman in a hospital bed with a woman in a hospital bed']\", \"performance\": {\"dataset\": \"Not provided\", \"accuracy\": \"Not provided\"}, \"description\": \"An image captioning model that uses transformers to generate captions for input images. The model is based on the Illustrated Image Captioning using transformers approach.\"}}\nWrite a python program in 1 to 2 lines to call API in huggingface.\n\nThe answer should follow the format: {<<<domain>>> $DOMAIN, <<<api_call>>>: $API_CALL, <<<api_provider>>>: $API_PROVIDER, <<<explanation>>>: $EXPLANATION, <<<code>>>: $CODE}. Here are the requirements:\n1. The $API_CALL should have only 1 line of code that calls api.\n2. The $API_PROVIDER should be the programming framework used.\n3. $EXPLANATION should be a step-by-step explanation.\n4. The $CODE is the python code.\n5. Do not repeat the format in your answer.", "category": "generic"}
{"question_id": 177, "text": " There is robot in our factory which reads the image from the production line and then generate a text output based on the image.\n \n Use this API documentation for reference:  {\"domain\": \"Multimodal Image-to-Text\", \"framework\": \"Hugging Face Transformers\", \"functionality\": \"Transformers\", \"api_name\": \"git-large-r-textcaps\", \"api_call\": \"pipeline('text-generation', model='microsoft/git-large-r-textcaps')\", \"api_arguments\": \"image\", \"python_environment_requirements\": \"transformers\", \"example_code\": \"\", \"performance\": {\"dataset\": \"TextCaps\", \"accuracy\": \"\"}, \"description\": \"GIT (short for GenerativeImage2Text) model, large-sized version, fine-tuned on TextCaps. It was introduced in the paper GIT: A Generative Image-to-text Transformer for Vision and Language by Wang et al. and first released in this repository. The model is trained using 'teacher forcing' on a lot of (image, text) pairs. The goal for the model is simply to predict the next text token, giving the image tokens and previous text tokens. This allows the model to be used for tasks like image and video captioning, visual question answering (VQA) on images and videos, and even image classification (by simply conditioning the model on the image and asking it to generate a class for it in text).\"}}\nWrite a python program in 1 to 2 lines to call API in huggingface.\n\nThe answer should follow the format: {<<<domain>>> $DOMAIN, <<<api_call>>>: $API_CALL, <<<api_provider>>>: $API_PROVIDER, <<<explanation>>>: $EXPLANATION, <<<code>>>: $CODE}. Here are the requirements:\n1. The $API_CALL should have only 1 line of code that calls api.\n2. The $API_PROVIDER should be the programming framework used.\n3. $EXPLANATION should be a step-by-step explanation.\n4. The $CODE is the python code.\n5. Do not repeat the format in your answer.", "category": "generic"}
{"question_id": 178, "text": " I am a filmmaker, and I need to make a short video based on a scene description from a script.\n \n Use this API documentation for reference:  {\"domain\": \"Multimodal Text-to-Video\", \"framework\": \"Hugging Face\", \"functionality\": \"Text-to-Video\", \"api_name\": \"ImRma/Brucelee\", \"api_call\": \"pipeline('text-to-video', model='ImRma/Brucelee')\", \"api_arguments\": [\"your_text\"], \"python_environment_requirements\": [\"transformers\"], \"example_code\": \"\", \"performance\": {\"dataset\": \"\", \"accuracy\": \"\"}, \"description\": \"A Hugging Face model for converting Persian and English text into video.\"}}\nWrite a python program in 1 to 2 lines to call API in huggingface.\n\nThe answer should follow the format: {<<<domain>>> $DOMAIN, <<<api_call>>>: $API_CALL, <<<api_provider>>>: $API_PROVIDER, <<<explanation>>>: $EXPLANATION, <<<code>>>: $CODE}. Here are the requirements:\n1. The $API_CALL should have only 1 line of code that calls api.\n2. The $API_PROVIDER should be the programming framework used.\n3. $EXPLANATION should be a step-by-step explanation.\n4. The $CODE is the python code.\n5. Do not repeat the format in your answer.", "category": "generic"}
{"question_id": 179, "text": " I'm an author and want to create a short video based on a brief passage from my book. Can you generate a video based on this text?\n \n Use this API documentation for reference:  {\"domain\": \"Multimodal Text-to-Video\", \"framework\": \"Hugging Face\", \"functionality\": \"Text-to-Video\", \"api_name\": \"camenduru/text2-video-zero\", \"api_call\": \"pipeline('text-to-video', model='camenduru/text2-video-zero')\", \"api_arguments\": [\"input_text\"], \"python_environment_requirements\": [\"transformers\"], \"example_code\": \"\", \"performance\": {\"dataset\": \"\", \"accuracy\": \"\"}, \"description\": \"This model is used for generating videos from text inputs. It is based on the Hugging Face framework and can be used with the transformers library. The model is trained on a variety of text and video datasets, and can be used for tasks such as video summarization, video generation from text prompts, and more.\"}}\nWrite a python program in 1 to 2 lines to call API in huggingface.\n\nThe answer should follow the format: {<<<domain>>> $DOMAIN, <<<api_call>>>: $API_CALL, <<<api_provider>>>: $API_PROVIDER, <<<explanation>>>: $EXPLANATION, <<<code>>>: $CODE}. Here are the requirements:\n1. The $API_CALL should have only 1 line of code that calls api.\n2. The $API_PROVIDER should be the programming framework used.\n3. $EXPLANATION should be a step-by-step explanation.\n4. The $CODE is the python code.\n5. Do not repeat the format in your answer.", "category": "generic"}
{"question_id": 180, "text": " I want to build an AI model that can analyze images and answer questions about the content of the image.\n \n Use this API documentation for reference:  {\"domain\": \"Multimodal Visual Question Answering\", \"framework\": \"Hugging Face Transformers\", \"functionality\": \"Transformers\", \"api_name\": \"git-large-textvqa\", \"api_call\": \"AutoModelForSeq2SeqLM.from_pretrained('microsoft/git-large-textvqa')\", \"api_arguments\": \"image, question\", \"python_environment_requirements\": \"transformers\", \"example_code\": \"For code examples, we refer to the documentation.\", \"performance\": {\"dataset\": \"TextVQA\", \"accuracy\": \"See table 11 in the paper for more details.\"}, \"description\": \"GIT (short for GenerativeImage2Text) model, large-sized version, fine-tuned on TextVQA. It was introduced in the paper GIT: A Generative Image-to-text Transformer for Vision and Language by Wang et al. and first released in this repository. The model is trained using 'teacher forcing' on a lot of (image, text) pairs. The goal for the model is simply to predict the next text token, giving the image tokens and previous text tokens. This allows the model to be used for tasks like: image and video captioning, visual question answering (VQA) on images and videos, and even image classification (by simply conditioning the model on the image and asking it to generate a class for it in text).\"}}\nWrite a python program in 1 to 2 lines to call API in huggingface.\n\nThe answer should follow the format: {<<<domain>>> $DOMAIN, <<<api_call>>>: $API_CALL, <<<api_provider>>>: $API_PROVIDER, <<<explanation>>>: $EXPLANATION, <<<code>>>: $CODE}. Here are the requirements:\n1. The $API_CALL should have only 1 line of code that calls api.\n2. The $API_PROVIDER should be the programming framework used.\n3. $EXPLANATION should be a step-by-step explanation.\n4. The $CODE is the python code.\n5. Do not repeat the format in your answer.", "category": "generic"}
{"question_id": 181, "text": " We are trying to develop an application that helps tourists get information about attractions by analyzing images they take and responding to questions.\n \n Use this API documentation for reference:  {\"domain\": \"Multimodal Visual Question Answering\", \"framework\": \"Hugging Face\", \"functionality\": \"Visual Question Answering\", \"api_name\": \"JosephusCheung/GuanacoVQAOnConsumerHardware\", \"api_call\": \"pipeline('visual-question-answering', model='JosephusCheung/GuanacoVQAOnConsumerHardware')\", \"api_arguments\": {\"model\": \"JosephusCheung/GuanacoVQAOnConsumerHardware\", \"tokenizer\": \"JosephusCheung/GuanacoVQAOnConsumerHardware\"}, \"python_environment_requirements\": {\"transformers\": \"latest\", \"torch\": \"latest\"}, \"example_code\": \"vqa(image_path, question)\", \"performance\": {\"dataset\": \"JosephusCheung/GuanacoVQADataset\", \"accuracy\": \"unknown\"}, \"description\": \"A Visual Question Answering model trained on the GuanacoVQADataset, designed to work on consumer hardware like Colab Free T4 GPU. The model can be used to answer questions about images.\"}}\nWrite a python program in 1 to 2 lines to call API in huggingface.\n\nThe answer should follow the format: {<<<domain>>> $DOMAIN, <<<api_call>>>: $API_CALL, <<<api_provider>>>: $API_PROVIDER, <<<explanation>>>: $EXPLANATION, <<<code>>>: $CODE}. Here are the requirements:\n1. The $API_CALL should have only 1 line of code that calls api.\n2. The $API_PROVIDER should be the programming framework used.\n3. $EXPLANATION should be a step-by-step explanation.\n4. The $CODE is the python code.\n5. Do not repeat the format in your answer.", "category": "generic"}
{"question_id": 182, "text": " We have the financial documents of a company and we want to extract information about the cash flow. Modify the model so it can answer the questions related to the cash flow.\n \n Use this API documentation for reference:  {\"domain\": \"Multimodal Document Question Answer\", \"framework\": \"Hugging Face Transformers\", \"functionality\": \"Transformers\", \"api_name\": \"CZ_DVQA_layoutxlm-base\", \"api_call\": \"LayoutXLMForQuestionAnswering.from_pretrained('fimu-docproc-research/CZ_DVQA_layoutxlm-base')\", \"api_arguments\": \"\", \"python_environment_requirements\": \"transformers\", \"example_code\": \"\", \"performance\": {\"dataset\": \"\", \"accuracy\": \"\"}, \"description\": \"A Document Question Answering model based on LayoutXLM.\"}}\nWrite a python program in 1 to 2 lines to call API in huggingface.\n\nThe answer should follow the format: {<<<domain>>> $DOMAIN, <<<api_call>>>: $API_CALL, <<<api_provider>>>: $API_PROVIDER, <<<explanation>>>: $EXPLANATION, <<<code>>>: $CODE}. Here are the requirements:\n1. The $API_CALL should have only 1 line of code that calls api.\n2. The $API_PROVIDER should be the programming framework used.\n3. $EXPLANATION should be a step-by-step explanation.\n4. The $CODE is the python code.\n5. Do not repeat the format in your answer.", "category": "generic"}
{"question_id": 183, "text": " Please generate a correct building plan leveraging the data given.\n \n Use this API documentation for reference:  {\"domain\": \"Multimodal Document Question Answer\", \"framework\": \"Hugging Face Transformers\", \"functionality\": \"Transformers\", \"api_name\": \"LayoutLMX_pt_question_answer_ocrazure_correct_V18_08_04_2023\", \"api_call\": \"AutoModelForDocumentQuestionAnswering.from_pretrained('L-oenai/LayoutLMX_pt_question_answer_ocrazure_correct_V18_08_04_2023')\", \"api_arguments\": {\"question\": \"string\", \"context\": \"string\"}, \"python_environment_requirements\": [\"transformers\"], \"example_code\": \"\", \"performance\": {\"dataset\": \"\", \"accuracy\": \"\"}, \"description\": \"A LayoutLM model for document question answering.\"}}\nWrite a python program in 1 to 2 lines to call API in huggingface.\n\nThe answer should follow the format: {<<<domain>>> $DOMAIN, <<<api_call>>>: $API_CALL, <<<api_provider>>>: $API_PROVIDER, <<<explanation>>>: $EXPLANATION, <<<code>>>: $CODE}. Here are the requirements:\n1. The $API_CALL should have only 1 line of code that calls api.\n2. The $API_PROVIDER should be the programming framework used.\n3. $EXPLANATION should be a step-by-step explanation.\n4. The $CODE is the python code.\n5. Do not repeat the format in your answer.", "category": "generic"}
{"question_id": 184, "text": " Help our drone video analyzing app estimate the depth in drone footage.\n \n Use this API documentation for reference:  {\"domain\": \"Computer Vision Depth Estimation\", \"framework\": \"Hugging Face Transformers\", \"functionality\": \"Transformers\", \"api_name\": \"tiny-random-DPTForDepthEstimation\", \"api_call\": \"DPTForDepthEstimation.from_pretrained('hf-tiny-model-private/tiny-random-DPTForDepthEstimation')\", \"api_arguments\": [], \"python_environment_requirements\": [\"transformers\"], \"example_code\": \"\", \"performance\": {\"dataset\": \"\", \"accuracy\": \"\"}, \"description\": \"A tiny random DPT model for depth estimation using Hugging Face Transformers library.\"}}\nWrite a python program in 1 to 2 lines to call API in huggingface.\n\nThe answer should follow the format: {<<<domain>>> $DOMAIN, <<<api_call>>>: $API_CALL, <<<api_provider>>>: $API_PROVIDER, <<<explanation>>>: $EXPLANATION, <<<code>>>: $CODE}. Here are the requirements:\n1. The $API_CALL should have only 1 line of code that calls api.\n2. The $API_PROVIDER should be the programming framework used.\n3. $EXPLANATION should be a step-by-step explanation.\n4. The $CODE is the python code.\n5. Do not repeat the format in your answer.", "category": "generic"}
{"question_id": 185, "text": " As a salesperson, I need to analyze customer invoices to answer questions about the total amount, tax, and due date from an image file.\n \n Use this API documentation for reference:  {\"domain\": \"Multimodal Document Question Answer\", \"framework\": \"Hugging Face Transformers\", \"functionality\": \"Document Question Answering\", \"api_name\": \"CQI_Visual_Question_Awnser_PT_v0\", \"api_call\": \"pipeline('question-answering', model=LayoutLMForQuestionAnswering.from_pretrained('microsoft/layoutlm-base-uncased'))\", \"api_arguments\": [\"url\", \"question\"], \"python_environment_requirements\": [\"PIL\", \"pytesseract\", \"PyTorch\", \"transformers\"], \"example_code\": [\"nlp('https://templates.invoicehome.com/invoice-template-us-neat-750px.png', 'What is the invoice number?')\", \"nlp('https://miro.medium.com/max/787/1*iECQRIiOGTmEFLdWkVIH2g.jpeg', 'What is the purchase amount?')\", \"nlp('https://www.accountingcoach.com/wp-content/uploads/2013/10/income-statement-example@2x.png', 'What are the 2020 net sales?')\"], \"performance\": {\"dataset\": [{\"accuracy\": 0.9943977}, {\"accuracy\": 0.9912159}, {\"accuracy\": 0.59147286}]}, \"description\": \"A model for visual question answering in Portuguese and English, capable of processing PDFs and images to extract information and answer questions.\"}}\nWrite a python program in 1 to 2 lines to call API in huggingface.\n\nThe answer should follow the format: {<<<domain>>> $DOMAIN, <<<api_call>>>: $API_CALL, <<<api_provider>>>: $API_PROVIDER, <<<explanation>>>: $EXPLANATION, <<<code>>>: $CODE}. Here are the requirements:\n1. The $API_CALL should have only 1 line of code that calls api.\n2. The $API_PROVIDER should be the programming framework used.\n3. $EXPLANATION should be a step-by-step explanation.\n4. The $CODE is the python code.\n5. Do not repeat the format in your answer.", "category": "generic"}
{"question_id": 186, "text": " We need to find out the depth information of a room for monitoring purposes.\n \n Use this API documentation for reference:  {\"domain\": \"Computer Vision Depth Estimation\", \"framework\": \"Hugging Face Transformers\", \"functionality\": \"Depth Estimation\", \"api_name\": \"glpn-nyu-finetuned-diode-221116-104421\", \"api_call\": \"AutoModel.from_pretrained('sayakpaul/glpn-nyu-finetuned-diode-221116-104421')\", \"api_arguments\": \"\", \"python_environment_requirements\": \"transformers==4.24.0, pytorch==1.12.1+cu113, tokenizers==0.13.2\", \"example_code\": \"\", \"performance\": {\"dataset\": \"diode-subset\", \"accuracy\": {\"Loss\": 0.3736, \"Mae\": 0.3079, \"Rmse\": 0.4321, \"Abs Rel\": 0.3666, \"Log Mae\": 0.1288, \"Log Rmse\": 0.1794, \"Delta1\": 0.4929, \"Delta2\": 0.7934, \"Delta3\": 0.9234}}, \"description\": \"This model is a fine-tuned version of vinvino02/glpn-nyu on the diode-subset dataset.\"}}\nWrite a python program in 1 to 2 lines to call API in huggingface.\n\nThe answer should follow the format: {<<<domain>>> $DOMAIN, <<<api_call>>>: $API_CALL, <<<api_provider>>>: $API_PROVIDER, <<<explanation>>>: $EXPLANATION, <<<code>>>: $CODE}. Here are the requirements:\n1. The $API_CALL should have only 1 line of code that calls api.\n2. The $API_PROVIDER should be the programming framework used.\n3. $EXPLANATION should be a step-by-step explanation.\n4. The $CODE is the python code.\n5. Do not repeat the format in your answer.", "category": "generic"}
{"question_id": 187, "text": " We are creating an autonomous car and need to estimate the depth of objects in a given scene.\n \n Use this API documentation for reference:  {\"domain\": \"Computer Vision Depth Estimation\", \"framework\": \"Hugging Face Transformers\", \"functionality\": \"Transformers\", \"api_name\": \"glpn-nyu-finetuned-diode-221122-044810\", \"api_call\": \"pipeline('depth-estimation', model='sayakpaul/glpn-nyu-finetuned-diode-221122-044810')\", \"api_arguments\": \"\", \"python_environment_requirements\": \"transformers==4.24.0, torch==1.12.1, tokenizers==0.13.2\", \"example_code\": \"\", \"performance\": {\"dataset\": \"diode-subset\", \"accuracy\": {\"Loss\": 0.369, \"Mae\": 0.2909, \"Rmse\": 0.4208, \"Abs Rel\": 0.3635, \"Log Mae\": 0.1224, \"Log Rmse\": 0.1793, \"Delta1\": 0.5323, \"Delta2\": 0.8179, \"Delta3\": 0.9258}}, \"description\": \"This model is a fine-tuned version of vinvino02/glpn-nyu on the diode-subset dataset.\"}}\nWrite a python program in 1 to 2 lines to call API in huggingface.\n\nThe answer should follow the format: {<<<domain>>> $DOMAIN, <<<api_call>>>: $API_CALL, <<<api_provider>>>: $API_PROVIDER, <<<explanation>>>: $EXPLANATION, <<<code>>>: $CODE}. Here are the requirements:\n1. The $API_CALL should have only 1 line of code that calls api.\n2. The $API_PROVIDER should be the programming framework used.\n3. $EXPLANATION should be a step-by-step explanation.\n4. The $CODE is the python code.\n5. Do not repeat the format in your answer.", "category": "generic"}
{"question_id": 188, "text": " Help us create an AI solution to automatically label images taken by a security camera.\n \n Use this API documentation for reference:  {\"domain\": \"Computer Vision Image Classification\", \"framework\": \"Hugging Face Transformers\", \"functionality\": \"Image Classification\", \"api_name\": \"facebook/regnet-y-008\", \"api_call\": \"RegNetForImageClassification.from_pretrained('zuppif/regnet-y-040')\", \"api_arguments\": {\"pretrained_model_name_or_path\": \"zuppif/regnet-y-040\"}, \"python_environment_requirements\": {\"transformers\": \"AutoFeatureExtractor, RegNetForImageClassification\", \"torch\": \"torch\", \"datasets\": \"load_dataset\"}, \"example_code\": \"from transformers import AutoFeatureExtractor, RegNetForImageClassification\nimport torch\nfrom datasets import load_dataset\ndataset = load_dataset(huggingface/cats-image)\nimage = dataset[test][image][0]\nfeature_extractor = AutoFeatureExtractor.from_pretrained(zuppif/regnet-y-040)\nmodel = RegNetForImageClassification.from_pretrained(zuppif/regnet-y-040)\ninputs = feature_extractor(image, return_tensors=pt)\nwith torch.no_grad():\n... logits = model(**inputs).logits\npredicted_label = logits.argmax(-1).item()\nprint(model.config.id2label[predicted_label])\", \"performance\": {\"dataset\": \"imagenet-1k\", \"accuracy\": \"Not provided\"}, \"description\": \"RegNet model trained on imagenet-1k. It was introduced in the paper Designing Network Design Spaces and first released in this repository.\"}}\nWrite a python program in 1 to 2 lines to call API in huggingface.\n\nThe answer should follow the format: {<<<domain>>> $DOMAIN, <<<api_call>>>: $API_CALL, <<<api_provider>>>: $API_PROVIDER, <<<explanation>>>: $EXPLANATION, <<<code>>>: $CODE}. Here are the requirements:\n1. The $API_CALL should have only 1 line of code that calls api.\n2. The $API_PROVIDER should be the programming framework used.\n3. $EXPLANATION should be a step-by-step explanation.\n4. The $CODE is the python code.\n5. Do not repeat the format in your answer.", "category": "generic"}
{"question_id": 189, "text": " Develop a software to classify an image from a URL into a thousand categories.\n \n Use this API documentation for reference:  {\"domain\": \"Computer Vision Image Classification\", \"framework\": \"Hugging Face Transformers\", \"functionality\": \"Image Classification, Feature Map Extraction, Image Embeddings\", \"api_name\": \"convnext_base.fb_in1k\", \"api_call\": \"timm.create_model('convnext_base.fb_in1k', pretrained=True)\", \"api_arguments\": {\"pretrained\": \"True\", \"features_only\": \"True\", \"num_classes\": \"0\"}, \"python_environment_requirements\": [\"timm\"], \"example_code\": [\"from urllib.request import urlopen\", \"from PIL import Image\", \"import timm\", \"img = Image.open(urlopen('https://huggingface.co/datasets/huggingface/documentation-images/resolve/main/beignets-task-guide.png'))\", \"model = timm.create_model('convnext_base.fb_in1k', pretrained=True)\", \"model = model.eval()\", \"data_config = timm.data.resolve_model_data_config(model)\", \"transforms = timm.data.create_transform(**data_config, is_training=False)\", \"output = model(transforms(img).unsqueeze(0))\"], \"performance\": {\"dataset\": \"imagenet-1k\", \"accuracy\": \"83.82%\"}, \"description\": \"A ConvNeXt image classification model pretrained on ImageNet-1k by paper authors. It can be used for image classification, feature map extraction, and image embeddings.\"}}\nWrite a python program in 1 to 2 lines to call API in huggingface.\n\nThe answer should follow the format: {<<<domain>>> $DOMAIN, <<<api_call>>>: $API_CALL, <<<api_provider>>>: $API_PROVIDER, <<<explanation>>>: $EXPLANATION, <<<code>>>: $CODE}. Here are the requirements:\n1. The $API_CALL should have only 1 line of code that calls api.\n2. The $API_PROVIDER should be the programming framework used.\n3. $EXPLANATION should be a step-by-step explanation.\n4. The $CODE is the python code.\n5. Do not repeat the format in your answer.", "category": "generic"}
{"question_id": 190, "text": " Our delivery drones need to detect and avoid obstacles while flying. Develop a solution for them to detect objects in their path.\n \n Use this API documentation for reference:  {\"domain\": \"Computer Vision Object Detection\", \"framework\": \"Hugging Face Transformers\", \"functionality\": \"Object Detection\", \"api_name\": \"hustvl/yolos-small\", \"api_call\": \"YolosForObjectDetection.from_pretrained('hustvl/yolos-small')\", \"api_arguments\": {\"model_name\": \"hustvl/yolos-small\"}, \"python_environment_requirements\": {\"packages\": [\"transformers\", \"PIL\", \"requests\"]}, \"example_code\": {\"import\": [\"from transformers import YolosFeatureExtractor, YolosForObjectDetection\", \"from PIL import Image\", \"import requests\"], \"url\": \"http://images.cocodataset.org/val2017/000000039769.jpg\", \"image\": \"Image.open(requests.get(url, stream=True).raw)\", \"feature_extractor\": \"YolosFeatureExtractor.from_pretrained('hustvl/yolos-small')\", \"model\": \"YolosForObjectDetection.from_pretrained('hustvl/yolos-small')\", \"inputs\": \"feature_extractor(images=image, return_tensors='pt')\", \"outputs\": \"model(**inputs)\", \"logits\": \"outputs.logits\", \"bboxes\": \"outputs.pred_boxes\"}, \"performance\": {\"dataset\": \"COCO 2017 validation\", \"accuracy\": \"36.1 AP\"}, \"description\": \"YOLOS model fine-tuned on COCO 2017 object detection (118k annotated images). It was introduced in the paper You Only Look at One Sequence: Rethinking Transformer in Vision through Object Detection by Fang et al. and first released in this repository. YOLOS is a Vision Transformer (ViT) trained using the DETR loss. Despite its simplicity, a base-sized YOLOS model is able to achieve 42 AP on COCO validation 2017 (similar to DETR and more complex frameworks such as Faster R-CNN).\"}}\nWrite a python program in 1 to 2 lines to call API in huggingface.\n\nThe answer should follow the format: {<<<domain>>> $DOMAIN, <<<api_call>>>: $API_CALL, <<<api_provider>>>: $API_PROVIDER, <<<explanation>>>: $EXPLANATION, <<<code>>>: $CODE}. Here are the requirements:\n1. The $API_CALL should have only 1 line of code that calls api.\n2. The $API_PROVIDER should be the programming framework used.\n3. $EXPLANATION should be a step-by-step explanation.\n4. The $CODE is the python code.\n5. Do not repeat the format in your answer.", "category": "generic"}
{"question_id": 191, "text": " Develop a code to recognize objects in images using deformable-detr model.\n \n Use this API documentation for reference:  {\"domain\": \"Computer Vision Object Detection\", \"framework\": \"Hugging Face Transformers\", \"functionality\": \"Object Detection\", \"api_name\": \"deformable-detr\", \"api_call\": \"DeformableDetrForObjectDetection.from_pretrained('SenseTime/deformable-detr')\", \"api_arguments\": [\"images\", \"return_tensors\"], \"python_environment_requirements\": [\"transformers\", \"torch\", \"PIL\", \"requests\"], \"example_code\": \"from transformers import AutoImageProcessor, DeformableDetrForObjectDetection\nimport torch\nfrom PIL import Image\nimport requests\nurl = 'http://images.cocodataset.org/val2017/000000039769.jpg'\nimage = Image.open(requests.get(url, stream=True).raw)\nprocessor = AutoImageProcessor.from_pretrained('SenseTime/deformable-detr')\nmodel = DeformableDetrForObjectDetection.from_pretrained('SenseTime/deformable-detr')\ninputs = processor(images=image, return_tensors='pt')\noutputs = model(**inputs)\", \"performance\": {\"dataset\": \"COCO 2017\", \"accuracy\": \"Not provided\"}, \"description\": \"Deformable DETR model with ResNet-50 backbone trained end-to-end on COCO 2017 object detection (118k annotated images). It was introduced in the paper Deformable DETR: Deformable Transformers for End-to-End Object Detection by Zhu et al. and first released in this repository.\"}}\nWrite a python program in 1 to 2 lines to call API in huggingface.\n\nThe answer should follow the format: {<<<domain>>> $DOMAIN, <<<api_call>>>: $API_CALL, <<<api_provider>>>: $API_PROVIDER, <<<explanation>>>: $EXPLANATION, <<<code>>>: $CODE}. Here are the requirements:\n1. The $API_CALL should have only 1 line of code that calls api.\n2. The $API_PROVIDER should be the programming framework used.\n3. $EXPLANATION should be a step-by-step explanation.\n4. The $CODE is the python code.\n5. Do not repeat the format in your answer.", "category": "generic"}
{"question_id": 192, "text": " I need to extract tables from a set of scanned document images to simplify data analysis.\n \n Use this API documentation for reference:  {\"domain\": \"Computer Vision Object Detection\", \"framework\": \"Hugging Face Transformers\", \"functionality\": \"Detect Bordered and Borderless tables in documents\", \"api_name\": \"TahaDouaji/detr-doc-table-detection\", \"api_call\": \"DetrForObjectDetection.from_pretrained('TahaDouaji/detr-doc-table-detection')\", \"api_arguments\": [\"images\", \"return_tensors\", \"threshold\"], \"python_environment_requirements\": [\"transformers\", \"torch\", \"PIL\", \"requests\"], \"example_code\": \"from transformers import DetrImageProcessor, DetrForObjectDetection\nimport torch\nfrom PIL import Image\nimport requests\nimage = Image.open(IMAGE_PATH)\nprocessor = DetrImageProcessor.from_pretrained(TahaDouaji/detr-doc-table-detection)\nmodel = DetrForObjectDetection.from_pretrained(TahaDouaji/detr-doc-table-detection)\ninputs = processor(images=image, return_tensors=pt)\noutputs = model(**inputs)\ntarget_sizes = torch.tensor([image.size[::-1]])\nresults = processor.post_process_object_detection(outputs, target_sizes=target_sizes, threshold=0.9)[0]\nfor score, label, box in zip(results[scores], results[labels], results[boxes]):\n box = [round(i, 2) for i in box.tolist()]\n print(\n fDetected {model.config.id2label[label.item()]} with confidence \n f{round(score.item(), 3)} at location {box}\n )\", \"performance\": {\"dataset\": \"ICDAR2019 Table Dataset\", \"accuracy\": \"Not provided\"}, \"description\": \"detr-doc-table-detection is a model trained to detect both Bordered and Borderless tables in documents, based on facebook/detr-resnet-50.\"}}\nWrite a python program in 1 to 2 lines to call API in huggingface.\n\nThe answer should follow the format: {<<<domain>>> $DOMAIN, <<<api_call>>>: $API_CALL, <<<api_provider>>>: $API_PROVIDER, <<<explanation>>>: $EXPLANATION, <<<code>>>: $CODE}. Here are the requirements:\n1. The $API_CALL should have only 1 line of code that calls api.\n2. The $API_PROVIDER should be the programming framework used.\n3. $EXPLANATION should be a step-by-step explanation.\n4. The $CODE is the python code.\n5. Do not repeat the format in your answer.", "category": "generic"}
{"question_id": 193, "text": " Our customers wish to automatically detect shoplifters in the store using a surveillance camera. Help them to implement object detection and identify potential thieves.\n \n Use this API documentation for reference:  {\"domain\": \"Computer Vision Object Detection\", \"framework\": \"Transformers\", \"functionality\": \"Object Detection\", \"api_name\": \"fcakyon/yolov5s-v7.0\", \"api_call\": \"yolov5.load('fcakyon/yolov5s-v7.0')\", \"api_arguments\": {\"conf\": 0.25, \"iou\": 0.45, \"agnostic\": false, \"multi_label\": false, \"max_det\": 1000, \"img\": \"https://github.com/ultralytics/yolov5/raw/master/data/images/zidane.jpg\", \"size\": 640, \"augment\": true}, \"python_environment_requirements\": \"pip install -U yolov5\", \"example_code\": \"import yolov5\nmodel = yolov5.load('fcakyon/yolov5s-v7.0')\nmodel.conf = 0.25\nmodel.iou = 0.45\nmodel.agnostic = False\nmodel.multi_label = False\nmodel.max_det = 1000\nimg = 'https://github.com/ultralytics/yolov5/raw/master/data/images/zidane.jpg'\nresults = model(img)\nresults = model(img, size=640)\nresults = model(img, augment=True)\npredictions = results.pred[0]\nboxes = predictions[:, :4]\nscores = predictions[:, 4]\ncategories = predictions[:, 5]\nresults.show()\nresults.save(save_dir='results/')\", \"performance\": {\"dataset\": \"detection-datasets/coco\", \"accuracy\": null}, \"description\": \"Yolov5s-v7.0 is an object detection model trained on the COCO dataset. It can detect objects in images and return their bounding boxes, scores, and categories.\"}}\nWrite a python program in 1 to 2 lines to call API in huggingface.\n\nThe answer should follow the format: {<<<domain>>> $DOMAIN, <<<api_call>>>: $API_CALL, <<<api_provider>>>: $API_PROVIDER, <<<explanation>>>: $EXPLANATION, <<<code>>>: $CODE}. Here are the requirements:\n1. The $API_CALL should have only 1 line of code that calls api.\n2. The $API_PROVIDER should be the programming framework used.\n3. $EXPLANATION should be a step-by-step explanation.\n4. The $CODE is the python code.\n5. Do not repeat the format in your answer.", "category": "generic"}
{"question_id": 194, "text": " Create an object detector that can detect blood cells in an image, such as platelets, red blood cells, and white blood cells.\n \n Use this API documentation for reference:  {\"domain\": \"Computer Vision Object Detection\", \"framework\": \"Hugging Face Transformers\", \"functionality\": \"Blood Cell Detection\", \"api_name\": \"keremberke/yolov8n-blood-cell-detection\", \"api_call\": \"YOLO('keremberke/yolov8n-blood-cell-detection')\", \"api_arguments\": {\"conf\": 0.25, \"iou\": 0.45, \"agnostic_nms\": false, \"max_det\": 1000}, \"python_environment_requirements\": \"ultralyticsplus==0.0.23 ultralytics==8.0.21\", \"example_code\": \"from ultralyticsplus import YOLO, render_result\nmodel = YOLO('keremberke/yolov8n-blood-cell-detection')\nmodel.overrides['conf'] = 0.25\nmodel.overrides['iou'] = 0.45\nmodel.overrides['agnostic_nms'] = False\nmodel.overrides['max_det'] = 1000\nimage = 'https://github.com/ultralytics/yolov5/raw/master/data/images/zidane.jpg'\nresults = model.predict(image)\nprint(results[0].boxes)\nrender = render_result(model=model, image=image, result=results[0])\nrender.show()\", \"performance\": {\"dataset\": \"blood-cell-object-detection\", \"accuracy\": 0.893}, \"description\": \"This model detects blood cells in images, specifically Platelets, RBC, and WBC. It is based on the YOLOv8 architecture and trained on the blood-cell-object-detection dataset.\"}}\nWrite a python program in 1 to 2 lines to call API in huggingface.\n\nThe answer should follow the format: {<<<domain>>> $DOMAIN, <<<api_call>>>: $API_CALL, <<<api_provider>>>: $API_PROVIDER, <<<explanation>>>: $EXPLANATION, <<<code>>>: $CODE}. Here are the requirements:\n1. The $API_CALL should have only 1 line of code that calls api.\n2. The $API_PROVIDER should be the programming framework used.\n3. $EXPLANATION should be a step-by-step explanation.\n4. The $CODE is the python code.\n5. Do not repeat the format in your answer.", "category": "generic"}
{"question_id": 195, "text": " I am a real-estate agent working on a project where I need to convert images of room plans to a better visual representation.\n \n Use this API documentation for reference:  {\"domain\": \"Computer Vision Image-to-Image\", \"framework\": \"Hugging Face\", \"functionality\": \"Image-to-Image\", \"api_name\": \"lllyasviel/sd-controlnet-canny\", \"api_call\": \"ControlNetModel.from_pretrained('lllyasviel/sd-controlnet-canny')\", \"api_arguments\": {\"torch_dtype\": \"torch.float16\"}, \"python_environment_requirements\": {\"opencv\": \"pip install opencv-contrib-python\", \"diffusers\": \"pip install diffusers transformers accelerate\"}, \"example_code\": \"import cv2\nfrom PIL import Image\nfrom diffusers import StableDiffusionControlNetPipeline, ControlNetModel, UniPCMultistepScheduler\nimport torch\nimport numpy as np\nfrom diffusers.utils import load_image\nimage = load_image(https://huggingface.co/lllyasviel/sd-controlnet-hed/resolve/main/images/bird.png)\nimage = np.array(image)\nlow_threshold = 100\nhigh_threshold = 200\nimage = cv2.Canny(image, low_threshold, high_threshold)\nimage = image[:, :, None]\nimage = np.concatenate([image, image, image], axis=2)\nimage = Image.fromarray(image)\ncontrolnet = ControlNetModel.from_pretrained(\n lllyasviel/sd-controlnet-canny, torch_dtype=torch.float16\n)\npipe = StableDiffusionControlNetPipeline.from_pretrained(\n runwayml/stable-diffusion-v1-5, controlnet=controlnet, safety_checker=None, torch_dtype=torch.float16\n)\npipe.scheduler = UniPCMultistepScheduler.from_config(pipe.scheduler.config)\npipe.enable_xformers_memory_efficient_attention()\npipe.enable_model_cpu_offload()\nimage = pipe(bird, image, num_inference_steps=20).images[0]\nimage.save('images/bird_canny_out.png')\", \"performance\": {\"dataset\": \"3M edge-image, caption pairs\", \"accuracy\": \"600 GPU-hours with Nvidia A100 80G\"}, \"description\": \"ControlNet is a neural network structure to control diffusion models by adding extra conditions. This checkpoint corresponds to the ControlNet conditioned on Canny edges. It can be used in combination with Stable Diffusion.\"}}\nWrite a python program in 1 to 2 lines to call API in huggingface.\n\nThe answer should follow the format: {<<<domain>>> $DOMAIN, <<<api_call>>>: $API_CALL, <<<api_provider>>>: $API_PROVIDER, <<<explanation>>>: $EXPLANATION, <<<code>>>: $CODE}. Here are the requirements:\n1. The $API_CALL should have only 1 line of code that calls api.\n2. The $API_PROVIDER should be the programming framework used.\n3. $EXPLANATION should be a step-by-step explanation.\n4. The $CODE is the python code.\n5. Do not repeat the format in your answer.", "category": "generic"}
{"question_id": 196, "text": " We recently received low resolution images of newly released products and need to upscale them for better quality.\n \n Use this API documentation for reference:  {\"domain\": \"Computer Vision Image-to-Image\", \"framework\": \"Hugging Face Transformers\", \"functionality\": \"Transformers\", \"api_name\": \"swin2SR-lightweight-x2-64\", \"api_call\": \"Swin2SRForConditionalGeneration.from_pretrained('condef/Swin2SR-lightweight-x2-64').\", \"api_arguments\": \"feature_extractor, model\", \"python_environment_requirements\": \"transformers, torch\", \"example_code\": \"\", \"performance\": {\"dataset\": \"\", \"accuracy\": \"\"}, \"description\": \"Swin2SR model that upscales images x2. It was introduced in the paper Swin2SR: SwinV2 Transformer for Compressed Image Super-Resolution and Restoration by Conde et al. and first released in this repository. This model is intended for lightweight image super resolution.\"}}\nWrite a python program in 1 to 2 lines to call API in huggingface.\n\nThe answer should follow the format: {<<<domain>>> $DOMAIN, <<<api_call>>>: $API_CALL, <<<api_provider>>>: $API_PROVIDER, <<<explanation>>>: $EXPLANATION, <<<code>>>: $CODE}. Here are the requirements:\n1. The $API_CALL should have only 1 line of code that calls api.\n2. The $API_PROVIDER should be the programming framework used.\n3. $EXPLANATION should be a step-by-step explanation.\n4. The $CODE is the python code.\n5. Do not repeat the format in your answer.", "category": "generic"}
{"question_id": 197, "text": " As a toy company, we are designing a new toy line. We'd like you to create an image of a toy robot using relevant text prompts as control input.\n \n Use this API documentation for reference:  {\"domain\": \"Computer Vision Image-to-Image\", \"framework\": \"Hugging Face\", \"functionality\": \"Text-to-Image Diffusion Models\", \"api_name\": \"lllyasviel/control_v11p_sd15_mlsd\", \"api_call\": \"ControlNetModel.from_pretrained('lllyasviel/control_v11p_sd15_mlsd')\", \"api_arguments\": [\"checkpoint\", \"torch_dtype\"], \"python_environment_requirements\": [\"diffusers\", \"transformers\", \"accelerate\", \"controlnet_aux\"], \"example_code\": \"import torch\nimport os\nfrom huggingface_hub import HfApi\nfrom pathlib import Path\nfrom diffusers.utils import load_image\nfrom PIL import Image\nimport numpy as np\nfrom controlnet_aux import MLSDdetector\nfrom diffusers import (\n ControlNetModel,\n StableDiffusionControlNetPipeline,\n UniPCMultistepScheduler,\n)\ncheckpoint = lllyasviel/control_v11p_sd15_mlsd\nimage = load_image(\n https://huggingface.co/lllyasviel/control_v11p_sd15_mlsd/resolve/main/images/input.png\n)\nprompt = royal chamber with fancy bed\nprocessor = MLSDdetector.from_pretrained('lllyasviel/ControlNet')\ncontrol_image = processor(image)\ncontrol_image.save(./images/control.png)\ncontrolnet = ControlNetModel.from_pretrained(checkpoint, torch_dtype=torch.float16)\npipe = StableDiffusionControlNetPipeline.from_pretrained(\n runwayml/stable-diffusion-v1-5, controlnet=controlnet, torch_dtype=torch.float16\n)\npipe.scheduler = UniPCMultistepScheduler.from_config(pipe.scheduler.config)\npipe.enable_model_cpu_offload()\ngenerator = torch.manual_seed(0)\nimage = pipe(prompt, num_inference_steps=30, generator=generator, image=control_image).images[0]\nimage.save('images/image_out.png')\", \"performance\": {\"dataset\": \"MLSD\", \"accuracy\": \"Not provided\"}, \"description\": \"Controlnet v1.1 is a neural network structure to control diffusion models by adding extra conditions. It can be used in combination with Stable Diffusion, such as runwayml/stable-diffusion-v1-5. This checkpoint corresponds to the ControlNet conditioned on MLSD images.\"}}\nWrite a python program in 1 to 2 lines to call API in huggingface.\n\nThe answer should follow the format: {<<<domain>>> $DOMAIN, <<<api_call>>>: $API_CALL, <<<api_provider>>>: $API_PROVIDER, <<<explanation>>>: $EXPLANATION, <<<code>>>: $CODE}. Here are the requirements:\n1. The $API_CALL should have only 1 line of code that calls api.\n2. The $API_PROVIDER should be the programming framework used.\n3. $EXPLANATION should be a step-by-step explanation.\n4. The $CODE is the python code.\n5. Do not repeat the format in your answer.", "category": "generic"}
{"question_id": 198, "text": " I'm developing a game that needs more Minecraft skins for some characters. How can I generete them with this model?\n \n Use this API documentation for reference:  {\"domain\": \"Computer Vision Unconditional Image Generation\", \"framework\": \"Hugging Face Transformers\", \"functionality\": \"Diffusers\", \"api_name\": \"WiNE-iNEFF/Minecraft-Skin-Diffusion-V2\", \"api_call\": \"DDPMPipeline.from_pretrained('WiNE-iNEFF/Minecraft-Skin-Diffusion-V2')\", \"api_arguments\": [], \"python_environment_requirements\": [\"diffusers\"], \"example_code\": \"from diffusers import DDPMPipeline\npipeline = DDPMPipeline.from_pretrained('WiNE-iNEFF/Minecraft-Skin-Diffusion-V2')\nimage = pipeline().images[0].convert('RGBA')\nimage\", \"performance\": {\"dataset\": null, \"accuracy\": null}, \"description\": \"An unconditional image generation model for generating Minecraft skin images using the diffusion model.\"}}\nWrite a python program in 1 to 2 lines to call API in huggingface.\n\nThe answer should follow the format: {<<<domain>>> $DOMAIN, <<<api_call>>>: $API_CALL, <<<api_provider>>>: $API_PROVIDER, <<<explanation>>>: $EXPLANATION, <<<code>>>: $CODE}. Here are the requirements:\n1. The $API_CALL should have only 1 line of code that calls api.\n2. The $API_PROVIDER should be the programming framework used.\n3. $EXPLANATION should be a step-by-step explanation.\n4. The $CODE is the python code.\n5. Do not repeat the format in your answer.", "category": "generic"}
{"question_id": 199, "text": " Find me a generative model to create cat images in 256x256 resolution.\n \n Use this API documentation for reference:  {\"domain\": \"Computer Vision Unconditional Image Generation\", \"framework\": \"Hugging Face Transformers\", \"functionality\": \"Denoising Diffusion Probabilistic Models (DDPM)\", \"api_name\": \"google/ddpm-ema-cat-256\", \"api_call\": \"DDPMPipeline.from_pretrained('google/ddpm-ema-cat-256')\", \"api_arguments\": [\"model_id\"], \"python_environment_requirements\": [\"!pip install diffusers\"], \"example_code\": \"from diffusers import DDPMPipeline, DDIMPipeline, PNDMPipeline\nmodel_id = google/ddpm-ema-cat-256\nddpm = DDPMPipeline.from_pretrained(model_id)\nimage = ddpm().images[0]\nimage.save(ddpm_generated_image.png)\", \"performance\": {\"dataset\": \"CIFAR10\", \"accuracy\": {\"Inception_score\": 9.46, \"FID_score\": 3.17}}, \"description\": \"Denoising Diffusion Probabilistic Models (DDPM) is a class of latent variable models inspired by considerations from nonequilibrium thermodynamics. It can generate high-quality images, and supports different noise schedulers such as scheduling_ddpm, scheduling_ddim, and scheduling_pndm. On the unconditional CIFAR10 dataset, it achieves an Inception score of 9.46 and a state-of-the-art FID score of 3.17.\"}}\nWrite a python program in 1 to 2 lines to call API in huggingface.\n\nThe answer should follow the format: {<<<domain>>> $DOMAIN, <<<api_call>>>: $API_CALL, <<<api_provider>>>: $API_PROVIDER, <<<explanation>>>: $EXPLANATION, <<<code>>>: $CODE}. Here are the requirements:\n1. The $API_CALL should have only 1 line of code that calls api.\n2. The $API_PROVIDER should be the programming framework used.\n3. $EXPLANATION should be a step-by-step explanation.\n4. The $CODE is the python code.\n5. Do not repeat the format in your answer.", "category": "generic"}
{"question_id": 200, "text": " Our organization works with video surveillance. We need a system to analyze the videos and classify various events happening inside the video.\n \n Use this API documentation for reference:  {\"domain\": \"Computer Vision Video Classification\", \"framework\": \"Hugging Face Transformers\", \"functionality\": \"Video Classification\", \"api_name\": \"MCG-NJU/videomae-base\", \"api_call\": \"VideoMAEForPreTraining.from_pretrained('MCG-NJU/videomae-base')\", \"api_arguments\": [\"video\"], \"python_environment_requirements\": [\"transformers\"], \"example_code\": \"from transformers import VideoMAEImageProcessor, VideoMAEForPreTraining\nimport numpy as np\nimport torch\nnum_frames = 16\nvideo = list(np.random.randn(16, 3, 224, 224))\nprocessor = VideoMAEImageProcessor.from_pretrained(MCG-NJU/videomae-base)\nmodel = VideoMAEForPreTraining.from_pretrained(MCG-NJU/videomae-base)\npixel_values = processor(video, return_tensors=pt).pixel_values\nnum_patches_per_frame = (model.config.image_size // model.config.patch_size) ** 2\nseq_length = (num_frames // model.config.tubelet_size) * num_patches_per_frame\nbool_masked_pos = torch.randint(0, 2, (1, seq_length)).bool()\noutputs = model(pixel_values, bool_masked_pos=bool_masked_pos)\nloss = outputs.loss\", \"performance\": {\"dataset\": \"Kinetics-400\", \"accuracy\": \"To be provided\"}, \"description\": \"VideoMAE is an extension of Masked Autoencoders (MAE) to video. The architecture of the model is very similar to that of a standard Vision Transformer (ViT), with a decoder on top for predicting pixel values for masked patches.\"}}\nWrite a python program in 1 to 2 lines to call API in huggingface.\n\nThe answer should follow the format: {<<<domain>>> $DOMAIN, <<<api_call>>>: $API_CALL, <<<api_provider>>>: $API_PROVIDER, <<<explanation>>>: $EXPLANATION, <<<code>>>: $CODE}. Here are the requirements:\n1. The $API_CALL should have only 1 line of code that calls api.\n2. The $API_PROVIDER should be the programming framework used.\n3. $EXPLANATION should be a step-by-step explanation.\n4. The $CODE is the python code.\n5. Do not repeat the format in your answer.", "category": "generic"}
{"question_id": 201, "text": " People in my company need an automatic solution to categorize videos based on their content. The system should be able to recognize the main theme of a video with high accuracy.\n \n Use this API documentation for reference:  {\"domain\": \"Computer Vision Video Classification\", \"framework\": \"Hugging Face Transformers\", \"functionality\": \"Video Classification\", \"api_name\": \"MCG-NJU/videomae-base-short-finetuned-kinetics\", \"api_call\": \"VideoMAEForVideoClassification.from_pretrained('MCG-NJU/videomae-base-short-finetuned-kinetics')\", \"api_arguments\": [\"video\"], \"python_environment_requirements\": [\"transformers\"], \"example_code\": \"from transformers import VideoMAEImageProcessor, VideoMAEForVideoClassification\nimport numpy as np\nimport torch\nvideo = list(np.random.randn(16, 3, 224, 224))\nprocessor = VideoMAEImageProcessor.from_pretrained('MCG-NJU/videomae-base-short-finetuned-kinetics')\nmodel = VideoMAEForVideoClassification.from_pretrained('MCG-NJU/videomae-base-short-finetuned-kinetics')\ninputs = processor(video, return_tensors='pt')\nwith torch.no_grad():\n outputs = model(**inputs)\n logits = outputs.logits\npredicted_class_idx = logits.argmax(-1).item()\nprint('Predicted class:', model.config.id2label[predicted_class_idx])\", \"performance\": {\"dataset\": \"Kinetics-400\", \"accuracy\": {\"top-1\": 79.4, \"top-5\": 94.1}}, \"description\": \"VideoMAE model pre-trained for 800 epochs in a self-supervised way and fine-tuned in a supervised way on Kinetics-400. It was introduced in the paper VideoMAE: Masked Autoencoders are Data-Efficient Learners for Self-Supervised Video Pre-Training by Tong et al. and first released in this repository.\"}}\nWrite a python program in 1 to 2 lines to call API in huggingface.\n\nThe answer should follow the format: {<<<domain>>> $DOMAIN, <<<api_call>>>: $API_CALL, <<<api_provider>>>: $API_PROVIDER, <<<explanation>>>: $EXPLANATION, <<<code>>>: $CODE}. Here are the requirements:\n1. The $API_CALL should have only 1 line of code that calls api.\n2. The $API_PROVIDER should be the programming framework used.\n3. $EXPLANATION should be a step-by-step explanation.\n4. The $CODE is the python code.\n5. Do not repeat the format in your answer.", "category": "generic"}
{"question_id": 202, "text": " We want to build a product to classify images of pets into different categories.\n \n Use this API documentation for reference:  {\"domain\": \"Computer Vision Zero-Shot Image Classification\", \"framework\": \"Hugging Face Transformers\", \"functionality\": \"Zero-Shot Image Classification\", \"api_name\": \"openai/clip-vit-large-patch14\", \"api_call\": \"CLIPModel.from_pretrained('openai/clip-vit-large-patch14')\", \"api_arguments\": {\"text\": [\"a photo of a cat\", \"a photo of a dog\"], \"images\": \"image\", \"return_tensors\": \"pt\", \"padding\": \"True\"}, \"python_environment_requirements\": {\"packages\": [\"PIL\", \"requests\", \"transformers\"]}, \"example_code\": \"from PIL import Image\nimport requests\nfrom transformers import CLIPProcessor, CLIPModel\nmodel = CLIPModel.from_pretrained(openai/clip-vit-large-patch14)\nprocessor = CLIPProcessor.from_pretrained(openai/clip-vit-large-patch14)\nurl = http://images.cocodataset.org/val2017/000000039769.jpg\nimage = Image.open(requests.get(url, stream=True).raw)\ninputs = processor(text=[a photo of a cat, a photo of a dog], images=image, return_tensors=pt, padding=True)\noutputs = model(**inputs)\nlogits_per_image = outputs.logits_per_image\nprobs = logits_per_image.softmax(dim=1)\", \"performance\": {\"dataset\": [\"Food101\", \"CIFAR10\", \"CIFAR100\", \"Birdsnap\", \"SUN397\", \"Stanford Cars\", \"FGVC Aircraft\", \"VOC2007\", \"DTD\", \"Oxford-IIIT Pet dataset\", \"Caltech101\", \"Flowers102\", \"MNIST\", \"SVHN\", \"IIIT5K\", \"Hateful Memes\", \"SST-2\", \"UCF101\", \"Kinetics700\", \"Country211\", \"CLEVR Counting\", \"KITTI Distance\", \"STL-10\", \"RareAct\", \"Flickr30\", \"MSCOCO\", \"ImageNet\", \"ImageNet-A\", \"ImageNet-R\", \"ImageNet Sketch\", \"ObjectNet (ImageNet Overlap)\", \"Youtube-BB\", \"ImageNet-Vid\"], \"accuracy\": \"varies depending on the dataset\"}, \"description\": \"The CLIP model was developed by researchers at OpenAI to learn about what contributes to robustness in computer vision tasks. The model was also developed to test the ability of models to generalize to arbitrary image classification tasks in a zero-shot manner.\"}}\nWrite a python program in 1 to 2 lines to call API in huggingface.\n\nThe answer should follow the format: {<<<domain>>> $DOMAIN, <<<api_call>>>: $API_CALL, <<<api_provider>>>: $API_PROVIDER, <<<explanation>>>: $EXPLANATION, <<<code>>>: $CODE}. Here are the requirements:\n1. The $API_CALL should have only 1 line of code that calls api.\n2. The $API_PROVIDER should be the programming framework used.\n3. $EXPLANATION should be a step-by-step explanation.\n4. The $CODE is the python code.\n5. Do not repeat the format in your answer.", "category": "generic"}
